Initializing Resizer input layer: bicubic interpolation
Initializing base_model: EfficientNetV2S ...
Compiling Resizer and base_model into key_model ...
Freezing key_model layers
Loading CIFAR-10 dataset ...
  Preprocessing training data ...
Applying TrivialAugment to training data...
Augmenting |██████████████████████████████████████████████████| 50000/50000 (100.0%) [time elapsed:0:04:20<remaining:0:00:00>, 191.9 samples/s]
  Preprocessing testing data ...
Checking for saved features...
Saved features detected. Validating ...
  Indexing training images to match features order
training data is of type: <class 'numpy.ndarray'>
trainig data shape: (50000, 32, 32, 3)
Creating and training top_model ...
top_model compiled!
Training top_model on extracted features in batches
Epoch 1/10
1562/1562 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.7874/home/blackrukh/.local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
1562/1562 [==============================] - 263s 167ms/step - loss: 0.6296 - accuracy: 0.7874 - val_loss: 0.2732 - val_accuracy: 0.9108 - lr: 0.0010
Epoch 2/10
1562/1562 [==============================] - 262s 168ms/step - loss: 0.4962 - accuracy: 0.8306 - val_loss: 0.2559 - val_accuracy: 0.9114 - lr: 0.0010
Epoch 3/10
1562/1562 [==============================] - 270s 173ms/step - loss: 0.4370 - accuracy: 0.8486 - val_loss: 0.2422 - val_accuracy: 0.9189 - lr: 0.0010
Epoch 4/10
1562/1562 [==============================] - 261s 167ms/step - loss: 0.3774 - accuracy: 0.8691 - val_loss: 0.2506 - val_accuracy: 0.9215 - lr: 0.0010
Epoch 5/10
1562/1562 [==============================] - 261s 167ms/step - loss: 0.3206 - accuracy: 0.8887 - val_loss: 0.2611 - val_accuracy: 0.9178 - lr: 0.0010
Epoch 6/10
1562/1562 [==============================] - 261s 167ms/step - loss: 0.2763 - accuracy: 0.9038 - val_loss: 0.2680 - val_accuracy: 0.9211 - lr: 0.0010
Epoch 7/10
1562/1562 [==============================] - 261s 167ms/step - loss: 0.1974 - accuracy: 0.9312 - val_loss: 0.2593 - val_accuracy: 0.9278 - lr: 5.0000e-04
Epoch 8/10
1562/1562 [==============================] - 265s 170ms/step - loss: 0.1582 - accuracy: 0.9459 - val_loss: 0.2790 - val_accuracy: 0.9273 - lr: 5.0000e-04
Epoch 9/10
1562/1562 [==============================] - 275s 176ms/step - loss: 0.1309 - accuracy: 0.9570 - val_loss: 0.3094 - val_accuracy: 0.9272 - lr: 5.0000e-04
Epoch 10/10
1562/1562 [==============================] - 262s 168ms/step - loss: 0.1026 - accuracy: 0.9669 - val_loss: 0.3111 - val_accuracy: 0.9305 - lr: 2.5000e-04

Generating predictions and visualization...
313/313 [==============================] - 31s 95ms/step

Classification Report:
              precision    recall  f1-score   support

    airplane       0.94      0.92      0.93      1000
  automobile       0.95      0.95      0.95      1000
        bird       0.95      0.92      0.94      1000
         cat       0.89      0.84      0.86      1000
        deer       0.94      0.92      0.93      1000
         dog       0.85      0.93      0.88      1000
        frog       0.95      0.96      0.95      1000
       horse       0.94      0.96      0.95      1000
        ship       0.96      0.95      0.95      1000
       truck       0.94      0.95      0.95      1000

    accuracy                           0.93     10000
   macro avg       0.93      0.93      0.93     10000
weighted avg       0.93      0.93      0.93     10000


Visualization files have been saved!
Creating and saving full model...
/home/blackrukh/.local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
Model saved successfully!