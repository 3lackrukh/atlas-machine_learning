./forecast_btc.py 
2025-03-30 08:25:19.084134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-30 08:25:19.084416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-30 08:25:19.312561: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-30 08:25:19.758790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-30 08:25:23.065080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Bitcoin Price Forecasting with RNNs
=================================

Loaded data: X shape (69289, 24, 21), y shape (69289,)
Using target column: Close for evaluation
Train set: (49887, 24, 21), (49887,)
Validation set: (5544, 24, 21), (5544,)
Test set: (13858, 24, 21), (13858,)
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 bidirectional (Bidirection  (None, 24, 256)           153600    
 al)                                                             
                                                                 
 bidirectional_1 (Bidirecti  (None, 128)               164352    
 onal)                                                           
                                                                 
 dense (Dense)               (None, 32)                4128      
                                                                 
 dense_1 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 322113 (1.23 MB)
Trainable params: 322113 (1.23 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/50
1559/1559 [==============================] - 122s 74ms/step - loss: 0.0343 - mae: 0.0148 - val_loss: 1.8643e-04 - val_mae: 0.0107 - lr: 0.0010
Epoch 2/50
1559/1559 [==============================] - 119s 76ms/step - loss: 8.4744e-05 - mae: 0.0038 - val_loss: 2.5423e-04 - val_mae: 0.0125 - lr: 0.0010
Epoch 3/50
1559/1559 [==============================] - 121s 77ms/step - loss: 5.4886e-05 - mae: 0.0029 - val_loss: 6.4179e-04 - val_mae: 0.0235 - lr: 0.0010
Epoch 4/50
1559/1559 [==============================] - 127s 82ms/step - loss: 8.2401e-05 - mae: 0.0029 - val_loss: 1.5676e-04 - val_mae: 0.0102 - lr: 0.0010
Epoch 5/50
1559/1559 [==============================] - 116s 75ms/step - loss: 3.4965e-05 - mae: 0.0024 - val_loss: 3.5727e-04 - val_mae: 0.0172 - lr: 0.0010
Epoch 6/50
1559/1559 [==============================] - 76s 49ms/step - loss: 4.7684e-05 - mae: 0.0024 - val_loss: 3.1922e-04 - val_mae: 0.0157 - lr: 0.0010
Epoch 7/50
1559/1559 [==============================] - 76s 49ms/step - loss: 1.7732e-05 - mae: 0.0016 - val_loss: 3.3379e-05 - val_mae: 0.0041 - lr: 5.0000e-04
Epoch 8/50
1559/1559 [==============================] - 84s 54ms/step - loss: 2.1759e-05 - mae: 0.0016 - val_loss: 5.4941e-05 - val_mae: 0.0061 - lr: 5.0000e-04
Epoch 9/50
1559/1559 [==============================] - 84s 54ms/step - loss: 2.7170e-05 - mae: 0.0017 - val_loss: 6.3694e-05 - val_mae: 0.0061 - lr: 5.0000e-04
Epoch 10/50
1559/1559 [==============================] - 91s 58ms/step - loss: 1.8981e-05 - mae: 0.0015 - val_loss: 4.8969e-05 - val_mae: 0.0055 - lr: 5.0000e-04
Epoch 11/50
1559/1559 [==============================] - 84s 54ms/step - loss: 1.5657e-05 - mae: 0.0014 - val_loss: 5.1450e-05 - val_mae: 0.0060 - lr: 5.0000e-04
Epoch 12/50
1559/1559 [==============================] - 84s 54ms/step - loss: 1.5473e-05 - mae: 0.0014 - val_loss: 5.2691e-05 - val_mae: 0.0055 - lr: 5.0000e-04
Epoch 13/50
1559/1559 [==============================] - 84s 54ms/step - loss: 1.1214e-05 - mae: 0.0011 - val_loss: 5.3606e-05 - val_mae: 0.0055 - lr: 2.5000e-04
Epoch 14/50
1559/1559 [==============================] - 84s 54ms/step - loss: 1.2790e-05 - mae: 0.0011 - val_loss: 4.1207e-05 - val_mae: 0.0049 - lr: 2.5000e-04
Epoch 15/50
1559/1559 [==============================] - 85s 54ms/step - loss: 1.4666e-05 - mae: 0.0012 - val_loss: 3.1185e-05 - val_mae: 0.0042 - lr: 2.5000e-04
Epoch 16/50
1559/1559 [==============================] - 83s 53ms/step - loss: 1.0797e-05 - mae: 0.0010 - val_loss: 4.0791e-05 - val_mae: 0.0051 - lr: 2.5000e-04
Epoch 17/50
1559/1559 [==============================] - 84s 54ms/step - loss: 1.1934e-05 - mae: 0.0010 - val_loss: 2.9401e-05 - val_mae: 0.0039 - lr: 2.5000e-04
Epoch 18/50
1559/1559 [==============================] - 81s 52ms/step - loss: 1.0252e-05 - mae: 9.6917e-04 - val_loss: 8.5465e-05 - val_mae: 0.0083 - lr: 1.2500e-04
Epoch 19/50
1559/1559 [==============================] - 80s 51ms/step - loss: 9.8099e-06 - mae: 8.9260e-04 - val_loss: 1.6866e-04 - val_mae: 0.0118 - lr: 1.2500e-04
Epoch 20/50
1559/1559 [==============================] - 82s 53ms/step - loss: 1.0407e-05 - mae: 9.6635e-04 - val_loss: 2.3049e-05 - val_mae: 0.0033 - lr: 1.2500e-04
Epoch 21/50
1559/1559 [==============================] - 90s 58ms/step - loss: 9.5220e-06 - mae: 9.0797e-04 - val_loss: 6.6397e-05 - val_mae: 0.0073 - lr: 1.2500e-04
Epoch 22/50
1559/1559 [==============================] - 87s 56ms/step - loss: 8.9621e-06 - mae: 8.6611e-04 - val_loss: 4.3485e-05 - val_mae: 0.0054 - lr: 1.2500e-04
Epoch 23/50
1559/1559 [==============================] - 82s 53ms/step - loss: 9.2008e-06 - mae: 8.2542e-04 - val_loss: 2.5421e-05 - val_mae: 0.0037 - lr: 6.2500e-05
Epoch 24/50
1559/1559 [==============================] - 81s 52ms/step - loss: 7.9704e-06 - mae: 7.8026e-04 - val_loss: 2.2929e-05 - val_mae: 0.0033 - lr: 6.2500e-05
Epoch 25/50
1559/1559 [==============================] - 81s 52ms/step - loss: 8.3487e-06 - mae: 8.0209e-04 - val_loss: 2.0967e-05 - val_mae: 0.0030 - lr: 6.2500e-05
Epoch 26/50
1559/1559 [==============================] - 79s 51ms/step - loss: 8.1483e-06 - mae: 7.8276e-04 - val_loss: 2.3102e-05 - val_mae: 0.0033 - lr: 6.2500e-05
Epoch 27/50
1559/1559 [==============================] - 77s 49ms/step - loss: 8.7581e-06 - mae: 8.1589e-04 - val_loss: 1.9876e-05 - val_mae: 0.0028 - lr: 6.2500e-05
Epoch 28/50
1559/1559 [==============================] - 90s 58ms/step - loss: 7.6471e-06 - mae: 7.3477e-04 - val_loss: 2.9612e-05 - val_mae: 0.0041 - lr: 3.1250e-05
Epoch 29/50
1559/1559 [==============================] - 91s 58ms/step - loss: 8.2995e-06 - mae: 7.5303e-04 - val_loss: 4.9692e-05 - val_mae: 0.0061 - lr: 3.1250e-05
Epoch 30/50
1559/1559 [==============================] - 79s 50ms/step - loss: 7.9241e-06 - mae: 7.5172e-04 - val_loss: 2.2436e-05 - val_mae: 0.0034 - lr: 3.1250e-05
Epoch 31/50
1559/1559 [==============================] - 76s 49ms/step - loss: 7.9134e-06 - mae: 7.3395e-04 - val_loss: 3.1512e-05 - val_mae: 0.0043 - lr: 3.1250e-05
Epoch 32/50
1559/1559 [==============================] - 75s 48ms/step - loss: 7.6916e-06 - mae: 7.2903e-04 - val_loss: 2.0266e-05 - val_mae: 0.0029 - lr: 3.1250e-05
Epoch 33/50
1559/1559 [==============================] - 74s 47ms/step - loss: 7.4349e-06 - mae: 7.0346e-04 - val_loss: 2.9378e-05 - val_mae: 0.0041 - lr: 1.5625e-05
Epoch 34/50
1559/1559 [==============================] - 74s 47ms/step - loss: 7.4415e-06 - mae: 7.0844e-04 - val_loss: 2.0190e-05 - val_mae: 0.0030 - lr: 1.5625e-05
Epoch 35/50
1559/1559 [==============================] - 74s 47ms/step - loss: 7.6115e-06 - mae: 7.2552e-04 - val_loss: 3.0942e-05 - val_mae: 0.0044 - lr: 1.5625e-05
Epoch 36/50
1559/1559 [==============================] - 74s 48ms/step - loss: 7.5463e-06 - mae: 7.1505e-04 - val_loss: 1.9099e-05 - val_mae: 0.0027 - lr: 1.5625e-05
Epoch 37/50
1559/1559 [==============================] - 74s 47ms/step - loss: 7.4220e-06 - mae: 6.9881e-04 - val_loss: 1.8916e-05 - val_mae: 0.0027 - lr: 1.5625e-05
Epoch 38/50
1559/1559 [==============================] - 74s 47ms/step - loss: 7.2979e-06 - mae: 6.7902e-04 - val_loss: 1.9804e-05 - val_mae: 0.0028 - lr: 7.8125e-06
Epoch 39/50
1559/1559 [==============================] - 74s 47ms/step - loss: 7.2566e-06 - mae: 6.8495e-04 - val_loss: 2.0626e-05 - val_mae: 0.0031 - lr: 7.8125e-06
Epoch 40/50
1559/1559 [==============================] - 75s 48ms/step - loss: 7.3443e-06 - mae: 6.9828e-04 - val_loss: 1.9083e-05 - val_mae: 0.0027 - lr: 7.8125e-06
Epoch 41/50
1559/1559 [==============================] - 80s 51ms/step - loss: 7.2670e-06 - mae: 6.8319e-04 - val_loss: 1.8702e-05 - val_mae: 0.0027 - lr: 7.8125e-06
Epoch 42/50
1559/1559 [==============================] - 78s 50ms/step - loss: 7.3019e-06 - mae: 6.8552e-04 - val_loss: 2.2296e-05 - val_mae: 0.0031 - lr: 7.8125e-06
Epoch 43/50
1559/1559 [==============================] - 74s 48ms/step - loss: 7.2458e-06 - mae: 6.8279e-04 - val_loss: 1.9051e-05 - val_mae: 0.0027 - lr: 3.9063e-06
Epoch 44/50
1559/1559 [==============================] - 78s 50ms/step - loss: 7.1887e-06 - mae: 6.7490e-04 - val_loss: 1.8668e-05 - val_mae: 0.0027 - lr: 3.9063e-06
Epoch 45/50
1559/1559 [==============================] - 76s 48ms/step - loss: 7.1632e-06 - mae: 6.7174e-04 - val_loss: 1.9244e-05 - val_mae: 0.0027 - lr: 3.9063e-06
Epoch 46/50
1559/1559 [==============================] - 76s 49ms/step - loss: 7.1705e-06 - mae: 6.7561e-04 - val_loss: 1.8650e-05 - val_mae: 0.0027 - lr: 3.9063e-06
Epoch 47/50
1559/1559 [==============================] - 76s 49ms/step - loss: 7.1914e-06 - mae: 6.7176e-04 - val_loss: 1.8793e-05 - val_mae: 0.0027 - lr: 3.9063e-06
Epoch 48/50
1559/1559 [==============================] - 75s 48ms/step - loss: 7.1590e-06 - mae: 6.7776e-04 - val_loss: 1.8695e-05 - val_mae: 0.0027 - lr: 1.9531e-06
Epoch 49/50
1559/1559 [==============================] - 78s 50ms/step - loss: 7.1492e-06 - mae: 6.6665e-04 - val_loss: 1.8797e-05 - val_mae: 0.0027 - lr: 1.9531e-06
Epoch 50/50
1559/1559 [==============================] - 76s 49ms/step - loss: 7.1680e-06 - mae: 6.6962e-04 - val_loss: 1.9135e-05 - val_mae: 0.0028 - lr: 1.9531e-06
434/434 [==============================] - 9s 19ms/step
Scaled MSE: 0.000016
Scaled RMSE: 0.004038
Scaled MAE: 0.002317

Metrics on actual prices:
MSE: 5483.18
RMSE: 74.05
MAE: 42.49
MAPE: 0.57%
Directional Accuracy: 45.23%
/home/blackrukh/.local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
Bitcoin price forecasting completed!