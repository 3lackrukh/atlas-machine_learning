blackrukh@3lackrukh:~/atlas-machine_learning/supervised_learning/time_series$ ./forecast_btc.py 
2025-03-24 13:47:30.754690: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-24 13:47:30.754813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-24 13:47:30.885001: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-24 13:47:31.150198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-24 13:47:33.246617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Bitcoin Price Forecasting with RNNs
=================================

Loaded data: X shape (68412, 24, 13), y shape (68412,)
Train set: (49256, 24, 13), (49256,)
Validation set: (5473, 24, 13), (5473,)
Test set: (13683, 24, 13), (13683,)
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 24, 128)           72704     
                                                                 
 dropout (Dropout)           (None, 24, 128)           0         
                                                                 
 lstm_1 (LSTM)               (None, 64)                49408     
                                                                 
 dropout_1 (Dropout)         (None, 64)                0         
                                                                 
 dense (Dense)               (None, 32)                2080      
                                                                 
 dense_1 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 124225 (485.25 KB)
Trainable params: 124225 (485.25 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/50
1540/1540 [==============================] - 56s 35ms/step - loss: 0.0029 - mae: 0.0131 - val_loss: 0.0040 - val_mae: 0.0593 - lr: 0.0010
Epoch 2/50
1540/1540 [==============================] - 56s 37ms/step - loss: 4.6374e-04 - mae: 0.0111 - val_loss: 0.0044 - val_mae: 0.0618 - lr: 0.0010
Epoch 3/50
1540/1540 [==============================] - 57s 37ms/step - loss: 3.8598e-04 - mae: 0.0095 - val_loss: 0.0031 - val_mae: 0.0512 - lr: 0.0010
Epoch 4/50
1540/1540 [==============================] - 55s 36ms/step - loss: 2.3743e-04 - mae: 0.0079 - val_loss: 0.0045 - val_mae: 0.0629 - lr: 0.0010
Epoch 5/50
1540/1540 [==============================] - 62s 40ms/step - loss: 2.0098e-04 - mae: 0.0070 - val_loss: 0.0052 - val_mae: 0.0671 - lr: 0.0010
Epoch 6/50
1540/1540 [==============================] - 59s 39ms/step - loss: 1.6884e-04 - mae: 0.0063 - val_loss: 0.0049 - val_mae: 0.0657 - lr: 0.0010
Epoch 7/50
1540/1540 [==============================] - 58s 37ms/step - loss: 1.6553e-04 - mae: 0.0063 - val_loss: 0.0030 - val_mae: 0.0466 - lr: 0.0010
Epoch 8/50
1540/1540 [==============================] - 62s 40ms/step - loss: 1.4033e-04 - mae: 0.0057 - val_loss: 0.0042 - val_mae: 0.0592 - lr: 0.0010
Epoch 9/50
1540/1540 [==============================] - 57s 37ms/step - loss: 1.4032e-04 - mae: 0.0056 - val_loss: 0.0025 - val_mae: 0.0427 - lr: 0.0010
Epoch 10/50
1540/1540 [==============================] - 55s 36ms/step - loss: 1.2919e-04 - mae: 0.0053 - val_loss: 0.0064 - val_mae: 0.0758 - lr: 0.0010
Epoch 11/50
1540/1540 [==============================] - 57s 37ms/step - loss: 8.1296e-05 - mae: 0.0044 - val_loss: 0.0029 - val_mae: 0.0496 - lr: 0.0010
Epoch 12/50
1540/1540 [==============================] - 84s 55ms/step - loss: 1.1176e-04 - mae: 0.0050 - val_loss: 0.0035 - val_mae: 0.0548 - lr: 0.0010
Epoch 13/50
1540/1540 [==============================] - 54s 35ms/step - loss: 7.7170e-05 - mae: 0.0043 - val_loss: 0.0023 - val_mae: 0.0419 - lr: 0.0010
Epoch 14/50
1540/1540 [==============================] - 54s 35ms/step - loss: 6.8740e-05 - mae: 0.0039 - val_loss: 0.0038 - val_mae: 0.0587 - lr: 0.0010
Epoch 15/50
1540/1540 [==============================] - 54s 35ms/step - loss: 6.9520e-05 - mae: 0.0039 - val_loss: 0.0024 - val_mae: 0.0440 - lr: 0.0010
Epoch 16/50
1540/1540 [==============================] - 54s 35ms/step - loss: 7.0408e-05 - mae: 0.0039 - val_loss: 0.0021 - val_mae: 0.0398 - lr: 0.0010
Epoch 17/50
1540/1540 [==============================] - 56s 37ms/step - loss: 5.7098e-05 - mae: 0.0036 - val_loss: 0.0025 - val_mae: 0.0463 - lr: 0.0010
Epoch 18/50
1540/1540 [==============================] - 63s 41ms/step - loss: 5.7831e-05 - mae: 0.0034 - val_loss: 0.0017 - val_mae: 0.0364 - lr: 0.0010
Epoch 19/50
1540/1540 [==============================] - 66s 43ms/step - loss: 6.3028e-05 - mae: 0.0035 - val_loss: 0.0028 - val_mae: 0.0475 - lr: 0.0010
Epoch 20/50
1540/1540 [==============================] - 60s 39ms/step - loss: 6.5493e-05 - mae: 0.0035 - val_loss: 5.9785e-04 - val_mae: 0.0172 - lr: 0.0010
Epoch 21/50
1540/1540 [==============================] - 58s 38ms/step - loss: 6.7220e-05 - mae: 0.0038 - val_loss: 6.7806e-04 - val_mae: 0.0182 - lr: 0.0010
Epoch 22/50
1540/1540 [==============================] - 54s 35ms/step - loss: 5.0497e-05 - mae: 0.0034 - val_loss: 9.2077e-04 - val_mae: 0.0247 - lr: 0.0010
Epoch 23/50
1540/1540 [==============================] - 54s 35ms/step - loss: 5.3389e-05 - mae: 0.0033 - val_loss: 6.5212e-04 - val_mae: 0.0193 - lr: 0.0010
Epoch 24/50
1540/1540 [==============================] - 54s 35ms/step - loss: 5.1795e-05 - mae: 0.0033 - val_loss: 9.9228e-04 - val_mae: 0.0246 - lr: 0.0010
Epoch 25/50
1540/1540 [==============================] - 54s 35ms/step - loss: 5.1503e-05 - mae: 0.0032 - val_loss: 9.1662e-04 - val_mae: 0.0239 - lr: 0.0010
Epoch 26/50
1540/1540 [==============================] - 54s 35ms/step - loss: 4.5881e-05 - mae: 0.0032 - val_loss: 9.1499e-04 - val_mae: 0.0239 - lr: 5.0000e-04
Epoch 27/50
1540/1540 [==============================] - 54s 35ms/step - loss: 3.5221e-05 - mae: 0.0028 - val_loss: 9.1228e-04 - val_mae: 0.0239 - lr: 5.0000e-04
Epoch 28/50
1540/1540 [==============================] - 55s 35ms/step - loss: 3.1306e-05 - mae: 0.0026 - val_loss: 0.0018 - val_mae: 0.0373 - lr: 5.0000e-04
Epoch 29/50
1540/1540 [==============================] - 54s 35ms/step - loss: 3.2562e-05 - mae: 0.0026 - val_loss: 0.0017 - val_mae: 0.0364 - lr: 5.0000e-04
Epoch 30/50
1540/1540 [==============================] - 54s 35ms/step - loss: 3.1983e-05 - mae: 0.0026 - val_loss: 0.0015 - val_mae: 0.0338 - lr: 5.0000e-04
428/428 [==============================] - 5s 12ms/step
Scaled MSE: 0.000904
Scaled RMSE: 0.030075
Scaled MAE: 0.022802
Original MSE: 304770.58
Original RMSE: 552.06
Original MAE: 418.57
/home/blackrukh/.local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
Bitcoin price forecasting completed!