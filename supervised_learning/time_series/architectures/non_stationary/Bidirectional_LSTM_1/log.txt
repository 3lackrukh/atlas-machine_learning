./forecast_btc.py 
2025-03-25 13:30:16.785425: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-25 13:30:16.785596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-25 13:30:16.788083: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-25 13:30:16.802862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-25 13:30:17.942009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Bitcoin Price Forecasting with RNNs
=================================

Loaded data: X shape (68412, 24, 13), y shape (68412,)
Train set: (49256, 24, 13), (49256,)
Validation set: (5473, 24, 13), (5473,)
Test set: (13683, 24, 13), (13683,)
2025-03-25 13:30:20.177921: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 122942976 exceeds 10% of free system memory.
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 bidirectional (Bidirection  (None, 2)                 120       
 al)                                                             
                                                                 
 dense (Dense)               (None, 1)                 3         
                                                                 
=================================================================
Total params: 123 (492.00 Byte)
Trainable params: 123 (492.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
2025-03-25 13:30:20.741967: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 122942976 exceeds 10% of free system memory.
Epoch 1/50
1540/1540 [==============================] - 27s 16ms/step - loss: 0.0035 - mae: 0.0172 - val_loss: 0.0368 - val_mae: 0.1782 - lr: 0.0010
Epoch 2/50
1540/1540 [==============================] - 25s 16ms/step - loss: 0.0225 - mae: 0.0753 - val_loss: 0.0056 - val_mae: 0.0592 - lr: 0.0010
Epoch 3/50
1540/1540 [==============================] - 27s 18ms/step - loss: 0.0119 - mae: 0.0462 - val_loss: 0.0032 - val_mae: 0.0459 - lr: 0.0010
Epoch 4/50
1540/1540 [==============================] - 28s 18ms/step - loss: 0.0071 - mae: 0.0309 - val_loss: 0.0024 - val_mae: 0.0400 - lr: 0.0010
Epoch 5/50
1540/1540 [==============================] - 26s 17ms/step - loss: 0.0021 - mae: 0.0199 - val_loss: 4.2741e-04 - val_mae: 0.0156 - lr: 0.0010
Epoch 6/50
1540/1540 [==============================] - 26s 17ms/step - loss: 6.5369e-04 - mae: 0.0130 - val_loss: 5.7615e-04 - val_mae: 0.0202 - lr: 0.0010
Epoch 7/50
1540/1540 [==============================] - 27s 18ms/step - loss: 2.5978e-04 - mae: 0.0086 - val_loss: 2.6116e-04 - val_mae: 0.0124 - lr: 0.0010
Epoch 8/50
1540/1540 [==============================] - 25s 16ms/step - loss: 1.3550e-04 - mae: 0.0062 - val_loss: 3.8251e-04 - val_mae: 0.0164 - lr: 0.0010
Epoch 9/50
1540/1540 [==============================] - 25s 16ms/step - loss: 9.8143e-05 - mae: 0.0049 - val_loss: 2.7889e-04 - val_mae: 0.0135 - lr: 0.0010
Epoch 10/50
1540/1540 [==============================] - 25s 16ms/step - loss: 7.8711e-05 - mae: 0.0040 - val_loss: 3.3481e-04 - val_mae: 0.0156 - lr: 0.0010
Epoch 11/50
1540/1540 [==============================] - 25s 16ms/step - loss: 6.8037e-05 - mae: 0.0033 - val_loss: 2.4963e-04 - val_mae: 0.0130 - lr: 0.0010
Epoch 12/50
1540/1540 [==============================] - 25s 16ms/step - loss: 6.2809e-05 - mae: 0.0030 - val_loss: 5.6431e-04 - val_mae: 0.0213 - lr: 0.0010
Epoch 13/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.9177e-05 - mae: 0.0028 - val_loss: 2.1352e-04 - val_mae: 0.0119 - lr: 5.0000e-04
Epoch 14/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.9404e-05 - mae: 0.0026 - val_loss: 3.0200e-04 - val_mae: 0.0148 - lr: 5.0000e-04
Epoch 15/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.8119e-05 - mae: 0.0025 - val_loss: 1.6006e-04 - val_mae: 0.0100 - lr: 5.0000e-04
Epoch 16/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.6297e-05 - mae: 0.0025 - val_loss: 1.3945e-04 - val_mae: 0.0088 - lr: 5.0000e-04
Epoch 17/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.6696e-05 - mae: 0.0025 - val_loss: 1.8475e-04 - val_mae: 0.0111 - lr: 5.0000e-04
Epoch 18/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.4817e-05 - mae: 0.0024 - val_loss: 2.1633e-04 - val_mae: 0.0122 - lr: 5.0000e-04
Epoch 19/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.4282e-05 - mae: 0.0025 - val_loss: 3.3371e-04 - val_mae: 0.0159 - lr: 5.0000e-04
Epoch 20/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.5322e-05 - mae: 0.0024 - val_loss: 3.2389e-04 - val_mae: 0.0157 - lr: 5.0000e-04
Epoch 21/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.3944e-05 - mae: 0.0024 - val_loss: 1.8523e-04 - val_mae: 0.0111 - lr: 2.5000e-04
Epoch 22/50
1540/1540 [==============================] - 26s 17ms/step - loss: 5.2437e-05 - mae: 0.0023 - val_loss: 3.1917e-04 - val_mae: 0.0156 - lr: 2.5000e-04
Epoch 23/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.3875e-05 - mae: 0.0024 - val_loss: 1.3412e-04 - val_mae: 0.0088 - lr: 2.5000e-04
Epoch 24/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.2213e-05 - mae: 0.0022 - val_loss: 2.9834e-04 - val_mae: 0.0150 - lr: 2.5000e-04
Epoch 25/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.2461e-05 - mae: 0.0023 - val_loss: 1.1463e-04 - val_mae: 0.0076 - lr: 2.5000e-04
Epoch 26/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.1254e-05 - mae: 0.0022 - val_loss: 1.3832e-04 - val_mae: 0.0091 - lr: 1.2500e-04
Epoch 27/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.1261e-05 - mae: 0.0023 - val_loss: 1.3984e-04 - val_mae: 0.0092 - lr: 1.2500e-04
Epoch 28/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.1437e-05 - mae: 0.0022 - val_loss: 1.4493e-04 - val_mae: 0.0095 - lr: 1.2500e-04
Epoch 29/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.1331e-05 - mae: 0.0022 - val_loss: 1.5625e-04 - val_mae: 0.0100 - lr: 1.2500e-04
Epoch 30/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.1047e-05 - mae: 0.0022 - val_loss: 1.2061e-04 - val_mae: 0.0082 - lr: 1.2500e-04
Epoch 31/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.1417e-05 - mae: 0.0023 - val_loss: 1.4148e-04 - val_mae: 0.0094 - lr: 6.2500e-05
Epoch 32/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.0176e-05 - mae: 0.0022 - val_loss: 1.6380e-04 - val_mae: 0.0104 - lr: 6.2500e-05
Epoch 33/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.0599e-05 - mae: 0.0022 - val_loss: 1.4721e-04 - val_mae: 0.0096 - lr: 6.2500e-05
Epoch 34/50
1540/1540 [==============================] - 25s 16ms/step - loss: 5.0541e-05 - mae: 0.0022 - val_loss: 1.6147e-04 - val_mae: 0.0103 - lr: 6.2500e-05
Epoch 35/50
1540/1540 [==============================] - 25s 16ms/step - loss: 4.9864e-05 - mae: 0.0022 - val_loss: 1.7352e-04 - val_mae: 0.0108 - lr: 6.2500e-05
428/428 [==============================] - 3s 7ms/step
Scaled MSE: 0.000141
Scaled RMSE: 0.011865
Scaled MAE: 0.007329
Original MSE: 47432.34
Original RMSE: 217.79
Original MAE: 134.54
/home/blackrukh/.local/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
Bitcoin price forecasting completed!