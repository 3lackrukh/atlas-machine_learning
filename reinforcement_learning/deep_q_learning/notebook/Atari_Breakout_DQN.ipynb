{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","mount_file_id":"17cJrvUsLqvT8gfYKb9MzODCVVJDwLGUg","authorship_tag":"ABX9TyOPVuNgpZY1IFkSgK7pQUfI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Atari Breakout DQN Training"],"metadata":{"id":"pbVlevTOa4F4"}},{"cell_type":"markdown","source":["### 1. Dependency Setup"],"metadata":{"id":"4-MDCROYLTbm"}},{"cell_type":"code","source":["!pip install numpy==1.25.2\n","!pip install tensorflow==2.15.0\n","!pip install keras==2.15.0\n","!pip install h5py==3.11.0\n","!pip install pillow==10.3.0\n","!pip install gymnasium[atari]==0.29.1\n","!pip install keras-rl2==1.0.4\n","!pip install autorom[accept-rom-license]\n","!AutoROM --accept-license\n","!echo \"y\" | AutoROM --accept-license > /dev/null\n","\n","# Force restart\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":4667},"id":"IzJKSgMrTycz","executionInfo":{"status":"ok","timestamp":1747568928292,"user_tz":300,"elapsed":79452,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}},"outputId":"e5a0823b-7e42-47e7-9c3d-0e0b2435d12b","collapsed":true},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.25.2\n","  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n","blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.25.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"a3cbf0c435f546c094e7684ee5411e35"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.15.0\n","  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.13.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n","Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n","  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n","  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.13.2)\n","Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n","  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.71.0)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n","  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n","Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m124.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, ml-dtypes, keras, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.17.2\n","    Uninstalling wrapt-1.17.2:\n","      Successfully uninstalled wrapt-1.17.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.4\n","    Uninstalling protobuf-5.29.4:\n","      Successfully uninstalled protobuf-5.29.4\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.4.1\n","    Uninstalling ml-dtypes-0.4.1:\n","      Successfully uninstalled ml-dtypes-0.4.1\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.8.0\n","    Uninstalling keras-3.8.0:\n","      Successfully uninstalled keras-3.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.18.0\n","    Uninstalling tensorboard-2.18.0:\n","      Successfully uninstalled tensorboard-2.18.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.18.0\n","    Uninstalling tensorflow-2.18.0:\n","      Successfully uninstalled tensorflow-2.18.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n","ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n","tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 protobuf-4.25.7 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n","Requirement already satisfied: keras==2.15.0 in /usr/local/lib/python3.11/dist-packages (2.15.0)\n","Collecting h5py==3.11.0\n","  Downloading h5py-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from h5py==3.11.0) (1.25.2)\n","Downloading h5py-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h5py\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.13.0\n","    Uninstalling h5py-3.13.0:\n","      Successfully uninstalled h5py-3.13.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h5py-3.11.0\n","Collecting pillow==10.3.0\n","  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n","Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pillow\n","  Attempting uninstall: pillow\n","    Found existing installation: pillow 11.2.1\n","    Uninstalling pillow-11.2.1:\n","      Successfully uninstalled pillow-11.2.1\n","Successfully installed pillow-10.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]},"id":"a1ae992c96ff453fa66c2c88ae731369"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting gymnasium==0.29.1 (from gymnasium[atari]==0.29.1)\n","  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (4.13.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (0.0.4)\n","Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1)\n","  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n","Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1)\n","  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1) (6.5.2)\n","Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n","Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gymnasium, ale-py, shimmy\n","  Attempting uninstall: gymnasium\n","    Found existing installation: gymnasium 1.1.1\n","    Uninstalling gymnasium-1.1.1:\n","      Successfully uninstalled gymnasium-1.1.1\n","  Attempting uninstall: ale-py\n","    Found existing installation: ale-py 0.11.0\n","    Uninstalling ale-py-0.11.0:\n","      Successfully uninstalled ale-py-0.11.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n","dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed ale-py-0.8.1 gymnasium-0.29.1 shimmy-0.2.1\n","Collecting keras-rl2==1.0.4\n","  Downloading keras_rl2-1.0.4-py3-none-any.whl.metadata (353 bytes)\n","Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.4) (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (4.25.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (4.13.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (1.71.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.1.0->keras-rl2==1.0.4) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->keras-rl2==1.0.4) (0.45.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (2.38.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (1.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (3.8)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (4.9.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.1.0->keras-rl2==1.0.4) (3.2.2)\n","Downloading keras_rl2-1.0.4-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.4\n","Collecting autorom[accept-rom-license]\n","  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n","Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n","  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.4.26)\n","Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446710 sha256=f096c3e711395567eceec031502271bb0edecd3fcc6de46e4ea6654cc0121dc3\n","  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom\n","Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\n","AutoROM will download the Atari 2600 ROMs.\n","They will be installed to:\n","\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n","\n","Existing ROMs will be overwritten.\n","\n","I own a license to these Atari 2600 ROMs.\n","I agree to not distribute these ROMs and wish to proceed: [Y/n]: y\n"]},{"output_type":"execute_result","data":{"text/plain":["{'status': 'ok', 'restart': True}"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["### 2. keras-rl2 compatibility patch"],"metadata":{"id":"-LD_oTWObPdF"}},{"cell_type":"code","source":["\"\"\" This module fixes the compatibility issue between keras-rl2 and gymnasium \"\"\"\n","import os\n","import rl\n","\n","# Apply the patch to fix keras-rl2 compatibility\n","rl_path = os.path.dirname(rl.__file__)\n","callbacks_path = os.path.join(rl_path, 'callbacks.py')\n","with open(callbacks_path, 'r') as file:\n","    content = file.read()\n","fixed_content = content.replace(\n","    'from tensorflow.keras import __version__ as KERAS_VERSION',\n","    'from keras import __version__ as KERAS_VERSION'\n",")\n","with open(callbacks_path, 'w') as file:\n","    file.write(fixed_content)\n","print(\"✓ keras-rl2 compatibility patch applied\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"collapsed":true,"id":"qSlRNEbCcznI","executionInfo":{"status":"ok","timestamp":1747568936618,"user_tz":300,"elapsed":11,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}},"outputId":"58abcbe8-7a94-47a9-e219-4092abde36a1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ keras-rl2 compatibility patch applied\n"]}]},{"cell_type":"markdown","source":["### 3. Imports\n","\n"],"metadata":{"id":"dH_m7YCYpSxG"}},{"cell_type":"code","source":["# Import all necessary libraries for the rest of the notebook\n","import numpy as np\n","import sys\n","import re\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import time\n","import datetime\n","\n","# Imports from keras-rl2\n","import rl\n","from rl.processors import Processor\n","from rl.agents.dqn import DQNAgent\n","from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n","from rl.memory import SequentialMemory\n","from rl.callbacks import ModelIntervalCheckpoint\n","\n","# Imports from Keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Conv2D, Input\n","from keras.optimizers.legacy import Adam\n","from keras.callbacks import Callback\n","\n","# Imports from Gymnasium\n","import gymnasium as gym\n","from gymnasium.wrappers import AtariPreprocessing\n","\n","print(\"✓ All core libraries imported\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"uiPiRMsUpckB","executionInfo":{"status":"ok","timestamp":1747568942696,"user_tz":300,"elapsed":3322,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}},"outputId":"86192978-2de4-43de-8848-951fd9af8568"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ All core libraries imported\n"]}]},{"cell_type":"markdown","source":["### 4. Mount Google Drive (For Saving Models and logs)"],"metadata":{"id":"bVLqAjycb1OY"}},{"cell_type":"code","source":["# Mount Google Drive for saving models and logs\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"✓ Google Drive mounted\")\n","\n","# Create directory for saving models\n","if not os.path.exists('/content/drive/MyDrive/breakout_dqn/logs'):\n","    os.makedirs('/content/drive/MyDrive/breakout_dqn/logs')\n","    print(\"✓ Directory created for saving models and logs\")\n","\n","print(\"✓ All setup complete\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"collapsed":true,"id":"IKZPsJd6b9yq","executionInfo":{"status":"ok","timestamp":1747568956072,"user_tz":300,"elapsed":10732,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}},"outputId":"10351fec-c452-4b40-a361-c5529dfe2c73"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✓ Google Drive mounted\n","✓ All setup complete\n"]}]},{"cell_type":"markdown","source":["### 5. Implementation"],"metadata":{"id":"QqcFoB20dFqI"}},{"cell_type":"code","source":["class GymCompatibilityWrapper(gym.Wrapper):\n","    \"\"\"\n","    Wrapper to make gymnasium compatible with keras-rl while supporting reward shaping.\n","\n","    This wrapper bridges the Gymnasium API with keras-rl expectations and ensures\n","    the processor receives terminal state information for proper reward shaping.\n","    \"\"\"\n","    def __init__(self, env, processor=None):\n","        super().__init__(env)\n","        self.processor = processor\n","\n","    def step(self, action):\n","        \"\"\"\n","        Update step method to match keras-rl output and enhance reward shaping.\n","\n","        Adds 'done' flag to info dict when episode terminates, allowing the\n","        processor to apply end-of-episode reward adjustments.\n","        \"\"\"\n","        obs, reward, terminated, truncated, info = self.env.step(action)\n","        done = terminated or truncated\n","\n","        # Signal episode termination to processor\n","        if done and self.processor is not None:\n","            info['done'] = True\n","\n","        return obs, reward, done, info\n","\n","    def render(self):\n","        \"\"\"Update render method to match keras-rl\"\"\"\n","        return self.env.render()\n","\n","    def reset(self, **kwargs):\n","        \"\"\"Update reset method to match keras-rl output\"\"\"\n","        obs, _ = self.env.reset(**kwargs)\n","        return obs\n","\n","class AdaptiveRewardScaler:\n","    def __init__(self, target_min=-1.0, target_best=1.2, decay_factor=0.95, initial_best=1.0):\n","        \"\"\"\n","        Adaptive reward scaler that adjusts based on best performance.\n","\n","        Args:\n","            target_min: The minimum (negative) scaled reward\n","            target_best: The scaled reward for the best performance so far\n","            decay_factor: Factor to decay best_reward when resetting (0.95 means 5% decay)\n","            initial_best: Initial value for best_reward\n","        \"\"\"\n","        self.best_reward = initial_best\n","        self.target_min = target_min\n","        self.target_best = target_best\n","        self.decay_factor = decay_factor\n","\n","    def scale_reward(self, shaped_reward):\n","        \"\"\"\n","        Scale reward relative to best performance seen so far.\n","        \"\"\"\n","        # Update best_reward tracker if we see a new best\n","        if shaped_reward > self.best_reward:\n","            self.best_reward = shaped_reward\n","\n","        # Scale the reward\n","        if shaped_reward == 0:\n","            return 0\n","        elif shaped_reward > 0:\n","            # Scale positive rewards relative to best seen\n","            # This ensures the best reward gets target_best value\n","            return self.target_best * (shaped_reward / self.best_reward)\n","        else:\n","            # Scale negative rewards using fixed approach\n","            return self.target_min * min(shaped_reward / -1.0, 1.0)\n","\n","    def reset_on_target_update(self):\n","        \"\"\"\n","        Slightly decays the best reward to allow for scaling adjustment.\n","        \"\"\"\n","        self.best_reward = max(1.0, self.best_reward * self.decay_factor)\n","\n","class StackDimProcessor(Processor):\n","    \"\"\"\n","    Custom processor that resolves dimension mismatches and implements reward shaping.\n","    Reward shaping is designed to mimic human-like motivation in games:\n","    - Breaking bricks is the primary objective and main source of satisfaction\n","    - Dying after scoring feels more disappointing than dying without scoring\n","    - Surviving longer builds anticipation and makes failure more consequential\n","    These human-like motivational signals help the agent learn faster by providing\n","    a richer reward landscape while maintaining the proper incentive hierarchy.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.episode_steps = 0\n","        self.episode_rewards = 0\n","        self._is_terminal = False\n","        self.reward_scaler = AdaptiveRewardScaler(\n","            target_min=-1.0,\n","            target_best=1.2,\n","            decay_factor=0.95,\n","            initial_best=1.0\n","        )\n","\n","    def process_observation(self, observation):\n","        \"\"\"Return the observation as is\"\"\"\n","        return observation\n","\n","    def process_state_batch(self, batch):\n","        \"\"\"Fix dimension mismatch between environment obs and model inputs\"\"\"\n","        # If we have a 5D tensor (batch, window_length, height, width, channel)\n","        if len(batch.shape) == 5:\n","            # Get dimensions\n","            batch_size, window_length, height, width, channels = batch.shape\n","            # Reshape to (batch, height, width, window_length*channels)\n","            # This stacks the frames along the channel dimension\n","            return np.reshape(batch, (batch_size, height, width, window_length * channels))\n","        return batch\n","\n","    def process_reset(self, observation):\n","        \"\"\"Reset episode tracking when environment resets\"\"\"\n","        self.episode_steps = 0\n","        self.episode_rewards = 0\n","        self._is_terminal = False\n","        return observation\n","\n","    def process_reward(self, reward):\n","        \"\"\"\n","        Shape rewards to provide meaningful learning signals between sparse game rewards.\n","        Modified to ensure strong negative signal for deaths without creating perverse incentives.\n","        \"\"\"\n","        # Track accumulated rewards and steps\n","        self.episode_rewards += reward\n","        self.episode_steps += 1\n","\n","        # Base reward (from breaking bricks)\n","        shaped_reward = reward\n","\n","        # Terminal state detection (end of episode/life loss)\n","        if hasattr(self, '_is_terminal') and self._is_terminal:\n","            # Fixed penalty for all deaths: -0.5\n","            # This creates a consistent, strong negative signal that doesn't\n","            # penalize scoring behavior\n","            end_adjustment = -0.5\n","\n","            # Optional: Small bonus for lasting longer (but still keeping net negative)\n","            survival_factor = min(1.0, self.episode_steps / 500)\n","            survival_bonus = 0.1 * survival_factor\n","\n","            # Final adjustment is still negative but rewards survival\n","            end_adjustment += survival_bonus  # At most reduces penalty to -0.4\n","\n","            shaped_reward += end_adjustment\n","\n","            # Reset episode tracking\n","            self.episode_steps = 0\n","            self.episode_rewards = 0\n","            self._is_terminal = False\n","\n","        # Use adaptive scaling instead of clipping\n","        return self.reward_scaler.scale_reward(shaped_reward)\n","\n","    def process_info(self, info):\n","        \"\"\"Process game information to detect episode termination.\"\"\"\n","        # Track terminal state for next reward processing\n","        if 'done' in info and info['done']:\n","            self._is_terminal = True\n","        return info\n","\n","\n","class EpisodicTargetNetworkUpdate(Callback):\n","    \"\"\"\n","    Custom callback to update the target network after a specific number of episodes.\n","    This overrides the default step-based update mechanism in DQNAgent.\n","    \"\"\"\n","    def __init__(self, update_frequency=10, verbose=0):\n","        \"\"\"\n","        Args:\n","            update_frequency: Number of episodes between target network updates\n","            verbose: Verbosity level (0=silent, 1=progress bar, 2=one line per epoch)\n","        \"\"\"\n","        super(EpisodicTargetNetworkUpdate, self).__init__()\n","        self.update_frequency = update_frequency\n","        self.episodes_since_update = 0\n","        self.verbose = verbose\n","\n","    def on_episode_end(self, episode, logs={}):\n","        \"\"\"Called at the end of each episode.\"\"\"\n","        self.episodes_since_update += 1\n","\n","        # Check if it's time to update the target network\n","        if self.episodes_since_update >= self.update_frequency:\n","            # Update target network by manually copying weights\n","            # In keras-rl2, we need to directly access and update the target model weights\n","            target_weights = self.model.target_model.get_weights()\n","            online_weights = self.model.model.get_weights()\n","\n","            # Manual update\n","            for i in range(len(target_weights)):\n","                target_weights[i] = online_weights[i]\n","\n","            # Set the updated weights\n","            self.model.target_model.set_weights(target_weights)\n","\n","            # Also update reward scaler if processor has one\n","            if hasattr(self.model.processor, 'reward_scaler'):\n","                self.model.processor.reward_scaler.reset_on_target_update()\n","\n","            # Reset counter\n","            self.episodes_since_update = 0\n","\n","            if self.verbose >= 1:\n","                print(f\"\\nTarget network updated after {self.update_frequency} episodes\")\n","\n","\n","def make_env(env_id):\n","    \"\"\"\n","    Creates a wrapped Atari environment with reward shaping for faster learning.\n","\n","    The environment includes human-like motivational signals that help\n","    the agent learn from sparse rewards by providing a richer feedback landscape.\n","    \"\"\"\n","    env = gym.make(env_id)\n","\n","    # Apply Atari preprocessing\n","\n","    # NOTE:\n","\n","    # Setting noop_max=7 strikes a balance for Breakout:\n","    # Based on rate of movement for the paddle and ball; enough randomization to\n","    # avoid fixed starting paddle positions that could create unwinnable ball\n","    # trajectories (which would disproportionately penalize the agent with negative\n","    # learning signals), while keeping training efficient.\n","    env = AtariPreprocessing(\n","        env,\n","        noop_max=7,\n","        frame_skip=4,\n","        screen_size=84,\n","        terminal_on_life_loss=True,  # End episode on life loss\n","        grayscale_obs=True,\n","        grayscale_newaxis=True,\n","        scale_obs=False,\n","    )\n","\n","    # Create processor for dimension handling and reward shaping\n","    processor = StackDimProcessor()\n","\n","    # Make compatible with keras-rl, passing the processor reference\n","    env = GymCompatibilityWrapper(env, processor)\n","\n","    return env, processor\n","\n","def model_template(state_shape, n_actions):\n","    \"\"\"Defines the DQN model architecture for policy and target networks\"\"\"\n","    model = Sequential()\n","    model.add(Input(shape=state_shape))\n","    model.add(Conv2D(32, (8, 8), strides=4, activation='relu'))\n","    model.add(Conv2D(64, (4, 4), strides=2, activation='relu'))\n","    model.add(Conv2D(64, (3, 3), strides=1, activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dense(n_actions, activation='linear'))\n","    return model"],"metadata":{"id":"ricg10VwdKi5","executionInfo":{"status":"ok","timestamp":1747568964037,"user_tz":300,"elapsed":31,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### 6. Training Monitor"],"metadata":{"id":"dUpYclsWe7-k"}},{"cell_type":"code","source":["# Training Monitor for efficient Progress Tracking\n","class TrainingMonitor:\n","    \"\"\"\n","    Memory-efficient training monitor using rolling statistics.\n","\n","    Tracks metrics using running averages and periodic sampling\n","    to minimize memory usage and computational overhead.\n","    \"\"\"\n","    def __init__(self, log_dir='/content/drive/MyDrive/breakout_dqn/logs',\n","                 window_size=100, log_interval=10):\n","        self.log_dir = log_dir\n","        self.window_size = window_size\n","        self.log_interval = log_interval  # Only log every N episodes to CSV\n","        os.makedirs(log_dir, exist_ok=True)\n","\n","        # Rolling windows for recent metrics (fixed memory usage)\n","        self.reward_window = deque(maxlen=window_size)\n","        self.length_window = deque(maxlen=window_size)\n","        self.q_window = deque(maxlen=window_size)\n","        self.loss_window = deque(maxlen=window_size)\n","        self.sps_window = deque(maxlen=window_size)\n","\n","        # Running statistics (constant memory regardless of training length)\n","        self.episode_count = 0\n","        self.total_steps = 0\n","        self.max_reward = float('-inf')\n","        self.max_reward_episode = 0\n","\n","        # For checkpoint statistics\n","        self.checkpoint_episodes = []\n","        self.checkpoint_rewards = []\n","        self.checkpoint_steps = []\n","        self.checkpoint_q_values = []\n","\n","        # Timing\n","        self.last_checkpoint_time = time.time()\n","        self.last_checkpoint_steps = 0\n","\n","        # Create the CSV log file\n","        self.log_file = os.path.join(log_dir, 'training_log.csv')\n","        self.create_log_file()\n","\n","    def create_log_file(self):\n","        \"\"\"Initialize the CSV log file with headers\"\"\"\n","        with open(self.log_file, 'w') as f:\n","            f.write('episode,total_steps,reward,length,duration,loss,mean_q,epsilon,steps_per_second\\n')\n","\n","    def on_episode_end(self, episode, logs):\n","        \"\"\"Record metrics at the end of each episode using efficient rolling stats\"\"\"\n","        # Extract metrics\n","        reward = logs.get('episode_reward', 0)\n","        steps = logs.get('nb_steps', 0)\n","        duration = logs.get('duration', 0)\n","        loss = logs.get('loss', None)\n","        mean_q = logs.get('mean_q', None)\n","        epsilon = logs.get('mean_eps', None)\n","        sps = steps / max(duration, 0.001)  # Steps per second\n","\n","        # Update counters\n","        self.episode_count += 1\n","        self.total_steps += steps\n","\n","        # Update rolling windows (fixed memory usage)\n","        self.reward_window.append(reward)\n","        self.length_window.append(steps)\n","        self.sps_window.append(sps)\n","\n","        if loss is not None:\n","            self.loss_window.append(loss)\n","        if mean_q is not None:\n","            self.q_window.append(mean_q)\n","\n","        # Track maximum reward\n","        if reward > self.max_reward:\n","            self.max_reward = reward\n","            self.max_reward_episode = self.episode_count\n","\n","        # Log to CSV periodically (not every episode)\n","        if self.episode_count % self.log_interval == 0:\n","            with open(self.log_file, 'a') as f:\n","                f.write(f'{self.episode_count},{self.total_steps},{reward},{steps},{duration},{loss},{mean_q},{epsilon},{sps}\\n')\n","\n","    def on_checkpoint(self, step_count):\n","        \"\"\"Generate summary visualizations at checkpoint intervals with minimal data\"\"\"\n","        # Calculate performance\n","        now = time.time()\n","        time_elapsed = now - self.last_checkpoint_time\n","        steps_done = step_count - self.last_checkpoint_steps\n","        steps_per_sec = steps_done / max(time_elapsed, 0.001)\n","\n","        # Store checkpoint metrics (minimal data points)\n","        self.checkpoint_episodes.append(self.episode_count)\n","        self.checkpoint_rewards.append(self._get_window_avg(self.reward_window))\n","        self.checkpoint_steps.append(step_count)\n","        if self.q_window:\n","            self.checkpoint_q_values.append(self._get_window_avg(self.q_window))\n","\n","        # Create timestamp for files\n","        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","        # Create visualizations\n","        self.generate_plots(step_count, timestamp)\n","\n","        # Generate summary statistics\n","        summary = self.generate_summary(step_count, steps_per_sec, time_elapsed)\n","\n","        # Update checkpoint tracking\n","        self.last_checkpoint_time = now\n","        self.last_checkpoint_steps = step_count\n","\n","        return summary\n","\n","    def _get_window_avg(self, window):\n","        \"\"\"Compute average of a window deque efficiently\"\"\"\n","        if not window:\n","            return 0\n","        return sum(window) / len(window)\n","\n","    def generate_plots(self, step_count, timestamp):\n","        \"\"\"Create visualization plots using only checkpoint data and current windows\"\"\"\n","        # Create figure with multiple subplots\n","        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n","\n","        # Plot 1: Episode rewards (using checkpoints and current window)\n","        ax = axes[0, 0]\n","        # Plot checkpoint data (sparse historical data)\n","        if self.checkpoint_rewards:\n","            ax.plot(self.checkpoint_episodes, self.checkpoint_rewards, 'b-o', label='Checkpoint Avg')\n","\n","        # Plot recent episodes in detail (from rolling window)\n","        recent_indices = list(range(self.episode_count - len(self.reward_window) + 1,\n","                                   self.episode_count + 1))\n","        ax.plot(recent_indices, list(self.reward_window), 'g-', alpha=0.5, label='Recent Episodes')\n","\n","        ax.set_title(f'Episode Rewards (Max: {self.max_reward} at #{self.max_reward_episode})')\n","        ax.set_xlabel('Episode')\n","        ax.set_ylabel('Reward')\n","        ax.grid(True)\n","        if self.checkpoint_rewards or self.reward_window:\n","            ax.legend()\n","\n","        # Plot 2: Episode lengths (current window only)\n","        ax = axes[0, 1]\n","        if self.length_window:\n","            ax.plot(recent_indices, list(self.length_window))\n","            ax.set_title(f'Recent Episode Lengths (Avg: {self._get_window_avg(self.length_window):.1f})')\n","            ax.set_xlabel('Episode')\n","            ax.set_ylabel('Steps')\n","            ax.grid(True)\n","        else:\n","            ax.text(0.5, 0.5, 'No episode length data available',\n","                   horizontalalignment='center', verticalalignment='center')\n","\n","        # Plot 3: Mean Q-values (checkpoints + current window)\n","        ax = axes[1, 0]\n","        if self.checkpoint_q_values:\n","            ax.plot(self.checkpoint_episodes[len(self.checkpoint_episodes)-len(self.checkpoint_q_values):],\n","                   self.checkpoint_q_values, 'b-o', label='Checkpoint Avg')\n","\n","        if self.q_window:\n","            ax.plot(recent_indices, list(self.q_window), 'g-', alpha=0.5, label='Recent Episodes')\n","            ax.set_title(f'Mean Q-Values (Recent Avg: {self._get_window_avg(self.q_window):.4f})')\n","            ax.set_xlabel('Episode')\n","            ax.set_ylabel('Q-Value')\n","            ax.grid(True)\n","            ax.legend()\n","        else:\n","            ax.text(0.5, 0.5, 'No Q-values recorded yet\\n(still in warmup phase)',\n","                   horizontalalignment='center', verticalalignment='center')\n","            ax.set_title('Mean Q-Values (Not Available)')\n","\n","        # Plot 4: Training progress (steps vs episodes)\n","        ax = axes[1, 1]\n","        if self.checkpoint_episodes:\n","            ax.plot(self.checkpoint_episodes, self.checkpoint_steps, 'b-o')\n","            ax.set_title('Training Progress')\n","            ax.set_xlabel('Episodes')\n","            ax.set_ylabel('Total Steps')\n","            ax.grid(True)\n","\n","            # Add second y-axis for SPS\n","            if self.sps_window:\n","                ax2 = ax.twinx()\n","                ax2.plot(recent_indices, list(self.sps_window), 'r-', alpha=0.5)\n","                ax2.set_ylabel('Steps/Second', color='r')\n","                ax2.tick_params(axis='y', labelcolor='r')\n","        else:\n","            ax.text(0.5, 0.5, 'No checkpoint data available yet',\n","                   horizontalalignment='center', verticalalignment='center')\n","\n","        plt.tight_layout()\n","\n","        # Save the figure\n","        plt.savefig(os.path.join(self.log_dir, f'training_progress_{step_count}_{timestamp}.png'))\n","        plt.close()\n","\n","    def generate_summary(self, step_count, steps_per_sec, time_elapsed):\n","        \"\"\"Generate checkpoint summary statistics from rolling windows\"\"\"\n","        # Summary statistics use only current windows (constant memory)\n","        avg_reward = self._get_window_avg(self.reward_window)\n","        avg_length = self._get_window_avg(self.length_window)\n","        avg_q = self._get_window_avg(self.q_window) if self.q_window else None\n","\n","        # Return formatted summary\n","        summary = {\n","            'step_count': step_count,\n","            'episodes_completed': self.episode_count,\n","            'avg_reward_recent': avg_reward,\n","            'max_reward_all_time': self.max_reward,\n","            'avg_episode_length': avg_length,\n","            'steps_per_second': steps_per_sec,\n","            'time_elapsed_minutes': time_elapsed / 60\n","        }\n","\n","        if avg_q is not None:\n","            summary['avg_q_value'] = avg_q\n","\n","        # Save summary to file\n","        with open(os.path.join(self.log_dir, f'summary_{step_count}.txt'), 'w') as f:\n","            for key, value in summary.items():\n","                f.write(f\"{key}: {value}\\n\")\n","\n","        return summary\n","\n","    def compare_runs(self, other_log_file, output_path=None):\n","        \"\"\"Compare current run with another training run\"\"\"\n","        try:\n","            # Load just the necessary data from CSV files (memory efficient)\n","            current_df = pd.read_csv(self.log_file, usecols=['episode', 'reward'])\n","            other_df = pd.read_csv(other_log_file, usecols=['episode', 'reward'])\n","\n","            # Calculate rolling averages\n","            window = min(100, len(current_df), len(other_df))\n","            current_df['reward_avg'] = current_df['reward'].rolling(window=window, min_periods=1).mean()\n","            other_df['reward_avg'] = other_df['reward'].rolling(window=window, min_periods=1).mean()\n","\n","            # Create comparison plot\n","            plt.figure(figsize=(10, 6))\n","            plt.plot(current_df['episode'], current_df['reward_avg'], 'b-', linewidth=2, label='Current Run')\n","            plt.plot(other_df['episode'], other_df['reward_avg'], 'r-', linewidth=2, label='Comparison Run')\n","            plt.title('Training Strategy Comparison')\n","            plt.xlabel('Episode')\n","            plt.ylabel(f'Avg Reward ({window} ep window)')\n","            plt.legend()\n","            plt.grid(True)\n","\n","            if output_path:\n","                plt.savefig(output_path)\n","            plt.show()\n","\n","            # Return statistics for comparison\n","            return {\n","                'current_run': {\n","                    'episodes': len(current_df),\n","                    'avg_reward': current_df['reward'].mean(),\n","                    'max_reward': current_df['reward'].max(),\n","                    'final_avg': current_df['reward_avg'].iloc[-1] if not current_df.empty else 0\n","                },\n","                'comparison_run': {\n","                    'episodes': len(other_df),\n","                    'avg_reward': other_df['reward'].mean(),\n","                    'max_reward': other_df['reward'].max(),\n","                    'final_avg': other_df['reward_avg'].iloc[-1] if not other_df.empty else 0\n","                }\n","            }\n","\n","        except Exception as e:\n","            print(f\"Error comparing runs: {e}\")\n","            return None"],"metadata":{"id":"XlNJdtaigTx8","executionInfo":{"status":"ok","timestamp":1747568966571,"user_tz":300,"elapsed":36,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### 7. Training Function"],"metadata":{"id":"ivWaCeXhdX2I"}},{"cell_type":"code","source":["def patch_dqn_for_continuous_training(dqn_agent):\n","    \"\"\"\n","    Patch DQNAgent to allow continuous training without warmup reset.\n","    This modifies the DQNAgent instance to skip warmup on subsequent fit() calls.\n","    \"\"\"\n","    # Store original fit method\n","    original_fit = dqn_agent.fit\n","\n","    # Flag to track if we've already done the warmup\n","    dqn_agent._warmup_done = False\n","\n","    # Define patched fit method\n","    def patched_fit(env, nb_steps, **kwargs):\n","        # If we've already done warmup in a previous fit call\n","        if dqn_agent._warmup_done:\n","            # Temporarily set warmup steps to 0\n","            original_warmup_steps = dqn_agent.nb_steps_warmup\n","            dqn_agent.nb_steps_warmup = 0\n","\n","            # Call original fit\n","            result = original_fit(env, nb_steps, **kwargs)\n","\n","            # Restore original warmup steps\n","            dqn_agent.nb_steps_warmup = original_warmup_steps\n","            return result\n","        else:\n","            # First time training, do normal warmup\n","            result = original_fit(env, nb_steps, **kwargs)\n","            # Mark warmup as done for future fit calls\n","            dqn_agent._warmup_done = True\n","            return result\n","\n","    # Replace the fit method with our patched version\n","    dqn_agent.fit = patched_fit.__get__(dqn_agent)\n","    return dqn_agent\n","\n","\n","def train_dqn(steps=1000000, save_path='/content/drive/MyDrive/breakout_dqn'):\n","    \"\"\"\n","    Train a DQN agent on Breakout with human-like reward shaping for faster learning.\n","\n","    Uses reward signals that mimic human motivation in games to accelerate learning\n","    while maintaining proper incentive alignment between objectives.\n","    \"\"\"\n","    # Verify GPU setup\n","    physical_devices = tf.config.list_physical_devices('GPU')\n","    print(\"GPU devices detected by TensorFlow:\", physical_devices)\n","\n","    # Option to force CPU mode if GPU still doesn't work\n","    # os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Uncomment this line to force CPU\n","\n","    # Check if any GPU devices were found. If not, force CPU mode.\n","    if not physical_devices:\n","        print(\"No GPU devices detected by TensorFlow. Forcing CPU mode.\")\n","        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n","        # Re-check devices after forcing CPU (should show none)\n","        print(\"GPU devices after forcing CPU:\", tf.config.list_physical_devices('GPU'))\n","        print(\"Using CPU mode for training.\")\n","        # Reduce steps if using CPU to make training faster\n","        if steps > 1000000:\n","             print(f\"Reducing steps from {steps} to 1000000 for CPU training\")\n","             steps = 1000000\n","    else:\n","         print(\"GPU detected and will be used for training.\")\n","\n","    # Create save directory if it doesn't exist\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    # Initialize memory-efficient training monitor\n","    monitor = TrainingMonitor(\n","        log_dir=os.path.join(save_path, 'logs'),\n","        window_size=100,  # Only keep last 100 episodes in memory\n","        log_interval=10   # Only log every 10 episodes to reduce I/O\n","    )\n","\n","    # Find the latest checkpoint (if any)\n","    latest_step = 0\n","    latest_checkpoint = None\n","\n","    # Regular expression to extract step count from filenames\n","    weight_pattern = re.compile(r'breakout_dqn_weights_(\\d+)\\.h5')\n","\n","    # Check for existing checkpoint files\n","    if os.path.exists(save_path):\n","        for filename in os.listdir(save_path):\n","            match = weight_pattern.match(filename)\n","            if match:\n","                step_count = int(match.group(1))\n","                if step_count > latest_step:\n","                    latest_step = step_count\n","                    latest_checkpoint = os.path.join(save_path, filename)\n","\n","    # Create environment with reward shaping\n","    env, processor = make_env('BreakoutNoFrameskip-v4')\n","\n","    # Set window length for frame stacking\n","    window_length = 4\n","\n","    # Calculate input shape for model\n","    state_shape = (84, 84, window_length)\n","    n_actions = env.action_space.n\n","\n","    # Build DQN model\n","    model = model_template(state_shape, n_actions)\n","    model.summary()\n","\n","    # Use annealed exploration policy for better results\n","    policy = LinearAnnealedPolicy(\n","        EpsGreedyQPolicy(),\n","        attr='eps',\n","        value_max=1.0,\n","        value_min=0.1,\n","        value_test=0.05,\n","        nb_steps=1000000\n","    )\n","\n","    # Configure agent\n","    memory = SequentialMemory(limit=1000000, window_length=window_length)\n","\n","    # Configure agent with no automatic target updates\n","    dqn = DQNAgent(\n","        model=model,\n","        nb_actions=n_actions,\n","        memory=memory,\n","        nb_steps_warmup=50000,\n","        target_model_update=1000000000,  # Disable automatic updates, we'll use our callback\n","        policy=policy,\n","        enable_double_dqn=True,\n","        processor=processor\n","    )\n","\n","    # Compile DQN agent\n","    # Ensure Adam from legacy optimizers is used\n","    dqn.compile(Adam(learning_rate=0.00025), metrics=['mae'])\n","\n","\n","    # Load weights if checkpoint exists\n","    if latest_checkpoint:\n","        print(f\"Found checkpoint at step {latest_step}. Resuming training from {latest_checkpoint}\")\n","        # Ensure the optimizer state is not loaded if structure changed, but should be fine here\n","        dqn.load_weights(latest_checkpoint)\n","    else:\n","        print(\"No checkpoint found. Starting training from scratch.\")\n","        latest_step = 0\n","\n","    # Update remaining steps\n","    remaining_steps = steps - latest_step\n","    if remaining_steps <= 0:\n","        print(f\"Training already completed ({latest_step} steps). No further training needed.\")\n","        return dqn\n","\n","    print(f\"Training for {remaining_steps} more steps (total target: {steps})\")\n","\n","    # Manual checkpointing\n","    checkpoint_interval = 100000\n","    step_count = latest_step\n","\n","    # Custom training with checkpoints and monitoring\n","    while step_count < steps:\n","        # Determine how many steps to train in this batch\n","        batch_steps = min(checkpoint_interval, steps - step_count)\n","\n","        # Setup custom callback for monitoring\n","        class MonitorCallback(Callback):\n","            def on_episode_end(self, episode, logs={}):\n","                monitor.on_episode_end(episode, logs)\n","\n","        # Create episodic target update callback\n","        episode_update_callback = EpisodicTargetNetworkUpdate(\n","            update_frequency=30,  # Update target network every 30 episodes\n","            verbose=1\n","        )\n","\n","        # Train for a batch of steps\n","        # Ensure callbacks are passed correctly\n","        dqn.fit(env, nb_steps=batch_steps, visualize=False, verbose=2,\n","                callbacks=[MonitorCallback(), episode_update_callback])\n","\n","        # Update step count\n","        step_count += batch_steps\n","\n","        # Save checkpoint\n","        filename = f'breakout_dqn_weights_{step_count}.h5'\n","        filepath = os.path.join(save_path, filename)\n","        dqn.save_weights(filepath, overwrite=True)\n","        print(f\"Model saved at step {step_count} to {filepath}\")\n","\n","        # Generate and display checkpoint summary\n","        summary = monitor.on_checkpoint(step_count)\n","        print(\"\\n===== TRAINING PROGRESS SUMMARY =====\")\n","        for key, value in summary.items():\n","            print(f\"{key}: {value}\")\n","        print(\"=====================================\\n\")\n","\n","    # Save final model weights\n","    final_path = os.path.join(save_path, 'breakout_dqn_final.h5')\n","    dqn.save_weights(final_path, overwrite=True)\n","    print(f\"Final model saved to {final_path}\")\n","\n","    # Final visualization\n","    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    monitor.generate_plots(step_count, timestamp)\n","\n","    env.close()\n","    return dqn\n"],"metadata":{"id":"PbIx_Pmxdciv","executionInfo":{"status":"ok","timestamp":1747568971031,"user_tz":300,"elapsed":9,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### 8. Test Fucntion"],"metadata":{"id":"Oq9_CgxOdmPv"}},{"cell_type":"code","source":["def test_model(weights_path, episodes=5):\n","    \"\"\"Test a trained model on Breakout\"\"\"\n","    # Create environment with reward shaping\n","    env, processor = make_env('BreakoutNoFrameskip-v4')\n","\n","    # Set window length for frame stacking\n","    window_length = 4\n","\n","    # Calculate input shape for model\n","    state_shape = (84, 84, window_length)\n","    n_actions = env.action_space.n\n","\n","    # Build DQN model\n","    model = model_template(state_shape, n_actions)\n","\n","    # Configure agent\n","    memory = SequentialMemory(limit=10000, window_length=window_length)\n","    policy = EpsGreedyQPolicy(eps=0.05)  # Low exploration for testing\n","\n","    dqn = DQNAgent(\n","        model=model,\n","        nb_actions=n_actions,\n","        memory=memory,\n","        nb_steps_warmup=100,\n","        target_model_update=10000,\n","        policy=policy,\n","        enable_double_dqn=True,\n","        processor=processor\n","    )\n","\n","    # Compile DQN agent\n","    dqn.compile(Adam(learning_rate=0.00025), metrics=['mae'])\n","\n","    # Patch the agent to avoid warmup resets\n","    dqn = patch_dqn_for_continuous_training(dqn)\n","\n","    # Load weights\n","    dqn.load_weights(weights_path)\n","\n","    # Test for episodes\n","    dqn.test(env, nb_episodes=episodes, visualize=True)\n","\n","    env.close()"],"metadata":{"id":"xw--dluZdqAY","executionInfo":{"status":"ok","timestamp":1747568978705,"user_tz":300,"elapsed":6,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### 9. Analytics Dashboard Function"],"metadata":{"id":"TBL9iDZ3jKIH"}},{"cell_type":"code","source":["def analyze_training_logs(log_path='/content/drive/MyDrive/breakout_dqn/logs/training_log.csv'):\n","    \"\"\"\n","    Generate interactive analytics dashboard from training logs.\n","\n","    This function loads saved log data and creates visualizations to analyze\n","    training performance with minimal memory usage.\n","    \"\"\"\n","    try:\n","        # Load the training log efficiently (only load what we need)\n","        df = pd.read_csv(log_path)\n","\n","        # Check if data exists\n","        if len(df) == 0:\n","            print(\"No training data found in log file.\")\n","            return\n","\n","        # Calculate rolling averages\n","        window = min(100, len(df))\n","        df['reward_avg'] = df['reward'].rolling(window=window, min_periods=1).mean()\n","        df['length_avg'] = df['length'].rolling(window=window, min_periods=1).mean()\n","\n","        if 'mean_q' in df.columns and not df['mean_q'].isna().all():\n","            df['q_avg'] = df['mean_q'].rolling(window=window, min_periods=1).mean()\n","\n","        # Create visualizations\n","        plt.figure(figsize=(15, 12))\n","\n","        # Plot 1: Episode rewards over time\n","        plt.subplot(2, 2, 1)\n","        plt.plot(df['episode'], df['reward'], 'b-', alpha=0.3)\n","        plt.plot(df['episode'], df['reward_avg'], 'r-', linewidth=2)\n","        plt.title('Reward per Episode')\n","        plt.xlabel('Episode')\n","        plt.ylabel('Reward')\n","        plt.grid(True)\n","\n","        # Plot 2: Reward distribution histogram\n","        plt.subplot(2, 2, 2)\n","        plt.hist(df['reward'], bins=20)\n","        plt.axvline(df['reward'].mean(), color='r', linestyle='dashed', linewidth=2)\n","        plt.title(f'Reward Distribution (Mean: {df[\"reward\"].mean():.2f})')\n","        plt.xlabel('Reward')\n","        plt.ylabel('Count')\n","\n","        # Plot 3: Episode length over time\n","        plt.subplot(2, 2, 3)\n","        plt.plot(df['episode'], df['length'], 'b-', alpha=0.3)\n","        plt.plot(df['episode'], df['length_avg'], 'r-', linewidth=2)\n","        plt.title('Episode Length Over Time')\n","        plt.xlabel('Episode')\n","        plt.ylabel('Steps')\n","        plt.grid(True)\n","\n","        # Plot 4: Q-values over time (if available)\n","        plt.subplot(2, 2, 4)\n","        if 'mean_q' in df.columns and not df['mean_q'].isna().all():\n","            plt.plot(df['episode'], df['mean_q'], 'b-', alpha=0.3)\n","            plt.plot(df['episode'], df['q_avg'], 'r-', linewidth=2)\n","            plt.title('Mean Q-Value Over Time')\n","            plt.grid(True)\n","            plt.xlabel('Episode')\n","            plt.ylabel('Q-Value')\n","        else:\n","            plt.text(0.5, 0.5, 'No Q-values recorded yet',\n","                    horizontalalignment='center', verticalalignment='center')\n","            plt.title('Mean Q-Values (Not Available)')\n","\n","        plt.tight_layout()\n","        plt.savefig('/content/drive/MyDrive/breakout_dqn/logs/training_analytics.png')\n","        plt.show()\n","\n","        # Generate summary statistics\n","        print(\"\\n===== TRAINING ANALYTICS SUMMARY =====\")\n","        print(f\"Total Episodes: {len(df)}\")\n","        print(f\"Total Steps: {df['total_steps'].max()}\")\n","        print(f\"Average Reward: {df['reward'].mean():.2f}\")\n","        print(f\"Max Reward: {df['reward'].max()}\")\n","        print(f\"Average Episode Length: {df['length'].mean():.2f}\")\n","        print(f\"Last 100 Episodes Average Reward: {df['reward'].tail(100).mean():.2f}\")\n","        if 'mean_q' in df.columns and not df['mean_q'].isna().all():\n","            print(f\"Average Q-Value: {df['mean_q'].mean():.4f}\")\n","        print(\"=====================================\\n\")\n","\n","        return df\n","    except Exception as e:\n","        print(f\"Error analyzing logs: {e}\")\n","        return None\n","\n","def compare_training_strategies(log_path1, log_path2, labels=None, output_path=None):\n","    \"\"\"\n","    Compare two different training strategies side by side.\n","\n","    Args:\n","        log_path1: Path to first training log CSV\n","        log_path2: Path to second training log CSV\n","        labels: Tuple of (label1, label2) for the legend\n","        output_path: Path to save comparison image\n","    \"\"\"\n","    try:\n","        # Load logs efficiently\n","        df1 = pd.read_csv(log_path1)\n","        df2 = pd.read_csv(log_path2)\n","\n","        # Use default labels if none provided\n","        if labels is None:\n","            labels = ('Strategy 1', 'Strategy 2')\n","\n","        # Calculate rolling averages\n","        window = min(100, len(df1), len(df2))\n","        df1['reward_avg'] = df1['reward'].rolling(window=window, min_periods=1).mean()\n","        df2['reward_avg'] = df2['reward'].rolling(window=window, min_periods=1).mean()\n","\n","        # Create comparison plot\n","        plt.figure(figsize=(12, 8))\n","\n","        # Rewards\n","        plt.subplot(2, 1, 1)\n","        plt.plot(df1['episode'], df1['reward_avg'], 'b-', linewidth=2, label=labels[0])\n","        plt.plot(df2['episode'], df2['reward_avg'], 'r-', linewidth=2, label=labels[1])\n","        plt.title('Reward Comparison')\n","        plt.xlabel('Episode')\n","        plt.ylabel(f'Avg Reward ({window} ep window)')\n","        plt.legend()\n","        plt.grid(True)\n","\n","        # Episode lengths\n","        plt.subplot(2, 1, 2)\n","        df1['length_avg'] = df1['length'].rolling(window=window, min_periods=1).mean()\n","        df2['length_avg'] = df2['length'].rolling(window=window, min_periods=1).mean()\n","        plt.plot(df1['episode'], df1['length_avg'], 'b-', linewidth=2, label=labels[0])\n","        plt.plot(df2['episode'], df2['length_avg'], 'r-', linewidth=2, label=labels[1])\n","        plt.title('Episode Length Comparison')\n","        plt.xlabel('Episode')\n","        plt.ylabel('Avg Length (steps)')\n","        plt.legend()\n","        plt.grid(True)\n","\n","        plt.tight_layout()\n","\n","        if output_path:\n","            plt.savefig(output_path)\n","        plt.show()\n","\n","        # Compare statistics\n","        print(\"\\n===== TRAINING STRATEGY COMPARISON =====\")\n","        print(f\"{labels[0]} vs {labels[1]}\")\n","        print(f\"Episodes: {len(df1)} vs {len(df2)}\")\n","        print(f\"Final Avg Reward: {df1['reward_avg'].iloc[-1]:.2f} vs {df2['reward_avg'].iloc[-1]:.2f}\")\n","        print(f\"Max Reward: {df1['reward'].max():.1f} vs {df2['reward'].max():.1f}\")\n","        print(f\"Avg Episode Length: {df1['length'].mean():.1f} vs {df2['length'].mean():.1f}\")\n","        print(\"========================================\\n\")\n","\n","    except Exception as e:\n","        print(f\"Error comparing training strategies: {e}\")"],"metadata":{"id":"9cHnTxZkjRXR","executionInfo":{"status":"ok","timestamp":1747568981191,"user_tz":300,"elapsed":22,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 10. Train the Agent"],"metadata":{"id":"P8x5xTnkdwe9"}},{"cell_type":"code","source":["train_dqn(3000000)  # 1M steps for testing, 5M for full training\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"__KLpZPTd2PF","outputId":"874e789d-b983-4826-f32d-93c9c367a259"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" 65797/100000: episode: 1658, duration: 5.813s, episode steps:  79, steps per second:  14, episode reward:  0.711, mean reward:  0.009 [-0.489,  1.200], mean action: 1.544 [0.000, 3.000],  loss: 0.036226, mae: 1.906149, mean_q: 2.601055, mean_eps: 0.940819\n"," 65898/100000: episode: 1659, duration: 7.482s, episode steps: 101, steps per second:  13, episode reward:  1.916, mean reward:  0.019 [-0.484,  1.200], mean action: 1.485 [0.000, 3.000],  loss: 0.045354, mae: 1.920589, mean_q: 2.620998, mean_eps: 0.940738\n"," 65923/100000: episode: 1660, duration: 1.952s, episode steps:  25, steps per second:  13, episode reward: -0.480, mean reward: -0.019 [-0.480,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.039359, mae: 1.913287, mean_q: 2.611405, mean_eps: 0.940681\n"," 65950/100000: episode: 1661, duration: 1.996s, episode steps:  27, steps per second:  14, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.049432, mae: 1.888896, mean_q: 2.577991, mean_eps: 0.940658\n"," 65976/100000: episode: 1662, duration: 1.952s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.044086, mae: 1.880298, mean_q: 2.565812, mean_eps: 0.940634\n"," 66001/100000: episode: 1663, duration: 1.940s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.039089, mae: 1.880947, mean_q: 2.574967, mean_eps: 0.940611\n"," 66030/100000: episode: 1664, duration: 2.137s, episode steps:  29, steps per second:  14, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.035172, mae: 1.880528, mean_q: 2.571056, mean_eps: 0.940586\n"," 66063/100000: episode: 1665, duration: 2.504s, episode steps:  33, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.485 [0.000, 3.000],  loss: 0.023853, mae: 1.847364, mean_q: 2.533797, mean_eps: 0.940559\n"," 66117/100000: episode: 1666, duration: 3.992s, episode steps:  54, steps per second:  14, episode reward:  0.707, mean reward:  0.013 [-0.493,  1.200], mean action: 1.481 [0.000, 3.000],  loss: 0.041305, mae: 1.850137, mean_q: 2.525802, mean_eps: 0.940519\n"," 66142/100000: episode: 1667, duration: 1.977s, episode steps:  25, steps per second:  13, episode reward: -0.489, mean reward: -0.020 [-0.489,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.037469, mae: 1.879589, mean_q: 2.570225, mean_eps: 0.940484\n"," 66168/100000: episode: 1668, duration: 2.060s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.041202, mae: 1.922042, mean_q: 2.628307, mean_eps: 0.940461\n"," 66226/100000: episode: 1669, duration: 4.338s, episode steps:  58, steps per second:  13, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.448 [0.000, 3.000],  loss: 0.041789, mae: 1.899375, mean_q: 2.592754, mean_eps: 0.940423\n"," 66252/100000: episode: 1670, duration: 1.925s, episode steps:  26, steps per second:  14, episode reward: -0.488, mean reward: -0.019 [-0.488,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.029221, mae: 1.907889, mean_q: 2.617911, mean_eps: 0.940385\n"," 66307/100000: episode: 1671, duration: 4.259s, episode steps:  55, steps per second:  13, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.545 [0.000, 3.000],  loss: 0.038417, mae: 1.896000, mean_q: 2.591701, mean_eps: 0.940349\n"," 66333/100000: episode: 1672, duration: 2.063s, episode steps:  26, steps per second:  13, episode reward: -0.489, mean reward: -0.019 [-0.489,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.044579, mae: 1.901395, mean_q: 2.606599, mean_eps: 0.940312\n"," 66405/100000: episode: 1673, duration: 5.213s, episode steps:  72, steps per second:  14, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.292 [0.000, 3.000],  loss: 0.037823, mae: 1.892725, mean_q: 2.587564, mean_eps: 0.940268\n"," 66433/100000: episode: 1674, duration: 2.109s, episode steps:  28, steps per second:  13, episode reward: -0.486, mean reward: -0.017 [-0.486,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.034424, mae: 1.889372, mean_q: 2.591860, mean_eps: 0.940223\n"," 66459/100000: episode: 1675, duration: 1.985s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.040620, mae: 1.893280, mean_q: 2.586068, mean_eps: 0.940199\n"," 66537/100000: episode: 1676, duration: 5.751s, episode steps:  78, steps per second:  14, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.551 [0.000, 3.000],  loss: 0.034976, mae: 1.905336, mean_q: 2.606542, mean_eps: 0.940152\n"," 66633/100000: episode: 1677, duration: 7.084s, episode steps:  96, steps per second:  14, episode reward:  1.916, mean reward:  0.020 [-0.484,  1.200], mean action: 1.406 [0.000, 3.000],  loss: 0.031300, mae: 1.931409, mean_q: 2.648442, mean_eps: 0.940074\n"," 66661/100000: episode: 1678, duration: 2.214s, episode steps:  28, steps per second:  13, episode reward: -0.481, mean reward: -0.017 [-0.481,  0.000], mean action: 1.821 [0.000, 3.000],  loss: 0.047816, mae: 1.862008, mean_q: 2.540380, mean_eps: 0.940018\n"," 66691/100000: episode: 1679, duration: 2.214s, episode steps:  30, steps per second:  14, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.042692, mae: 1.894983, mean_q: 2.577893, mean_eps: 0.939992\n","\n","Target network updated after 30 episodes\n"," 66717/100000: episode: 1680, duration: 1.996s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.027609, mae: 1.865054, mean_q: 2.557331, mean_eps: 0.939967\n"," 66742/100000: episode: 1681, duration: 1.926s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.070546, mae: 1.914041, mean_q: 2.595443, mean_eps: 0.939944\n"," 66771/100000: episode: 1682, duration: 2.207s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.050055, mae: 1.839866, mean_q: 2.503870, mean_eps: 0.939920\n"," 66797/100000: episode: 1683, duration: 2.085s, episode steps:  26, steps per second:  12, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.031328, mae: 1.924438, mean_q: 2.626198, mean_eps: 0.939895\n"," 66822/100000: episode: 1684, duration: 1.917s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.044052, mae: 1.918041, mean_q: 2.633274, mean_eps: 0.939872\n"," 66848/100000: episode: 1685, duration: 1.947s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.045997, mae: 1.957109, mean_q: 2.684806, mean_eps: 0.939849\n"," 66873/100000: episode: 1686, duration: 1.915s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.040417, mae: 1.880242, mean_q: 2.586792, mean_eps: 0.939826\n"," 66900/100000: episode: 1687, duration: 2.087s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.259 [0.000, 3.000],  loss: 0.041405, mae: 1.852058, mean_q: 2.544604, mean_eps: 0.939803\n"," 66925/100000: episode: 1688, duration: 1.850s, episode steps:  25, steps per second:  14, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.031696, mae: 1.819775, mean_q: 2.493061, mean_eps: 0.939779\n"," 66953/100000: episode: 1689, duration: 2.202s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.026302, mae: 1.911257, mean_q: 2.628290, mean_eps: 0.939755\n"," 66978/100000: episode: 1690, duration: 1.919s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.960 [0.000, 3.000],  loss: 0.024070, mae: 1.844021, mean_q: 2.526975, mean_eps: 0.939731\n"," 67006/100000: episode: 1691, duration: 2.047s, episode steps:  28, steps per second:  14, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.035157, mae: 1.843925, mean_q: 2.518094, mean_eps: 0.939708\n"," 67035/100000: episode: 1692, duration: 2.155s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.724 [0.000, 3.000],  loss: 0.036231, mae: 1.939139, mean_q: 2.666549, mean_eps: 0.939682\n"," 67112/100000: episode: 1693, duration: 5.783s, episode steps:  77, steps per second:  13, episode reward:  0.706, mean reward:  0.009 [-0.494,  1.200], mean action: 1.468 [0.000, 3.000],  loss: 0.031715, mae: 1.891512, mean_q: 2.598665, mean_eps: 0.939634\n"," 67212/100000: episode: 1694, duration: 7.291s, episode steps: 100, steps per second:  14, episode reward:  1.915, mean reward:  0.019 [-0.485,  1.200], mean action: 1.450 [0.000, 3.000],  loss: 0.034179, mae: 1.908562, mean_q: 2.614036, mean_eps: 0.939555\n"," 67248/100000: episode: 1695, duration: 2.701s, episode steps:  36, steps per second:  13, episode reward: -0.480, mean reward: -0.013 [-0.480,  0.000], mean action: 1.389 [0.000, 3.000],  loss: 0.037113, mae: 1.887620, mean_q: 2.588585, mean_eps: 0.939493\n"," 67274/100000: episode: 1696, duration: 2.028s, episode steps:  26, steps per second:  13, episode reward: -0.493, mean reward: -0.019 [-0.493,  0.000], mean action: 1.846 [0.000, 3.000],  loss: 0.036259, mae: 1.898876, mean_q: 2.620188, mean_eps: 0.939466\n"," 67445/100000: episode: 1697, duration: 12.658s, episode steps: 171, steps per second:  14, episode reward:  4.305, mean reward:  0.025 [-0.495,  1.200], mean action: 1.614 [0.000, 3.000],  loss: 0.033794, mae: 1.895279, mean_q: 2.608939, mean_eps: 0.939377\n"," 67482/100000: episode: 1698, duration: 2.799s, episode steps:  37, steps per second:  13, episode reward: -0.466, mean reward: -0.013 [-0.466,  0.000], mean action: 1.432 [0.000, 3.000],  loss: 0.033189, mae: 1.917138, mean_q: 2.628925, mean_eps: 0.939283\n"," 67507/100000: episode: 1699, duration: 1.924s, episode steps:  25, steps per second:  13, episode reward: -0.493, mean reward: -0.020 [-0.493,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.038324, mae: 1.869436, mean_q: 2.573234, mean_eps: 0.939255\n"," 67538/100000: episode: 1700, duration: 2.339s, episode steps:  31, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.387 [0.000, 3.000],  loss: 0.040622, mae: 1.914420, mean_q: 2.614743, mean_eps: 0.939230\n"," 67563/100000: episode: 1701, duration: 1.861s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.035134, mae: 1.897636, mean_q: 2.620863, mean_eps: 0.939205\n"," 67589/100000: episode: 1702, duration: 1.962s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.024062, mae: 1.973513, mean_q: 2.718214, mean_eps: 0.939182\n"," 67658/100000: episode: 1703, duration: 5.331s, episode steps:  69, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.464 [0.000, 3.000],  loss: 0.035647, mae: 1.904979, mean_q: 2.613894, mean_eps: 0.939139\n"," 67687/100000: episode: 1704, duration: 2.189s, episode steps:  29, steps per second:  13, episode reward: -0.486, mean reward: -0.017 [-0.486,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.035094, mae: 1.849354, mean_q: 2.537649, mean_eps: 0.939095\n"," 67716/100000: episode: 1705, duration: 2.200s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.026112, mae: 1.908500, mean_q: 2.616125, mean_eps: 0.939069\n"," 67742/100000: episode: 1706, duration: 1.994s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.031643, mae: 1.869096, mean_q: 2.570203, mean_eps: 0.939044\n"," 67767/100000: episode: 1707, duration: 1.962s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.021788, mae: 1.849659, mean_q: 2.551507, mean_eps: 0.939021\n"," 67793/100000: episode: 1708, duration: 1.995s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.032439, mae: 1.937193, mean_q: 2.664917, mean_eps: 0.938998\n"," 67819/100000: episode: 1709, duration: 1.969s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.027203, mae: 1.873537, mean_q: 2.580073, mean_eps: 0.938975\n","\n","Target network updated after 30 episodes\n"," 67845/100000: episode: 1710, duration: 1.975s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.231 [0.000, 3.000],  loss: 0.041361, mae: 1.867055, mean_q: 2.556711, mean_eps: 0.938952\n"," 67953/100000: episode: 1711, duration: 8.092s, episode steps: 108, steps per second:  13, episode reward:  1.905, mean reward:  0.018 [-0.495,  1.200], mean action: 1.648 [0.000, 3.000],  loss: 0.053701, mae: 1.909768, mean_q: 2.603514, mean_eps: 0.938891\n"," 67981/100000: episode: 1712, duration: 2.145s, episode steps:  28, steps per second:  13, episode reward: -0.478, mean reward: -0.017 [-0.478,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.031241, mae: 1.873696, mean_q: 2.571865, mean_eps: 0.938830\n"," 68009/100000: episode: 1713, duration: 2.119s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.023167, mae: 1.908258, mean_q: 2.620941, mean_eps: 0.938805\n"," 68036/100000: episode: 1714, duration: 2.066s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.030718, mae: 1.915154, mean_q: 2.622772, mean_eps: 0.938780\n"," 68061/100000: episode: 1715, duration: 1.884s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.034963, mae: 1.907589, mean_q: 2.599787, mean_eps: 0.938757\n"," 68086/100000: episode: 1716, duration: 1.951s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.920 [0.000, 3.000],  loss: 0.035260, mae: 1.814459, mean_q: 2.503660, mean_eps: 0.938734\n"," 68140/100000: episode: 1717, duration: 3.975s, episode steps:  54, steps per second:  14, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.574 [0.000, 3.000],  loss: 0.037587, mae: 1.903741, mean_q: 2.601243, mean_eps: 0.938699\n"," 68197/100000: episode: 1718, duration: 4.291s, episode steps:  57, steps per second:  13, episode reward:  0.711, mean reward:  0.012 [-0.489,  1.200], mean action: 1.368 [0.000, 3.000],  loss: 0.029818, mae: 1.854092, mean_q: 2.535786, mean_eps: 0.938649\n"," 68224/100000: episode: 1719, duration: 2.108s, episode steps:  27, steps per second:  13, episode reward: -0.489, mean reward: -0.018 [-0.489,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.042845, mae: 1.902933, mean_q: 2.591131, mean_eps: 0.938611\n"," 68351/100000: episode: 1720, duration: 9.583s, episode steps: 127, steps per second:  13, episode reward:  1.905, mean reward:  0.015 [-0.495,  1.200], mean action: 1.543 [0.000, 3.000],  loss: 0.032517, mae: 1.888693, mean_q: 2.586389, mean_eps: 0.938542\n"," 68423/100000: episode: 1721, duration: 5.424s, episode steps:  72, steps per second:  13, episode reward:  0.725, mean reward:  0.010 [-0.475,  1.200], mean action: 1.528 [0.000, 3.000],  loss: 0.032668, mae: 1.898836, mean_q: 2.594168, mean_eps: 0.938452\n"," 68448/100000: episode: 1722, duration: 1.917s, episode steps:  25, steps per second:  13, episode reward: -0.486, mean reward: -0.019 [-0.486,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.026326, mae: 1.830019, mean_q: 2.515076, mean_eps: 0.938408\n"," 68473/100000: episode: 1723, duration: 1.926s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.028506, mae: 1.890088, mean_q: 2.583662, mean_eps: 0.938386\n"," 68498/100000: episode: 1724, duration: 1.927s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.840 [0.000, 3.000],  loss: 0.039068, mae: 1.902015, mean_q: 2.597120, mean_eps: 0.938363\n"," 68603/100000: episode: 1725, duration: 8.041s, episode steps: 105, steps per second:  13, episode reward:  1.905, mean reward:  0.018 [-0.495,  1.200], mean action: 1.676 [0.000, 3.000],  loss: 0.028550, mae: 1.912077, mean_q: 2.623788, mean_eps: 0.938305\n"," 68629/100000: episode: 1726, duration: 1.916s, episode steps:  26, steps per second:  14, episode reward: -0.479, mean reward: -0.018 [-0.479,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.027826, mae: 1.941808, mean_q: 2.658657, mean_eps: 0.938246\n"," 68662/100000: episode: 1727, duration: 2.458s, episode steps:  33, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.034666, mae: 1.875542, mean_q: 2.574203, mean_eps: 0.938219\n"," 68785/100000: episode: 1728, duration: 9.219s, episode steps: 123, steps per second:  13, episode reward:  1.907, mean reward:  0.016 [-0.493,  1.200], mean action: 1.268 [0.000, 3.000],  loss: 0.026346, mae: 1.868824, mean_q: 2.564315, mean_eps: 0.938149\n"," 68813/100000: episode: 1729, duration: 2.094s, episode steps:  28, steps per second:  13, episode reward: -0.475, mean reward: -0.017 [-0.475,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.031061, mae: 1.940939, mean_q: 2.650209, mean_eps: 0.938081\n"," 68841/100000: episode: 1730, duration: 2.128s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.021460, mae: 1.847693, mean_q: 2.534394, mean_eps: 0.938056\n"," 68868/100000: episode: 1731, duration: 2.029s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.704 [0.000, 3.000],  loss: 0.030548, mae: 1.852643, mean_q: 2.534858, mean_eps: 0.938031\n"," 68895/100000: episode: 1732, duration: 2.143s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.035982, mae: 1.832264, mean_q: 2.509599, mean_eps: 0.938007\n"," 68923/100000: episode: 1733, duration: 2.188s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.029929, mae: 1.934999, mean_q: 2.644131, mean_eps: 0.937982\n"," 68950/100000: episode: 1734, duration: 1.955s, episode steps:  27, steps per second:  14, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.043617, mae: 1.952445, mean_q: 2.665222, mean_eps: 0.937958\n"," 68976/100000: episode: 1735, duration: 1.926s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.115 [0.000, 3.000],  loss: 0.032197, mae: 1.935077, mean_q: 2.641427, mean_eps: 0.937934\n"," 69007/100000: episode: 1736, duration: 2.348s, episode steps:  31, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.677 [0.000, 3.000],  loss: 0.023206, mae: 1.910669, mean_q: 2.614188, mean_eps: 0.937908\n"," 69032/100000: episode: 1737, duration: 1.913s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.033160, mae: 1.878414, mean_q: 2.563822, mean_eps: 0.937883\n"," 69153/100000: episode: 1738, duration: 9.058s, episode steps: 121, steps per second:  13, episode reward:  1.905, mean reward:  0.016 [-0.495,  1.200], mean action: 1.645 [0.000, 3.000],  loss: 0.030968, mae: 1.877363, mean_q: 2.566567, mean_eps: 0.937817\n"," 69224/100000: episode: 1739, duration: 5.333s, episode steps:  71, steps per second:  13, episode reward:  0.724, mean reward:  0.010 [-0.476,  1.200], mean action: 1.493 [0.000, 3.000],  loss: 0.033314, mae: 1.882468, mean_q: 2.582240, mean_eps: 0.937731\n","\n","Target network updated after 30 episodes\n"," 69251/100000: episode: 1740, duration: 2.114s, episode steps:  27, steps per second:  13, episode reward: -0.486, mean reward: -0.018 [-0.486,  0.000], mean action: 1.259 [0.000, 3.000],  loss: 0.018176, mae: 1.831317, mean_q: 2.514358, mean_eps: 0.937687\n"," 69278/100000: episode: 1741, duration: 2.097s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.075000, mae: 1.896691, mean_q: 2.578214, mean_eps: 0.937662\n"," 69335/100000: episode: 1742, duration: 4.296s, episode steps:  57, steps per second:  13, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.316 [0.000, 3.000],  loss: 0.046532, mae: 1.859347, mean_q: 2.540845, mean_eps: 0.937625\n"," 69360/100000: episode: 1743, duration: 1.859s, episode steps:  25, steps per second:  13, episode reward: -0.489, mean reward: -0.020 [-0.489,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.048572, mae: 1.890462, mean_q: 2.575882, mean_eps: 0.937588\n"," 69388/100000: episode: 1744, duration: 2.313s, episode steps:  28, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.036512, mae: 1.877587, mean_q: 2.574937, mean_eps: 0.937564\n"," 69413/100000: episode: 1745, duration: 1.931s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.048802, mae: 1.855266, mean_q: 2.537442, mean_eps: 0.937540\n"," 69484/100000: episode: 1746, duration: 5.143s, episode steps:  71, steps per second:  14, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.704 [0.000, 3.000],  loss: 0.029294, mae: 1.917237, mean_q: 2.630465, mean_eps: 0.937497\n"," 69525/100000: episode: 1747, duration: 3.099s, episode steps:  41, steps per second:  13, episode reward: -0.486, mean reward: -0.012 [-0.486,  0.000], mean action: 1.561 [0.000, 3.000],  loss: 0.026369, mae: 1.919539, mean_q: 2.624106, mean_eps: 0.937446\n"," 69556/100000: episode: 1748, duration: 2.417s, episode steps:  31, steps per second:  13, episode reward: -0.492, mean reward: -0.016 [-0.492,  0.000], mean action: 1.677 [0.000, 3.000],  loss: 0.029847, mae: 1.837282, mean_q: 2.525588, mean_eps: 0.937414\n"," 69589/100000: episode: 1749, duration: 2.448s, episode steps:  33, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.788 [0.000, 3.000],  loss: 0.032796, mae: 1.835168, mean_q: 2.514192, mean_eps: 0.937385\n"," 69622/100000: episode: 1750, duration: 2.513s, episode steps:  33, steps per second:  13, episode reward: -0.493, mean reward: -0.015 [-0.493,  0.000], mean action: 1.636 [0.000, 3.000],  loss: 0.045691, mae: 1.899749, mean_q: 2.602586, mean_eps: 0.937356\n"," 69648/100000: episode: 1751, duration: 1.983s, episode steps:  26, steps per second:  13, episode reward: -0.493, mean reward: -0.019 [-0.493,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.035369, mae: 1.859340, mean_q: 2.555794, mean_eps: 0.937329\n"," 69673/100000: episode: 1752, duration: 1.928s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.160 [0.000, 3.000],  loss: 0.033630, mae: 1.848944, mean_q: 2.525705, mean_eps: 0.937306\n"," 69748/100000: episode: 1753, duration: 5.553s, episode steps:  75, steps per second:  14, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.507 [0.000, 3.000],  loss: 0.025320, mae: 1.895775, mean_q: 2.598008, mean_eps: 0.937261\n"," 69776/100000: episode: 1754, duration: 2.032s, episode steps:  28, steps per second:  14, episode reward: -0.485, mean reward: -0.017 [-0.485,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.022613, mae: 1.813914, mean_q: 2.491117, mean_eps: 0.937215\n"," 69805/100000: episode: 1755, duration: 2.195s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.690 [0.000, 3.000],  loss: 0.050336, mae: 1.784055, mean_q: 2.458227, mean_eps: 0.937189\n"," 69833/100000: episode: 1756, duration: 2.069s, episode steps:  28, steps per second:  14, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.035813, mae: 1.976699, mean_q: 2.700404, mean_eps: 0.937163\n"," 69859/100000: episode: 1757, duration: 2.014s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.043213, mae: 1.862342, mean_q: 2.550564, mean_eps: 0.937139\n"," 69884/100000: episode: 1758, duration: 1.993s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.031049, mae: 1.878524, mean_q: 2.582668, mean_eps: 0.937116\n"," 69912/100000: episode: 1759, duration: 2.111s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.033293, mae: 1.844164, mean_q: 2.520900, mean_eps: 0.937092\n"," 69938/100000: episode: 1760, duration: 2.012s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.115 [0.000, 3.000],  loss: 0.028296, mae: 1.870713, mean_q: 2.570020, mean_eps: 0.937068\n"," 69963/100000: episode: 1761, duration: 1.860s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.047308, mae: 1.837658, mean_q: 2.523945, mean_eps: 0.937045\n"," 69990/100000: episode: 1762, duration: 2.000s, episode steps:  27, steps per second:  14, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.036574, mae: 1.834155, mean_q: 2.515330, mean_eps: 0.937022\n"," 70015/100000: episode: 1763, duration: 1.942s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.040 [0.000, 3.000],  loss: 0.020251, mae: 1.874282, mean_q: 2.567145, mean_eps: 0.936998\n"," 70068/100000: episode: 1764, duration: 4.037s, episode steps:  53, steps per second:  13, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.434 [0.000, 3.000],  loss: 0.033876, mae: 1.851873, mean_q: 2.535675, mean_eps: 0.936963\n"," 70143/100000: episode: 1765, duration: 5.557s, episode steps:  75, steps per second:  13, episode reward:  0.711, mean reward:  0.009 [-0.489,  1.200], mean action: 1.600 [0.000, 3.000],  loss: 0.039838, mae: 1.887523, mean_q: 2.584838, mean_eps: 0.936906\n"," 70168/100000: episode: 1766, duration: 1.884s, episode steps:  25, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.044648, mae: 1.859643, mean_q: 2.542873, mean_eps: 0.936860\n"," 70225/100000: episode: 1767, duration: 4.334s, episode steps:  57, steps per second:  13, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.351 [0.000, 3.000],  loss: 0.030116, mae: 1.828790, mean_q: 2.516382, mean_eps: 0.936824\n"," 70257/100000: episode: 1768, duration: 2.409s, episode steps:  32, steps per second:  13, episode reward: -0.489, mean reward: -0.015 [-0.489,  0.000], mean action: 1.719 [0.000, 3.000],  loss: 0.039024, mae: 1.818916, mean_q: 2.486394, mean_eps: 0.936784\n"," 70329/100000: episode: 1769, duration: 5.486s, episode steps:  72, steps per second:  13, episode reward:  0.706, mean reward:  0.010 [-0.494,  1.200], mean action: 1.625 [0.000, 3.000],  loss: 0.030265, mae: 1.844251, mean_q: 2.527116, mean_eps: 0.936737\n","\n","Target network updated after 30 episodes\n"," 70355/100000: episode: 1770, duration: 2.120s, episode steps:  26, steps per second:  12, episode reward: -0.486, mean reward: -0.019 [-0.486,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.019448, mae: 1.903029, mean_q: 2.607909, mean_eps: 0.936693\n"," 70459/100000: episode: 1771, duration: 7.790s, episode steps: 104, steps per second:  13, episode reward:  1.905, mean reward:  0.018 [-0.495,  1.200], mean action: 1.490 [0.000, 3.000],  loss: 0.041866, mae: 1.892362, mean_q: 2.580949, mean_eps: 0.936634\n"," 70485/100000: episode: 1772, duration: 1.986s, episode steps:  26, steps per second:  13, episode reward: -0.479, mean reward: -0.018 [-0.479,  0.000], mean action: 1.808 [0.000, 3.000],  loss: 0.034269, mae: 1.825455, mean_q: 2.487916, mean_eps: 0.936576\n"," 70514/100000: episode: 1773, duration: 2.242s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.552 [0.000, 3.000],  loss: 0.031256, mae: 1.815983, mean_q: 2.494617, mean_eps: 0.936551\n"," 70539/100000: episode: 1774, duration: 1.957s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.240 [0.000, 3.000],  loss: 0.023558, mae: 1.908212, mean_q: 2.604867, mean_eps: 0.936527\n"," 70570/100000: episode: 1775, duration: 2.248s, episode steps:  31, steps per second:  14, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.258 [0.000, 3.000],  loss: 0.033842, mae: 1.879382, mean_q: 2.575213, mean_eps: 0.936501\n"," 70595/100000: episode: 1776, duration: 1.813s, episode steps:  25, steps per second:  14, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.028670, mae: 1.862142, mean_q: 2.552851, mean_eps: 0.936476\n"," 70620/100000: episode: 1777, duration: 1.854s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.033465, mae: 1.872139, mean_q: 2.561911, mean_eps: 0.936454\n"," 70649/100000: episode: 1778, duration: 2.187s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.027516, mae: 1.914973, mean_q: 2.623327, mean_eps: 0.936429\n"," 70677/100000: episode: 1779, duration: 2.212s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.030232, mae: 1.812219, mean_q: 2.486947, mean_eps: 0.936404\n"," 70708/100000: episode: 1780, duration: 2.341s, episode steps:  31, steps per second:  13, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.020366, mae: 1.860553, mean_q: 2.548147, mean_eps: 0.936377\n"," 70734/100000: episode: 1781, duration: 1.969s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.030810, mae: 1.883407, mean_q: 2.589046, mean_eps: 0.936352\n"," 70759/100000: episode: 1782, duration: 1.909s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.022292, mae: 1.879785, mean_q: 2.565974, mean_eps: 0.936329\n"," 70785/100000: episode: 1783, duration: 1.946s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.028511, mae: 1.892328, mean_q: 2.594531, mean_eps: 0.936306\n"," 70811/100000: episode: 1784, duration: 1.954s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.030682, mae: 1.885531, mean_q: 2.575930, mean_eps: 0.936282\n"," 70837/100000: episode: 1785, duration: 2.081s, episode steps:  26, steps per second:  12, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.034960, mae: 1.860655, mean_q: 2.541919, mean_eps: 0.936259\n"," 70866/100000: episode: 1786, duration: 2.249s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.028236, mae: 1.880434, mean_q: 2.567113, mean_eps: 0.936234\n"," 70892/100000: episode: 1787, duration: 1.966s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.026141, mae: 1.922093, mean_q: 2.632886, mean_eps: 0.936209\n"," 70928/100000: episode: 1788, duration: 2.689s, episode steps:  36, steps per second:  13, episode reward: -0.495, mean reward: -0.014 [-0.495,  0.000], mean action: 1.139 [0.000, 3.000],  loss: 0.025625, mae: 1.860733, mean_q: 2.542863, mean_eps: 0.936181\n"," 70956/100000: episode: 1789, duration: 2.153s, episode steps:  28, steps per second:  13, episode reward: -0.493, mean reward: -0.018 [-0.493,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.024908, mae: 1.827695, mean_q: 2.506217, mean_eps: 0.936153\n"," 71029/100000: episode: 1790, duration: 5.453s, episode steps:  73, steps per second:  13, episode reward:  0.706, mean reward:  0.010 [-0.494,  1.200], mean action: 1.466 [0.000, 3.000],  loss: 0.027735, mae: 1.866555, mean_q: 2.555631, mean_eps: 0.936107\n"," 71057/100000: episode: 1791, duration: 2.131s, episode steps:  28, steps per second:  13, episode reward: -0.485, mean reward: -0.017 [-0.485,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.031164, mae: 1.857764, mean_q: 2.540091, mean_eps: 0.936062\n"," 71086/100000: episode: 1792, duration: 2.264s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.040795, mae: 1.843200, mean_q: 2.510483, mean_eps: 0.936036\n"," 71237/100000: episode: 1793, duration: 11.182s, episode steps: 151, steps per second:  14, episode reward:  3.106, mean reward:  0.021 [-0.494,  1.200], mean action: 1.417 [0.000, 3.000],  loss: 0.028089, mae: 1.870738, mean_q: 2.559326, mean_eps: 0.935955\n"," 71263/100000: episode: 1794, duration: 2.026s, episode steps:  26, steps per second:  13, episode reward: -0.470, mean reward: -0.018 [-0.470,  0.000], mean action: 1.115 [0.000, 3.000],  loss: 0.025230, mae: 1.778178, mean_q: 2.429745, mean_eps: 0.935875\n"," 71298/100000: episode: 1795, duration: 2.648s, episode steps:  35, steps per second:  13, episode reward: -0.495, mean reward: -0.014 [-0.495,  0.000], mean action: 1.257 [0.000, 3.000],  loss: 0.048027, mae: 1.896511, mean_q: 2.591823, mean_eps: 0.935848\n"," 71324/100000: episode: 1796, duration: 2.094s, episode steps:  26, steps per second:  12, episode reward: -0.493, mean reward: -0.019 [-0.493,  0.000], mean action: 1.808 [0.000, 3.000],  loss: 0.023066, mae: 1.822963, mean_q: 2.502107, mean_eps: 0.935821\n"," 71355/100000: episode: 1797, duration: 2.270s, episode steps:  31, steps per second:  14, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.839 [0.000, 3.000],  loss: 0.025069, mae: 1.880315, mean_q: 2.566356, mean_eps: 0.935795\n"," 71409/100000: episode: 1798, duration: 4.066s, episode steps:  54, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.593 [0.000, 3.000],  loss: 0.026184, mae: 1.834545, mean_q: 2.518462, mean_eps: 0.935757\n"," 71435/100000: episode: 1799, duration: 2.044s, episode steps:  26, steps per second:  13, episode reward: -0.489, mean reward: -0.019 [-0.489,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.030405, mae: 1.894805, mean_q: 2.592196, mean_eps: 0.935721\n","\n","Target network updated after 30 episodes\n"," 71460/100000: episode: 1800, duration: 1.897s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.019068, mae: 1.899838, mean_q: 2.603454, mean_eps: 0.935698\n"," 71531/100000: episode: 1801, duration: 5.355s, episode steps:  71, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.239 [0.000, 3.000],  loss: 0.050858, mae: 1.885158, mean_q: 2.587599, mean_eps: 0.935655\n"," 71558/100000: episode: 1802, duration: 2.070s, episode steps:  27, steps per second:  13, episode reward: -0.486, mean reward: -0.018 [-0.486,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.041667, mae: 1.848933, mean_q: 2.536146, mean_eps: 0.935610\n"," 71588/100000: episode: 1803, duration: 2.269s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.133 [0.000, 3.000],  loss: 0.046249, mae: 1.896666, mean_q: 2.593101, mean_eps: 0.935585\n"," 71615/100000: episode: 1804, duration: 2.033s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.037448, mae: 1.876235, mean_q: 2.583593, mean_eps: 0.935559\n"," 71643/100000: episode: 1805, duration: 2.339s, episode steps:  28, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.029821, mae: 1.866429, mean_q: 2.560186, mean_eps: 0.935534\n"," 71675/100000: episode: 1806, duration: 2.455s, episode steps:  32, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.024240, mae: 1.872987, mean_q: 2.568661, mean_eps: 0.935507\n"," 71701/100000: episode: 1807, duration: 2.001s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.037193, mae: 1.882512, mean_q: 2.569209, mean_eps: 0.935481\n"," 71726/100000: episode: 1808, duration: 1.909s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.032562, mae: 1.898664, mean_q: 2.603244, mean_eps: 0.935458\n"," 71797/100000: episode: 1809, duration: 5.339s, episode steps:  71, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.437 [0.000, 3.000],  loss: 0.032626, mae: 1.896005, mean_q: 2.580499, mean_eps: 0.935415\n"," 71823/100000: episode: 1810, duration: 2.043s, episode steps:  26, steps per second:  13, episode reward: -0.486, mean reward: -0.019 [-0.486,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.023529, mae: 1.824624, mean_q: 2.493201, mean_eps: 0.935371\n"," 71852/100000: episode: 1811, duration: 2.260s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.038935, mae: 1.877158, mean_q: 2.565065, mean_eps: 0.935347\n"," 71877/100000: episode: 1812, duration: 1.922s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.024423, mae: 1.823823, mean_q: 2.489690, mean_eps: 0.935322\n"," 71902/100000: episode: 1813, duration: 1.924s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.029713, mae: 1.798294, mean_q: 2.474572, mean_eps: 0.935300\n"," 71928/100000: episode: 1814, duration: 1.960s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.035661, mae: 1.831075, mean_q: 2.492366, mean_eps: 0.935277\n"," 71956/100000: episode: 1815, duration: 2.248s, episode steps:  28, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.893 [0.000, 3.000],  loss: 0.026693, mae: 1.923736, mean_q: 2.622528, mean_eps: 0.935253\n"," 71982/100000: episode: 1816, duration: 2.013s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.035113, mae: 1.845943, mean_q: 2.522359, mean_eps: 0.935228\n"," 72057/100000: episode: 1817, duration: 5.616s, episode steps:  75, steps per second:  13, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.453 [0.000, 3.000],  loss: 0.031029, mae: 1.885607, mean_q: 2.585183, mean_eps: 0.935183\n"," 72160/100000: episode: 1818, duration: 7.890s, episode steps: 103, steps per second:  13, episode reward:  1.915, mean reward:  0.019 [-0.485,  1.200], mean action: 1.621 [0.000, 3.000],  loss: 0.030609, mae: 1.883701, mean_q: 2.577376, mean_eps: 0.935103\n"," 72186/100000: episode: 1819, duration: 1.965s, episode steps:  26, steps per second:  13, episode reward: -0.479, mean reward: -0.018 [-0.479,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.027011, mae: 1.891721, mean_q: 2.591731, mean_eps: 0.935045\n"," 72211/100000: episode: 1820, duration: 1.935s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 2.040 [0.000, 3.000],  loss: 0.025961, mae: 1.832377, mean_q: 2.502469, mean_eps: 0.935022\n"," 72236/100000: episode: 1821, duration: 1.916s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.029222, mae: 1.912447, mean_q: 2.611591, mean_eps: 0.934999\n"," 72337/100000: episode: 1822, duration: 7.517s, episode steps: 101, steps per second:  13, episode reward:  1.905, mean reward:  0.019 [-0.495,  1.200], mean action: 1.347 [0.000, 3.000],  loss: 0.025371, mae: 1.873903, mean_q: 2.561054, mean_eps: 0.934943\n"," 72364/100000: episode: 1823, duration: 2.068s, episode steps:  27, steps per second:  13, episode reward: -0.480, mean reward: -0.018 [-0.480,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.029738, mae: 1.949273, mean_q: 2.670984, mean_eps: 0.934885\n"," 72423/100000: episode: 1824, duration: 4.502s, episode steps:  59, steps per second:  13, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.576 [0.000, 3.000],  loss: 0.030922, mae: 1.872168, mean_q: 2.562840, mean_eps: 0.934846\n"," 72452/100000: episode: 1825, duration: 2.389s, episode steps:  29, steps per second:  12, episode reward: -0.488, mean reward: -0.017 [-0.488,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.033048, mae: 1.873645, mean_q: 2.559360, mean_eps: 0.934807\n"," 72562/100000: episode: 1826, duration: 8.103s, episode steps: 110, steps per second:  14, episode reward:  1.906, mean reward:  0.017 [-0.494,  1.200], mean action: 1.636 [0.000, 3.000],  loss: 0.028665, mae: 1.882439, mean_q: 2.574362, mean_eps: 0.934744\n"," 72587/100000: episode: 1827, duration: 1.847s, episode steps:  25, steps per second:  14, episode reward: -0.478, mean reward: -0.019 [-0.478,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.033102, mae: 1.861972, mean_q: 2.550776, mean_eps: 0.934683\n"," 72612/100000: episode: 1828, duration: 1.989s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.028562, mae: 1.874351, mean_q: 2.563686, mean_eps: 0.934661\n"," 72638/100000: episode: 1829, duration: 2.003s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.020317, mae: 1.876110, mean_q: 2.575189, mean_eps: 0.934638\n","\n","Target network updated after 30 episodes\n"," 72668/100000: episode: 1830, duration: 2.307s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.900 [0.000, 3.000],  loss: 0.030320, mae: 1.908601, mean_q: 2.608878, mean_eps: 0.934613\n"," 72721/100000: episode: 1831, duration: 3.971s, episode steps:  53, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.528 [0.000, 3.000],  loss: 0.051055, mae: 1.907434, mean_q: 2.598260, mean_eps: 0.934575\n"," 72746/100000: episode: 1832, duration: 1.905s, episode steps:  25, steps per second:  13, episode reward: -0.489, mean reward: -0.020 [-0.489,  0.000], mean action: 1.160 [0.000, 3.000],  loss: 0.044803, mae: 1.907212, mean_q: 2.602489, mean_eps: 0.934540\n"," 72774/100000: episode: 1833, duration: 2.217s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.038987, mae: 1.864348, mean_q: 2.539951, mean_eps: 0.934516\n"," 72804/100000: episode: 1834, duration: 2.282s, episode steps:  30, steps per second:  13, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.040202, mae: 1.904639, mean_q: 2.615280, mean_eps: 0.934490\n"," 72863/100000: episode: 1835, duration: 4.391s, episode steps:  59, steps per second:  13, episode reward:  0.706, mean reward:  0.012 [-0.494,  1.200], mean action: 1.407 [0.000, 3.000],  loss: 0.035559, mae: 1.918052, mean_q: 2.628323, mean_eps: 0.934450\n"," 72924/100000: episode: 1836, duration: 4.587s, episode steps:  61, steps per second:  13, episode reward:  0.712, mean reward:  0.012 [-0.488,  1.200], mean action: 1.590 [0.000, 3.000],  loss: 0.041273, mae: 1.880540, mean_q: 2.581354, mean_eps: 0.934396\n"," 72950/100000: episode: 1837, duration: 1.992s, episode steps:  26, steps per second:  13, episode reward: -0.488, mean reward: -0.019 [-0.488,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.019930, mae: 1.946215, mean_q: 2.662716, mean_eps: 0.934357\n"," 73007/100000: episode: 1838, duration: 4.261s, episode steps:  57, steps per second:  13, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.614 [0.000, 3.000],  loss: 0.025092, mae: 1.906140, mean_q: 2.614789, mean_eps: 0.934320\n"," 73033/100000: episode: 1839, duration: 1.954s, episode steps:  26, steps per second:  13, episode reward: -0.489, mean reward: -0.019 [-0.489,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.035043, mae: 1.844482, mean_q: 2.515079, mean_eps: 0.934282\n"," 73059/100000: episode: 1840, duration: 1.948s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.029994, mae: 1.898001, mean_q: 2.605885, mean_eps: 0.934259\n"," 73086/100000: episode: 1841, duration: 2.137s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.033930, mae: 1.848505, mean_q: 2.533982, mean_eps: 0.934235\n"," 73113/100000: episode: 1842, duration: 2.019s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.037677, mae: 1.929545, mean_q: 2.639530, mean_eps: 0.934211\n"," 73141/100000: episode: 1843, duration: 2.140s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.046713, mae: 1.844947, mean_q: 2.537621, mean_eps: 0.934186\n"," 73170/100000: episode: 1844, duration: 2.157s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.036938, mae: 1.856292, mean_q: 2.546031, mean_eps: 0.934160\n"," 73224/100000: episode: 1845, duration: 4.070s, episode steps:  54, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.444 [0.000, 3.000],  loss: 0.034892, mae: 1.889666, mean_q: 2.590640, mean_eps: 0.934123\n"," 73250/100000: episode: 1846, duration: 2.145s, episode steps:  26, steps per second:  12, episode reward: -0.489, mean reward: -0.019 [-0.489,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.037487, mae: 1.871934, mean_q: 2.564437, mean_eps: 0.934087\n"," 73312/100000: episode: 1847, duration: 4.640s, episode steps:  62, steps per second:  13, episode reward:  0.705, mean reward:  0.011 [-0.495,  1.200], mean action: 1.403 [0.000, 3.000],  loss: 0.034693, mae: 1.853128, mean_q: 2.546915, mean_eps: 0.934048\n"," 73338/100000: episode: 1848, duration: 1.984s, episode steps:  26, steps per second:  13, episode reward: -0.488, mean reward: -0.019 [-0.488,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.041023, mae: 1.872144, mean_q: 2.563578, mean_eps: 0.934008\n"," 73364/100000: episode: 1849, duration: 1.949s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.192 [0.000, 3.000],  loss: 0.038585, mae: 1.841083, mean_q: 2.548123, mean_eps: 0.933985\n"," 73389/100000: episode: 1850, duration: 1.985s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.021300, mae: 1.882621, mean_q: 2.581873, mean_eps: 0.933962\n"," 73417/100000: episode: 1851, duration: 2.254s, episode steps:  28, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.143 [0.000, 3.000],  loss: 0.030275, mae: 1.871194, mean_q: 2.561062, mean_eps: 0.933938\n"," 73445/100000: episode: 1852, duration: 2.085s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.179 [0.000, 3.000],  loss: 0.035889, mae: 1.906403, mean_q: 2.616971, mean_eps: 0.933913\n"," 73500/100000: episode: 1853, duration: 4.066s, episode steps:  55, steps per second:  14, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.291 [0.000, 3.000],  loss: 0.036469, mae: 1.845945, mean_q: 2.544108, mean_eps: 0.933875\n"," 73529/100000: episode: 1854, duration: 2.167s, episode steps:  29, steps per second:  13, episode reward: -0.489, mean reward: -0.017 [-0.489,  0.000], mean action: 1.345 [0.000, 3.000],  loss: 0.033547, mae: 1.849220, mean_q: 2.523772, mean_eps: 0.933837\n"," 73554/100000: episode: 1855, duration: 1.942s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.027767, mae: 1.901415, mean_q: 2.606138, mean_eps: 0.933813\n"," 73579/100000: episode: 1856, duration: 1.995s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.920 [0.000, 3.000],  loss: 0.033777, mae: 1.844667, mean_q: 2.530764, mean_eps: 0.933791\n"," 73604/100000: episode: 1857, duration: 1.941s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.030946, mae: 1.899414, mean_q: 2.605714, mean_eps: 0.933768\n"," 73630/100000: episode: 1858, duration: 1.978s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.033936, mae: 1.868043, mean_q: 2.555793, mean_eps: 0.933745\n"," 73656/100000: episode: 1859, duration: 1.979s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.038404, mae: 1.886865, mean_q: 2.580709, mean_eps: 0.933722\n","\n","Target network updated after 30 episodes\n"," 73681/100000: episode: 1860, duration: 1.839s, episode steps:  25, steps per second:  14, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.026397, mae: 1.847407, mean_q: 2.537896, mean_eps: 0.933699\n"," 73744/100000: episode: 1861, duration: 4.696s, episode steps:  63, steps per second:  13, episode reward:  0.705, mean reward:  0.011 [-0.495,  1.200], mean action: 1.714 [0.000, 3.000],  loss: 0.050491, mae: 1.888714, mean_q: 2.575404, mean_eps: 0.933659\n"," 73774/100000: episode: 1862, duration: 2.236s, episode steps:  30, steps per second:  13, episode reward: -0.487, mean reward: -0.016 [-0.487,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.039816, mae: 1.883214, mean_q: 2.564170, mean_eps: 0.933617\n"," 73800/100000: episode: 1863, duration: 1.967s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.192 [0.000, 3.000],  loss: 0.045522, mae: 1.893255, mean_q: 2.586681, mean_eps: 0.933592\n"," 73828/100000: episode: 1864, duration: 2.098s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.214 [0.000, 3.000],  loss: 0.033941, mae: 1.887487, mean_q: 2.568711, mean_eps: 0.933568\n"," 73857/100000: episode: 1865, duration: 2.180s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.023982, mae: 1.848279, mean_q: 2.521273, mean_eps: 0.933542\n"," 74050/100000: episode: 1866, duration: 14.443s, episode steps: 193, steps per second:  13, episode reward:  4.306, mean reward:  0.022 [-0.494,  1.200], mean action: 1.513 [0.000, 3.000],  loss: 0.031339, mae: 1.884232, mean_q: 2.572900, mean_eps: 0.933442\n"," 74078/100000: episode: 1867, duration: 2.111s, episode steps:  28, steps per second:  13, episode reward: -0.461, mean reward: -0.016 [-0.461,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.028269, mae: 1.857916, mean_q: 2.544975, mean_eps: 0.933343\n"," 74103/100000: episode: 1868, duration: 1.900s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.031463, mae: 1.951150, mean_q: 2.655782, mean_eps: 0.933319\n"," 74130/100000: episode: 1869, duration: 2.024s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.035673, mae: 1.837842, mean_q: 2.518170, mean_eps: 0.933296\n"," 74161/100000: episode: 1870, duration: 2.333s, episode steps:  31, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 0.037070, mae: 1.905987, mean_q: 2.604231, mean_eps: 0.933269\n"," 74186/100000: episode: 1871, duration: 1.873s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.027031, mae: 1.887754, mean_q: 2.585722, mean_eps: 0.933244\n"," 74313/100000: episode: 1872, duration: 9.299s, episode steps: 127, steps per second:  14, episode reward:  1.905, mean reward:  0.015 [-0.495,  1.200], mean action: 1.606 [0.000, 3.000],  loss: 0.028628, mae: 1.863032, mean_q: 2.551655, mean_eps: 0.933176\n"," 74341/100000: episode: 1873, duration: 2.086s, episode steps:  28, steps per second:  13, episode reward: -0.475, mean reward: -0.017 [-0.475,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.032607, mae: 1.850124, mean_q: 2.525684, mean_eps: 0.933106\n"," 74373/100000: episode: 1874, duration: 2.464s, episode steps:  32, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.025590, mae: 1.859647, mean_q: 2.543325, mean_eps: 0.933079\n"," 74426/100000: episode: 1875, duration: 4.078s, episode steps:  53, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.698 [0.000, 3.000],  loss: 0.035408, mae: 1.869303, mean_q: 2.551423, mean_eps: 0.933041\n"," 74481/100000: episode: 1876, duration: 4.102s, episode steps:  55, steps per second:  13, episode reward:  0.711, mean reward:  0.013 [-0.489,  1.200], mean action: 1.764 [0.000, 3.000],  loss: 0.028862, mae: 1.882522, mean_q: 2.575027, mean_eps: 0.932992\n"," 74519/100000: episode: 1877, duration: 2.898s, episode steps:  38, steps per second:  13, episode reward: -0.489, mean reward: -0.013 [-0.489,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.031643, mae: 1.868191, mean_q: 2.559519, mean_eps: 0.932950\n"," 74546/100000: episode: 1878, duration: 2.118s, episode steps:  27, steps per second:  13, episode reward: -0.492, mean reward: -0.018 [-0.492,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.036243, mae: 1.930540, mean_q: 2.625445, mean_eps: 0.932921\n"," 74582/100000: episode: 1879, duration: 2.663s, episode steps:  36, steps per second:  14, episode reward: -0.495, mean reward: -0.014 [-0.495,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.041209, mae: 1.849504, mean_q: 2.527848, mean_eps: 0.932893\n"," 74607/100000: episode: 1880, duration: 1.870s, episode steps:  25, steps per second:  13, episode reward: -0.493, mean reward: -0.020 [-0.493,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.028108, mae: 1.837366, mean_q: 2.512454, mean_eps: 0.932865\n"," 74632/100000: episode: 1881, duration: 1.877s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.035382, mae: 1.915416, mean_q: 2.602505, mean_eps: 0.932843\n"," 74657/100000: episode: 1882, duration: 1.886s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.240 [0.000, 3.000],  loss: 0.033389, mae: 1.928681, mean_q: 2.624312, mean_eps: 0.932820\n"," 74682/100000: episode: 1883, duration: 1.928s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.028014, mae: 1.913186, mean_q: 2.610833, mean_eps: 0.932798\n"," 74709/100000: episode: 1884, duration: 2.173s, episode steps:  27, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.030974, mae: 1.924947, mean_q: 2.614829, mean_eps: 0.932774\n"," 74784/100000: episode: 1885, duration: 5.541s, episode steps:  75, steps per second:  14, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.853 [0.000, 3.000],  loss: 0.031705, mae: 1.864226, mean_q: 2.545390, mean_eps: 0.932729\n"," 74809/100000: episode: 1886, duration: 1.895s, episode steps:  25, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.028243, mae: 1.813391, mean_q: 2.480191, mean_eps: 0.932684\n"," 74834/100000: episode: 1887, duration: 1.861s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.049297, mae: 1.857142, mean_q: 2.539121, mean_eps: 0.932661\n"," 74862/100000: episode: 1888, duration: 2.216s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.029955, mae: 1.918595, mean_q: 2.619757, mean_eps: 0.932637\n"," 74962/100000: episode: 1889, duration: 7.257s, episode steps: 100, steps per second:  14, episode reward:  1.906, mean reward:  0.019 [-0.494,  1.200], mean action: 1.280 [0.000, 3.000],  loss: 0.028276, mae: 1.893664, mean_q: 2.595569, mean_eps: 0.932580\n","\n","Target network updated after 30 episodes\n"," 75093/100000: episode: 1890, duration: 9.663s, episode steps: 131, steps per second:  14, episode reward:  3.120, mean reward:  0.024 [-0.480,  1.200], mean action: 1.580 [0.000, 3.000],  loss: 0.034598, mae: 1.865623, mean_q: 2.550529, mean_eps: 0.932476\n"," 75151/100000: episode: 1891, duration: 4.291s, episode steps:  58, steps per second:  14, episode reward:  0.726, mean reward:  0.013 [-0.474,  1.200], mean action: 1.500 [0.000, 3.000],  loss: 0.088572, mae: 1.889731, mean_q: 2.570152, mean_eps: 0.932391\n"," 75181/100000: episode: 1892, duration: 2.344s, episode steps:  30, steps per second:  13, episode reward: -0.488, mean reward: -0.016 [-0.488,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.050537, mae: 1.852076, mean_q: 2.525648, mean_eps: 0.932351\n"," 75282/100000: episode: 1893, duration: 7.453s, episode steps: 101, steps per second:  14, episode reward:  1.906, mean reward:  0.019 [-0.494,  1.200], mean action: 1.525 [0.000, 3.000],  loss: 0.064587, mae: 1.878120, mean_q: 2.576045, mean_eps: 0.932292\n"," 75307/100000: episode: 1894, duration: 1.917s, episode steps:  25, steps per second:  13, episode reward: -0.480, mean reward: -0.019 [-0.480,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.089472, mae: 1.900780, mean_q: 2.605347, mean_eps: 0.932235\n"," 75364/100000: episode: 1895, duration: 4.371s, episode steps:  57, steps per second:  13, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.877 [0.000, 3.000],  loss: 0.058450, mae: 1.852447, mean_q: 2.562405, mean_eps: 0.932199\n"," 75438/100000: episode: 1896, duration: 5.462s, episode steps:  74, steps per second:  14, episode reward:  0.711, mean reward:  0.010 [-0.489,  1.200], mean action: 1.473 [0.000, 3.000],  loss: 0.037828, mae: 1.872820, mean_q: 2.585849, mean_eps: 0.932140\n"," 75464/100000: episode: 1897, duration: 1.985s, episode steps:  26, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.064732, mae: 1.901144, mean_q: 2.634403, mean_eps: 0.932095\n"," 75491/100000: episode: 1898, duration: 2.032s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.926 [0.000, 3.000],  loss: 0.035338, mae: 1.856744, mean_q: 2.570632, mean_eps: 0.932071\n"," 75516/100000: episode: 1899, duration: 1.988s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.083284, mae: 1.971301, mean_q: 2.707703, mean_eps: 0.932047\n"," 75545/100000: episode: 1900, duration: 2.199s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.759 [0.000, 3.000],  loss: 0.051649, mae: 1.853633, mean_q: 2.567961, mean_eps: 0.932023\n"," 75570/100000: episode: 1901, duration: 1.852s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.060880, mae: 1.945605, mean_q: 2.685863, mean_eps: 0.931999\n"," 75595/100000: episode: 1902, duration: 1.938s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.044211, mae: 1.851421, mean_q: 2.551711, mean_eps: 0.931976\n"," 75630/100000: episode: 1903, duration: 2.653s, episode steps:  35, steps per second:  13, episode reward: -0.495, mean reward: -0.014 [-0.495,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.043931, mae: 1.909882, mean_q: 2.630407, mean_eps: 0.931949\n"," 75656/100000: episode: 1904, duration: 1.993s, episode steps:  26, steps per second:  13, episode reward: -0.493, mean reward: -0.019 [-0.493,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.042394, mae: 1.865752, mean_q: 2.591539, mean_eps: 0.931922\n"," 75686/100000: episode: 1905, duration: 2.373s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.052757, mae: 1.840173, mean_q: 2.540780, mean_eps: 0.931897\n"," 75720/100000: episode: 1906, duration: 2.498s, episode steps:  34, steps per second:  14, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.176 [0.000, 3.000],  loss: 0.033399, mae: 1.867915, mean_q: 2.574483, mean_eps: 0.931868\n"," 75750/100000: episode: 1907, duration: 2.239s, episode steps:  30, steps per second:  13, episode reward: -0.493, mean reward: -0.016 [-0.493,  0.000], mean action: 1.433 [0.000, 3.000],  loss: 0.052047, mae: 1.837940, mean_q: 2.528602, mean_eps: 0.931839\n"," 75778/100000: episode: 1908, duration: 2.164s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.052842, mae: 1.845416, mean_q: 2.529684, mean_eps: 0.931813\n"," 75811/100000: episode: 1909, duration: 2.466s, episode steps:  33, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.273 [0.000, 3.000],  loss: 0.058048, mae: 1.802079, mean_q: 2.486486, mean_eps: 0.931785\n"," 75884/100000: episode: 1910, duration: 5.621s, episode steps:  73, steps per second:  13, episode reward:  0.707, mean reward:  0.010 [-0.493,  1.200], mean action: 1.534 [0.000, 3.000],  loss: 0.053343, mae: 1.869822, mean_q: 2.574730, mean_eps: 0.931738\n"," 75958/100000: episode: 1911, duration: 5.424s, episode steps:  74, steps per second:  14, episode reward:  0.715, mean reward:  0.010 [-0.485,  1.200], mean action: 1.338 [0.000, 3.000],  loss: 0.054363, mae: 1.903851, mean_q: 2.619320, mean_eps: 0.931672\n"," 75987/100000: episode: 1912, duration: 2.242s, episode steps:  29, steps per second:  13, episode reward: -0.485, mean reward: -0.017 [-0.485,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.043369, mae: 1.898339, mean_q: 2.613568, mean_eps: 0.931625\n"," 76016/100000: episode: 1913, duration: 2.273s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.862 [0.000, 3.000],  loss: 0.058945, mae: 1.869871, mean_q: 2.565485, mean_eps: 0.931599\n"," 76048/100000: episode: 1914, duration: 2.419s, episode steps:  32, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.054294, mae: 1.853855, mean_q: 2.552084, mean_eps: 0.931572\n"," 76102/100000: episode: 1915, duration: 3.977s, episode steps:  54, steps per second:  14, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.741 [0.000, 3.000],  loss: 0.068716, mae: 1.842495, mean_q: 2.543552, mean_eps: 0.931533\n"," 76253/100000: episode: 1916, duration: 11.115s, episode steps: 151, steps per second:  14, episode reward:  3.111, mean reward:  0.021 [-0.489,  1.200], mean action: 1.596 [0.000, 3.000],  loss: 0.050969, mae: 1.860197, mean_q: 2.572941, mean_eps: 0.931441\n"," 76279/100000: episode: 1917, duration: 1.936s, episode steps:  26, steps per second:  13, episode reward: -0.470, mean reward: -0.018 [-0.470,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.054862, mae: 1.821540, mean_q: 2.507322, mean_eps: 0.931361\n"," 76309/100000: episode: 1918, duration: 2.224s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.069847, mae: 1.936954, mean_q: 2.671152, mean_eps: 0.931336\n"," 76383/100000: episode: 1919, duration: 5.533s, episode steps:  74, steps per second:  13, episode reward:  0.706, mean reward:  0.010 [-0.494,  1.200], mean action: 1.405 [0.000, 3.000],  loss: 0.051365, mae: 1.833977, mean_q: 2.540756, mean_eps: 0.931289\n","\n","Target network updated after 30 episodes\n"," 76408/100000: episode: 1920, duration: 1.925s, episode steps:  25, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 2.040 [0.000, 3.000],  loss: 0.049180, mae: 1.822869, mean_q: 2.520195, mean_eps: 0.931244\n"," 76433/100000: episode: 1921, duration: 1.894s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.076218, mae: 1.859744, mean_q: 2.550814, mean_eps: 0.931222\n"," 76459/100000: episode: 1922, duration: 1.987s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.051875, mae: 1.888666, mean_q: 2.586617, mean_eps: 0.931199\n"," 76484/100000: episode: 1923, duration: 1.992s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.057315, mae: 1.920295, mean_q: 2.625016, mean_eps: 0.931176\n"," 76514/100000: episode: 1924, duration: 2.242s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.044589, mae: 1.905803, mean_q: 2.628763, mean_eps: 0.931151\n"," 76567/100000: episode: 1925, duration: 3.929s, episode steps:  53, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.453 [0.000, 3.000],  loss: 0.037152, mae: 1.908797, mean_q: 2.614319, mean_eps: 0.931114\n"," 76600/100000: episode: 1926, duration: 2.389s, episode steps:  33, steps per second:  14, episode reward: -0.489, mean reward: -0.015 [-0.489,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.055924, mae: 1.851984, mean_q: 2.536053, mean_eps: 0.931075\n"," 76627/100000: episode: 1927, duration: 2.023s, episode steps:  27, steps per second:  13, episode reward: -0.493, mean reward: -0.018 [-0.493,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.040961, mae: 1.884330, mean_q: 2.586970, mean_eps: 0.931048\n"," 76654/100000: episode: 1928, duration: 2.179s, episode steps:  27, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.049678, mae: 1.903527, mean_q: 2.606064, mean_eps: 0.931024\n"," 76685/100000: episode: 1929, duration: 2.355s, episode steps:  31, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.419 [0.000, 3.000],  loss: 0.034201, mae: 1.834100, mean_q: 2.524034, mean_eps: 0.930998\n"," 76717/100000: episode: 1930, duration: 2.365s, episode steps:  32, steps per second:  14, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.048853, mae: 1.887966, mean_q: 2.587677, mean_eps: 0.930970\n"," 76794/100000: episode: 1931, duration: 5.690s, episode steps:  77, steps per second:  14, episode reward:  0.706, mean reward:  0.009 [-0.494,  1.200], mean action: 1.740 [0.000, 3.000],  loss: 0.050808, mae: 1.887235, mean_q: 2.591985, mean_eps: 0.930920\n"," 76821/100000: episode: 1932, duration: 2.156s, episode steps:  27, steps per second:  13, episode reward: -0.485, mean reward: -0.018 [-0.485,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.044882, mae: 1.878103, mean_q: 2.573585, mean_eps: 0.930874\n"," 76851/100000: episode: 1933, duration: 2.280s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.036144, mae: 1.888708, mean_q: 2.587404, mean_eps: 0.930848\n"," 76876/100000: episode: 1934, duration: 1.899s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.047331, mae: 1.796198, mean_q: 2.457040, mean_eps: 0.930823\n"," 76904/100000: episode: 1935, duration: 2.075s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.039963, mae: 1.910883, mean_q: 2.605062, mean_eps: 0.930799\n"," 76929/100000: episode: 1936, duration: 1.905s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.044316, mae: 1.885817, mean_q: 2.571257, mean_eps: 0.930776\n"," 76954/100000: episode: 1937, duration: 1.911s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 2.160 [0.000, 3.000],  loss: 0.039035, mae: 1.838040, mean_q: 2.509024, mean_eps: 0.930753\n"," 76985/100000: episode: 1938, duration: 2.343s, episode steps:  31, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.062054, mae: 1.948659, mean_q: 2.645687, mean_eps: 0.930728\n"," 77059/100000: episode: 1939, duration: 5.442s, episode steps:  74, steps per second:  14, episode reward:  0.706, mean reward:  0.010 [-0.494,  1.200], mean action: 1.500 [0.000, 3.000],  loss: 0.042990, mae: 1.829584, mean_q: 2.502173, mean_eps: 0.930681\n"," 77163/100000: episode: 1940, duration: 7.736s, episode steps: 104, steps per second:  13, episode reward:  1.915, mean reward:  0.018 [-0.485,  1.200], mean action: 1.587 [0.000, 3.000],  loss: 0.039906, mae: 1.891733, mean_q: 2.581669, mean_eps: 0.930601\n"," 77193/100000: episode: 1941, duration: 2.285s, episode steps:  30, steps per second:  13, episode reward: -0.479, mean reward: -0.016 [-0.479,  0.000], mean action: 1.433 [0.000, 3.000],  loss: 0.042598, mae: 1.893295, mean_q: 2.588891, mean_eps: 0.930540\n"," 77223/100000: episode: 1942, duration: 2.265s, episode steps:  30, steps per second:  13, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.038986, mae: 1.889432, mean_q: 2.592233, mean_eps: 0.930513\n"," 77249/100000: episode: 1943, duration: 1.923s, episode steps:  26, steps per second:  14, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.024389, mae: 1.885909, mean_q: 2.570133, mean_eps: 0.930488\n"," 77274/100000: episode: 1944, duration: 1.931s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.037376, mae: 1.831211, mean_q: 2.497354, mean_eps: 0.930465\n"," 77304/100000: episode: 1945, duration: 2.359s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.050484, mae: 1.872877, mean_q: 2.547622, mean_eps: 0.930440\n"," 77330/100000: episode: 1946, duration: 1.965s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.033768, mae: 1.859649, mean_q: 2.534812, mean_eps: 0.930415\n"," 77359/100000: episode: 1947, duration: 2.165s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.032082, mae: 1.832675, mean_q: 2.521234, mean_eps: 0.930390\n"," 77384/100000: episode: 1948, duration: 1.892s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.036695, mae: 1.933943, mean_q: 2.657011, mean_eps: 0.930366\n"," 77485/100000: episode: 1949, duration: 7.460s, episode steps: 101, steps per second:  14, episode reward:  1.905, mean reward:  0.019 [-0.495,  1.200], mean action: 1.307 [0.000, 3.000],  loss: 0.042530, mae: 1.851446, mean_q: 2.534300, mean_eps: 0.930309\n","\n","Target network updated after 30 episodes\n"," 77511/100000: episode: 1950, duration: 1.946s, episode steps:  26, steps per second:  13, episode reward: -0.480, mean reward: -0.018 [-0.480,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.031138, mae: 1.856145, mean_q: 2.546570, mean_eps: 0.930252\n"," 77552/100000: episode: 1951, duration: 3.032s, episode steps:  41, steps per second:  14, episode reward: -0.495, mean reward: -0.012 [-0.495,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.065113, mae: 1.894252, mean_q: 2.565934, mean_eps: 0.930222\n"," 77580/100000: episode: 1952, duration: 2.099s, episode steps:  28, steps per second:  13, episode reward: -0.492, mean reward: -0.018 [-0.492,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.049748, mae: 1.868485, mean_q: 2.543988, mean_eps: 0.930191\n"," 77608/100000: episode: 1953, duration: 2.132s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.321 [0.000, 3.000],  loss: 0.038448, mae: 1.903943, mean_q: 2.596936, mean_eps: 0.930166\n"," 77667/100000: episode: 1954, duration: 4.404s, episode steps:  59, steps per second:  13, episode reward:  0.706, mean reward:  0.012 [-0.494,  1.200], mean action: 1.542 [0.000, 3.000],  loss: 0.042782, mae: 1.824148, mean_q: 2.501172, mean_eps: 0.930127\n"," 77698/100000: episode: 1955, duration: 2.321s, episode steps:  31, steps per second:  13, episode reward: -0.488, mean reward: -0.016 [-0.488,  0.000], mean action: 1.548 [0.000, 3.000],  loss: 0.045226, mae: 1.832860, mean_q: 2.514784, mean_eps: 0.930086\n"," 77723/100000: episode: 1956, duration: 1.859s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.840 [0.000, 3.000],  loss: 0.038829, mae: 1.775054, mean_q: 2.453576, mean_eps: 0.930061\n"," 77873/100000: episode: 1957, duration: 11.062s, episode steps: 150, steps per second:  14, episode reward:  3.105, mean reward:  0.021 [-0.495,  1.200], mean action: 1.673 [0.000, 3.000],  loss: 0.056549, mae: 1.872024, mean_q: 2.565220, mean_eps: 0.929982\n"," 77898/100000: episode: 1958, duration: 1.886s, episode steps:  25, steps per second:  13, episode reward: -0.470, mean reward: -0.019 [-0.470,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.046174, mae: 1.879041, mean_q: 2.565203, mean_eps: 0.929904\n"," 77924/100000: episode: 1959, duration: 1.966s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.042937, mae: 1.903092, mean_q: 2.608070, mean_eps: 0.929881\n"," 77951/100000: episode: 1960, duration: 2.228s, episode steps:  27, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.037675, mae: 1.901260, mean_q: 2.614366, mean_eps: 0.929857\n"," 77977/100000: episode: 1961, duration: 1.991s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.044017, mae: 1.857029, mean_q: 2.539758, mean_eps: 0.929833\n"," 78005/100000: episode: 1962, duration: 2.129s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.036610, mae: 1.841640, mean_q: 2.512770, mean_eps: 0.929809\n"," 78178/100000: episode: 1963, duration: 12.654s, episode steps: 173, steps per second:  14, episode reward:  4.306, mean reward:  0.025 [-0.494,  1.200], mean action: 1.486 [0.000, 3.000],  loss: 0.044197, mae: 1.861251, mean_q: 2.549356, mean_eps: 0.929718\n"," 78204/100000: episode: 1964, duration: 1.944s, episode steps:  26, steps per second:  13, episode reward: -0.465, mean reward: -0.018 [-0.465,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.051457, mae: 1.865465, mean_q: 2.559306, mean_eps: 0.929629\n"," 78229/100000: episode: 1965, duration: 1.903s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.038168, mae: 1.891716, mean_q: 2.593648, mean_eps: 0.929606\n"," 78289/100000: episode: 1966, duration: 4.494s, episode steps:  60, steps per second:  13, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.467 [0.000, 3.000],  loss: 0.034016, mae: 1.859272, mean_q: 2.563397, mean_eps: 0.929567\n"," 78365/100000: episode: 1967, duration: 5.570s, episode steps:  76, steps per second:  14, episode reward:  0.712, mean reward:  0.009 [-0.488,  1.200], mean action: 1.592 [0.000, 3.000],  loss: 0.046035, mae: 1.839910, mean_q: 2.524620, mean_eps: 0.929506\n"," 78392/100000: episode: 1968, duration: 2.054s, episode steps:  27, steps per second:  13, episode reward: -0.485, mean reward: -0.018 [-0.485,  0.000], mean action: 1.852 [0.000, 3.000],  loss: 0.036336, mae: 1.891382, mean_q: 2.584149, mean_eps: 0.929460\n"," 78421/100000: episode: 1969, duration: 2.188s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.069 [0.000, 3.000],  loss: 0.034000, mae: 1.846686, mean_q: 2.528501, mean_eps: 0.929435\n"," 78449/100000: episode: 1970, duration: 2.218s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.033485, mae: 1.843794, mean_q: 2.514575, mean_eps: 0.929409\n"," 78474/100000: episode: 1971, duration: 1.885s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 2.080 [0.000, 3.000],  loss: 0.036566, mae: 1.896192, mean_q: 2.593369, mean_eps: 0.929385\n"," 78501/100000: episode: 1972, duration: 2.043s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.296 [0.000, 3.000],  loss: 0.035619, mae: 1.871913, mean_q: 2.557331, mean_eps: 0.929362\n"," 78604/100000: episode: 1973, duration: 7.714s, episode steps: 103, steps per second:  13, episode reward:  1.905, mean reward:  0.018 [-0.495,  1.200], mean action: 1.534 [0.000, 3.000],  loss: 0.047423, mae: 1.889961, mean_q: 2.585844, mean_eps: 0.929303\n"," 78629/100000: episode: 1974, duration: 1.930s, episode steps:  25, steps per second:  13, episode reward: -0.479, mean reward: -0.019 [-0.479,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.059627, mae: 1.862933, mean_q: 2.551833, mean_eps: 0.929246\n"," 78737/100000: episode: 1975, duration: 8.041s, episode steps: 108, steps per second:  13, episode reward:  1.905, mean reward:  0.018 [-0.495,  1.200], mean action: 1.769 [0.000, 3.000],  loss: 0.038781, mae: 1.850285, mean_q: 2.542695, mean_eps: 0.929186\n"," 78906/100000: episode: 1976, duration: 12.472s, episode steps: 169, steps per second:  14, episode reward:  4.322, mean reward:  0.026 [-0.478,  1.200], mean action: 1.426 [0.000, 3.000],  loss: 0.038003, mae: 1.857564, mean_q: 2.541882, mean_eps: 0.929061\n"," 78937/100000: episode: 1977, duration: 2.398s, episode steps:  31, steps per second:  13, episode reward: -0.466, mean reward: -0.015 [-0.466,  0.000], mean action: 1.581 [0.000, 3.000],  loss: 0.054709, mae: 1.865578, mean_q: 2.545305, mean_eps: 0.928971\n"," 79023/100000: episode: 1978, duration: 6.292s, episode steps:  86, steps per second:  14, episode reward:  0.706, mean reward:  0.008 [-0.494,  1.200], mean action: 1.698 [0.000, 3.000],  loss: 0.052075, mae: 1.873933, mean_q: 2.561313, mean_eps: 0.928918\n"," 79054/100000: episode: 1979, duration: 2.299s, episode steps:  31, steps per second:  13, episode reward: -0.483, mean reward: -0.016 [-0.483,  0.000], mean action: 1.355 [0.000, 3.000],  loss: 0.030480, mae: 1.817989, mean_q: 2.492469, mean_eps: 0.928866\n","\n","Target network updated after 30 episodes\n"," 79084/100000: episode: 1980, duration: 2.330s, episode steps:  30, steps per second:  13, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.027586, mae: 1.848378, mean_q: 2.536885, mean_eps: 0.928838\n"," 79109/100000: episode: 1981, duration: 1.910s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.093984, mae: 1.965333, mean_q: 2.658106, mean_eps: 0.928814\n"," 79137/100000: episode: 1982, duration: 2.049s, episode steps:  28, steps per second:  14, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.064814, mae: 1.892035, mean_q: 2.571351, mean_eps: 0.928790\n"," 79163/100000: episode: 1983, duration: 1.896s, episode steps:  26, steps per second:  14, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.308 [0.000, 3.000],  loss: 0.066605, mae: 1.902961, mean_q: 2.601162, mean_eps: 0.928765\n"," 79189/100000: episode: 1984, duration: 1.956s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.051890, mae: 1.873820, mean_q: 2.569823, mean_eps: 0.928742\n"," 79245/100000: episode: 1985, duration: 4.180s, episode steps:  56, steps per second:  13, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.804 [0.000, 3.000],  loss: 0.064621, mae: 1.926162, mean_q: 2.651352, mean_eps: 0.928705\n"," 79319/100000: episode: 1986, duration: 5.462s, episode steps:  74, steps per second:  14, episode reward:  0.711, mean reward:  0.010 [-0.489,  1.200], mean action: 1.459 [0.000, 3.000],  loss: 0.052524, mae: 1.899078, mean_q: 2.615927, mean_eps: 0.928647\n"," 79345/100000: episode: 1987, duration: 1.956s, episode steps:  26, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.046224, mae: 1.932172, mean_q: 2.661545, mean_eps: 0.928602\n"," 79372/100000: episode: 1988, duration: 1.980s, episode steps:  27, steps per second:  14, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.046841, mae: 1.866221, mean_q: 2.567244, mean_eps: 0.928578\n"," 79397/100000: episode: 1989, duration: 1.907s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.045807, mae: 1.917314, mean_q: 2.634649, mean_eps: 0.928554\n"," 79469/100000: episode: 1990, duration: 5.420s, episode steps:  72, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.583 [0.000, 3.000],  loss: 0.060539, mae: 1.922576, mean_q: 2.654710, mean_eps: 0.928511\n"," 79570/100000: episode: 1991, duration: 7.470s, episode steps: 101, steps per second:  14, episode reward:  1.914, mean reward:  0.019 [-0.486,  1.200], mean action: 1.465 [0.000, 3.000],  loss: 0.048549, mae: 1.883295, mean_q: 2.594821, mean_eps: 0.928433\n"," 79595/100000: episode: 1992, duration: 1.982s, episode steps:  25, steps per second:  13, episode reward: -0.480, mean reward: -0.019 [-0.480,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.041590, mae: 1.927101, mean_q: 2.659659, mean_eps: 0.928376\n"," 79621/100000: episode: 1993, duration: 1.990s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.043559, mae: 1.907391, mean_q: 2.624965, mean_eps: 0.928353\n"," 79646/100000: episode: 1994, duration: 1.930s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.055657, mae: 1.847439, mean_q: 2.549093, mean_eps: 0.928330\n"," 79671/100000: episode: 1995, duration: 1.910s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.064812, mae: 1.828145, mean_q: 2.506978, mean_eps: 0.928308\n"," 79697/100000: episode: 1996, duration: 2.015s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.062535, mae: 1.917942, mean_q: 2.630807, mean_eps: 0.928285\n"," 79725/100000: episode: 1997, duration: 2.151s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.030337, mae: 1.872907, mean_q: 2.583069, mean_eps: 0.928261\n"," 79780/100000: episode: 1998, duration: 4.206s, episode steps:  55, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.527 [0.000, 3.000],  loss: 0.044832, mae: 1.915587, mean_q: 2.629127, mean_eps: 0.928223\n"," 79805/100000: episode: 1999, duration: 1.919s, episode steps:  25, steps per second:  13, episode reward: -0.489, mean reward: -0.020 [-0.489,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.069607, mae: 1.887423, mean_q: 2.587277, mean_eps: 0.928187\n"," 79835/100000: episode: 2000, duration: 2.291s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.047053, mae: 1.913180, mean_q: 2.636598, mean_eps: 0.928162\n"," 79862/100000: episode: 2001, duration: 2.098s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.045718, mae: 1.853938, mean_q: 2.547325, mean_eps: 0.928137\n"," 79889/100000: episode: 2002, duration: 2.146s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.061561, mae: 1.871710, mean_q: 2.555570, mean_eps: 0.928113\n"," 79914/100000: episode: 2003, duration: 1.925s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.057186, mae: 1.901844, mean_q: 2.605747, mean_eps: 0.928089\n"," 79939/100000: episode: 2004, duration: 1.899s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.760 [0.000, 3.000],  loss: 0.049079, mae: 1.882815, mean_q: 2.590243, mean_eps: 0.928067\n"," 79964/100000: episode: 2005, duration: 1.854s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.058507, mae: 1.902361, mean_q: 2.615710, mean_eps: 0.928044\n"," 79992/100000: episode: 2006, duration: 2.148s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.821 [0.000, 3.000],  loss: 0.054022, mae: 1.865793, mean_q: 2.564051, mean_eps: 0.928020\n"," 80018/100000: episode: 2007, duration: 1.986s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.045484, mae: 1.859218, mean_q: 2.562554, mean_eps: 0.927996\n"," 80047/100000: episode: 2008, duration: 2.194s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.038620, mae: 1.939847, mean_q: 2.660990, mean_eps: 0.927971\n"," 80074/100000: episode: 2009, duration: 2.098s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.033146, mae: 1.859349, mean_q: 2.553872, mean_eps: 0.927946\n","\n","Target network updated after 30 episodes\n"," 80130/100000: episode: 2010, duration: 4.187s, episode steps:  56, steps per second:  13, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.571 [0.000, 3.000],  loss: 0.049124, mae: 1.905210, mean_q: 2.611696, mean_eps: 0.927909\n"," 80163/100000: episode: 2011, duration: 2.451s, episode steps:  33, steps per second:  13, episode reward: -0.489, mean reward: -0.015 [-0.489,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.075285, mae: 1.926515, mean_q: 2.623265, mean_eps: 0.927869\n"," 80353/100000: episode: 2012, duration: 13.766s, episode steps: 190, steps per second:  14, episode reward:  4.307, mean reward:  0.023 [-0.493,  1.200], mean action: 1.568 [0.000, 3.000],  loss: 0.054188, mae: 1.888630, mean_q: 2.591346, mean_eps: 0.927768\n"," 80386/100000: episode: 2013, duration: 2.630s, episode steps:  33, steps per second:  13, episode reward: -0.462, mean reward: -0.014 [-0.462,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.046689, mae: 1.893788, mean_q: 2.598807, mean_eps: 0.927668\n"," 80413/100000: episode: 2014, duration: 2.050s, episode steps:  27, steps per second:  13, episode reward: -0.493, mean reward: -0.018 [-0.493,  0.000], mean action: 1.704 [0.000, 3.000],  loss: 0.038995, mae: 1.906925, mean_q: 2.607798, mean_eps: 0.927641\n"," 80468/100000: episode: 2015, duration: 4.086s, episode steps:  55, steps per second:  13, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.618 [0.000, 3.000],  loss: 0.035847, mae: 1.878822, mean_q: 2.573663, mean_eps: 0.927604\n"," 80572/100000: episode: 2016, duration: 7.646s, episode steps: 104, steps per second:  14, episode reward:  1.911, mean reward:  0.018 [-0.489,  1.200], mean action: 1.606 [0.000, 3.000],  loss: 0.047632, mae: 1.883702, mean_q: 2.579764, mean_eps: 0.927532\n"," 80625/100000: episode: 2017, duration: 3.914s, episode steps:  53, steps per second:  14, episode reward:  0.721, mean reward:  0.014 [-0.479,  1.200], mean action: 1.434 [0.000, 3.000],  loss: 0.041568, mae: 1.890438, mean_q: 2.588275, mean_eps: 0.927462\n"," 80651/100000: episode: 2018, duration: 1.951s, episode steps:  26, steps per second:  13, episode reward: -0.489, mean reward: -0.019 [-0.489,  0.000], mean action: 1.962 [0.000, 3.000],  loss: 0.064151, mae: 1.890209, mean_q: 2.576814, mean_eps: 0.927426\n"," 80705/100000: episode: 2019, duration: 3.961s, episode steps:  54, steps per second:  14, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.296 [0.000, 3.000],  loss: 0.052972, mae: 1.885123, mean_q: 2.578454, mean_eps: 0.927390\n"," 80733/100000: episode: 2020, duration: 2.159s, episode steps:  28, steps per second:  13, episode reward: -0.489, mean reward: -0.017 [-0.489,  0.000], mean action: 2.036 [0.000, 3.000],  loss: 0.032637, mae: 1.935986, mean_q: 2.653001, mean_eps: 0.927353\n"," 80758/100000: episode: 2021, duration: 1.878s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.760 [0.000, 3.000],  loss: 0.037506, mae: 1.877113, mean_q: 2.564628, mean_eps: 0.927330\n"," 80831/100000: episode: 2022, duration: 5.336s, episode steps:  73, steps per second:  14, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.575 [0.000, 3.000],  loss: 0.048075, mae: 1.856129, mean_q: 2.541383, mean_eps: 0.927285\n"," 80857/100000: episode: 2023, duration: 1.987s, episode steps:  26, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.047857, mae: 1.891789, mean_q: 2.590073, mean_eps: 0.927241\n"," 80933/100000: episode: 2024, duration: 5.560s, episode steps:  76, steps per second:  14, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.395 [0.000, 3.000],  loss: 0.047657, mae: 1.908573, mean_q: 2.611852, mean_eps: 0.927195\n"," 80960/100000: episode: 2025, duration: 1.955s, episode steps:  27, steps per second:  14, episode reward: -0.485, mean reward: -0.018 [-0.485,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.040064, mae: 1.863024, mean_q: 2.551446, mean_eps: 0.927149\n"," 81031/100000: episode: 2026, duration: 5.323s, episode steps:  71, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.549 [0.000, 3.000],  loss: 0.040694, mae: 1.913609, mean_q: 2.612460, mean_eps: 0.927105\n"," 81133/100000: episode: 2027, duration: 7.495s, episode steps: 102, steps per second:  14, episode reward:  1.914, mean reward:  0.019 [-0.486,  1.200], mean action: 1.598 [0.000, 3.000],  loss: 0.049713, mae: 1.873835, mean_q: 2.566289, mean_eps: 0.927027\n"," 81209/100000: episode: 2028, duration: 5.628s, episode steps:  76, steps per second:  14, episode reward:  0.720, mean reward:  0.009 [-0.480,  1.200], mean action: 1.553 [0.000, 3.000],  loss: 0.046990, mae: 1.902127, mean_q: 2.602956, mean_eps: 0.926947\n"," 81234/100000: episode: 2029, duration: 1.969s, episode steps:  25, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.046094, mae: 1.833395, mean_q: 2.510197, mean_eps: 0.926901\n"," 81259/100000: episode: 2030, duration: 1.876s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.032745, mae: 1.882431, mean_q: 2.580823, mean_eps: 0.926879\n"," 81285/100000: episode: 2031, duration: 1.940s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.056242, mae: 1.900661, mean_q: 2.599433, mean_eps: 0.926856\n"," 81311/100000: episode: 2032, duration: 1.993s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 2.038 [0.000, 3.000],  loss: 0.038892, mae: 1.826201, mean_q: 2.503169, mean_eps: 0.926832\n"," 81340/100000: episode: 2033, duration: 2.161s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.051334, mae: 1.877038, mean_q: 2.562915, mean_eps: 0.926808\n"," 81365/100000: episode: 2034, duration: 1.949s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.061438, mae: 1.874357, mean_q: 2.560652, mean_eps: 0.926783\n"," 81390/100000: episode: 2035, duration: 1.882s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.043570, mae: 1.908725, mean_q: 2.611774, mean_eps: 0.926761\n"," 81415/100000: episode: 2036, duration: 1.839s, episode steps:  25, steps per second:  14, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.049303, mae: 1.896943, mean_q: 2.591024, mean_eps: 0.926738\n"," 81440/100000: episode: 2037, duration: 1.882s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.038851, mae: 1.867148, mean_q: 2.558992, mean_eps: 0.926716\n"," 81472/100000: episode: 2038, duration: 2.371s, episode steps:  32, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.812 [0.000, 3.000],  loss: 0.059280, mae: 1.839184, mean_q: 2.521991, mean_eps: 0.926690\n"," 81499/100000: episode: 2039, duration: 1.996s, episode steps:  27, steps per second:  14, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.050072, mae: 1.902406, mean_q: 2.600773, mean_eps: 0.926664\n","\n","Target network updated after 30 episodes\n"," 81528/100000: episode: 2040, duration: 2.230s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.070629, mae: 1.848702, mean_q: 2.530699, mean_eps: 0.926638\n"," 81553/100000: episode: 2041, duration: 1.894s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.064938, mae: 1.902009, mean_q: 2.583695, mean_eps: 0.926614\n"," 81578/100000: episode: 2042, duration: 1.862s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.052337, mae: 1.939956, mean_q: 2.625941, mean_eps: 0.926592\n"," 81770/100000: episode: 2043, duration: 14.129s, episode steps: 192, steps per second:  14, episode reward:  4.305, mean reward:  0.022 [-0.495,  1.200], mean action: 1.453 [0.000, 3.000],  loss: 0.045382, mae: 1.852749, mean_q: 2.530862, mean_eps: 0.926494\n"," 81796/100000: episode: 2044, duration: 1.972s, episode steps:  26, steps per second:  13, episode reward: -0.462, mean reward: -0.018 [-0.462,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.040501, mae: 1.888715, mean_q: 2.569178, mean_eps: 0.926396\n"," 81826/100000: episode: 2045, duration: 2.311s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.038900, mae: 1.918340, mean_q: 2.605368, mean_eps: 0.926371\n"," 81904/100000: episode: 2046, duration: 5.902s, episode steps:  78, steps per second:  13, episode reward:  0.706, mean reward:  0.009 [-0.494,  1.200], mean action: 1.526 [0.000, 3.000],  loss: 0.036399, mae: 1.855264, mean_q: 2.527170, mean_eps: 0.926322\n"," 81932/100000: episode: 2047, duration: 2.095s, episode steps:  28, steps per second:  13, episode reward: -0.484, mean reward: -0.017 [-0.484,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.045125, mae: 1.855571, mean_q: 2.549955, mean_eps: 0.926274\n"," 82053/100000: episode: 2048, duration: 8.968s, episode steps: 121, steps per second:  13, episode reward:  1.906, mean reward:  0.016 [-0.494,  1.200], mean action: 1.612 [0.000, 3.000],  loss: 0.044461, mae: 1.865168, mean_q: 2.548223, mean_eps: 0.926207\n"," 82079/100000: episode: 2049, duration: 1.963s, episode steps:  26, steps per second:  13, episode reward: -0.476, mean reward: -0.018 [-0.476,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.041798, mae: 1.872010, mean_q: 2.560328, mean_eps: 0.926141\n"," 82108/100000: episode: 2050, duration: 2.215s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.759 [0.000, 3.000],  loss: 0.045652, mae: 1.918642, mean_q: 2.620160, mean_eps: 0.926116\n"," 82140/100000: episode: 2051, duration: 2.324s, episode steps:  32, steps per second:  14, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.044133, mae: 1.897366, mean_q: 2.592911, mean_eps: 0.926089\n"," 82198/100000: episode: 2052, duration: 4.428s, episode steps:  58, steps per second:  13, episode reward:  0.706, mean reward:  0.012 [-0.494,  1.200], mean action: 1.534 [0.000, 3.000],  loss: 0.045602, mae: 1.849580, mean_q: 2.536408, mean_eps: 0.926048\n"," 82251/100000: episode: 2053, duration: 3.916s, episode steps:  53, steps per second:  14, episode reward:  0.712, mean reward:  0.013 [-0.488,  1.200], mean action: 1.698 [0.000, 3.000],  loss: 0.039403, mae: 1.881769, mean_q: 2.577459, mean_eps: 0.925998\n"," 82407/100000: episode: 2054, duration: 11.676s, episode steps: 156, steps per second:  13, episode reward:  4.311, mean reward:  0.028 [-0.489,  1.200], mean action: 1.596 [0.000, 3.000],  loss: 0.034358, mae: 1.847099, mean_q: 2.528219, mean_eps: 0.925904\n"," 82439/100000: episode: 2055, duration: 2.370s, episode steps:  32, steps per second:  14, episode reward: -0.469, mean reward: -0.015 [-0.469,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.047696, mae: 1.881460, mean_q: 2.578068, mean_eps: 0.925820\n"," 82468/100000: episode: 2056, duration: 2.226s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.276 [0.000, 3.000],  loss: 0.042041, mae: 1.803052, mean_q: 2.474329, mean_eps: 0.925792\n"," 82541/100000: episode: 2057, duration: 5.542s, episode steps:  73, steps per second:  13, episode reward:  0.706, mean reward:  0.010 [-0.494,  1.200], mean action: 1.712 [0.000, 3.000],  loss: 0.058126, mae: 1.884771, mean_q: 2.578984, mean_eps: 0.925746\n"," 82569/100000: episode: 2058, duration: 2.181s, episode steps:  28, steps per second:  13, episode reward: -0.485, mean reward: -0.017 [-0.485,  0.000], mean action: 1.893 [0.000, 3.000],  loss: 0.047632, mae: 1.867344, mean_q: 2.550670, mean_eps: 0.925701\n"," 82599/100000: episode: 2059, duration: 2.232s, episode steps:  30, steps per second:  13, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.044175, mae: 1.820467, mean_q: 2.495554, mean_eps: 0.925675\n"," 82628/100000: episode: 2060, duration: 2.171s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.655 [0.000, 3.000],  loss: 0.033738, mae: 1.937299, mean_q: 2.652111, mean_eps: 0.925648\n"," 82658/100000: episode: 2061, duration: 2.317s, episode steps:  30, steps per second:  13, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.043437, mae: 1.828378, mean_q: 2.499013, mean_eps: 0.925622\n"," 82685/100000: episode: 2062, duration: 2.144s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.296 [0.000, 3.000],  loss: 0.040652, mae: 1.875881, mean_q: 2.573544, mean_eps: 0.925596\n"," 82742/100000: episode: 2063, duration: 4.218s, episode steps:  57, steps per second:  14, episode reward:  0.705, mean reward:  0.012 [-0.495,  1.200], mean action: 1.456 [0.000, 3.000],  loss: 0.041895, mae: 1.870490, mean_q: 2.556439, mean_eps: 0.925558\n"," 82768/100000: episode: 2064, duration: 1.981s, episode steps:  26, steps per second:  13, episode reward: -0.489, mean reward: -0.019 [-0.489,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.042024, mae: 1.885279, mean_q: 2.575809, mean_eps: 0.925521\n"," 82796/100000: episode: 2065, duration: 2.178s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.047195, mae: 1.780659, mean_q: 2.439245, mean_eps: 0.925497\n"," 82823/100000: episode: 2066, duration: 2.079s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.027284, mae: 1.882147, mean_q: 2.585607, mean_eps: 0.925472\n"," 82896/100000: episode: 2067, duration: 5.480s, episode steps:  73, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.479 [0.000, 3.000],  loss: 0.042086, mae: 1.907328, mean_q: 2.605587, mean_eps: 0.925427\n"," 82930/100000: episode: 2068, duration: 2.492s, episode steps:  34, steps per second:  14, episode reward: -0.485, mean reward: -0.014 [-0.485,  0.000], mean action: 1.324 [0.000, 3.000],  loss: 0.046893, mae: 1.847136, mean_q: 2.532525, mean_eps: 0.925379\n"," 82959/100000: episode: 2069, duration: 2.221s, episode steps:  29, steps per second:  13, episode reward: -0.493, mean reward: -0.017 [-0.493,  0.000], mean action: 1.724 [0.000, 3.000],  loss: 0.039129, mae: 1.894040, mean_q: 2.587205, mean_eps: 0.925350\n","\n","Target network updated after 30 episodes\n"," 83017/100000: episode: 2070, duration: 4.409s, episode steps:  58, steps per second:  13, episode reward:  0.706, mean reward:  0.012 [-0.494,  1.200], mean action: 1.672 [0.000, 3.000],  loss: 0.036434, mae: 1.860375, mean_q: 2.550450, mean_eps: 0.925311\n"," 83043/100000: episode: 2071, duration: 2.010s, episode steps:  26, steps per second:  13, episode reward: -0.488, mean reward: -0.019 [-0.488,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.078793, mae: 1.821758, mean_q: 2.471697, mean_eps: 0.925273\n"," 83072/100000: episode: 2072, duration: 2.202s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.055655, mae: 1.881629, mean_q: 2.584971, mean_eps: 0.925249\n"," 83101/100000: episode: 2073, duration: 2.168s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.793 [0.000, 3.000],  loss: 0.049258, mae: 1.911652, mean_q: 2.613260, mean_eps: 0.925223\n"," 83222/100000: episode: 2074, duration: 8.980s, episode steps: 121, steps per second:  13, episode reward:  1.906, mean reward:  0.016 [-0.494,  1.200], mean action: 1.512 [0.000, 3.000],  loss: 0.055486, mae: 1.862971, mean_q: 2.551887, mean_eps: 0.925155\n"," 83250/100000: episode: 2075, duration: 2.071s, episode steps:  28, steps per second:  14, episode reward: -0.476, mean reward: -0.017 [-0.476,  0.000], mean action: 1.321 [0.000, 3.000],  loss: 0.038956, mae: 1.824452, mean_q: 2.510427, mean_eps: 0.925088\n"," 83275/100000: episode: 2076, duration: 1.851s, episode steps:  25, steps per second:  14, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.160 [0.000, 3.000],  loss: 0.041337, mae: 1.875435, mean_q: 2.561382, mean_eps: 0.925064\n"," 83300/100000: episode: 2077, duration: 1.903s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.031009, mae: 1.878859, mean_q: 2.576965, mean_eps: 0.925042\n"," 83333/100000: episode: 2078, duration: 2.623s, episode steps:  33, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.040626, mae: 1.884293, mean_q: 2.592457, mean_eps: 0.925016\n"," 83359/100000: episode: 2079, duration: 1.946s, episode steps:  26, steps per second:  13, episode reward: -0.493, mean reward: -0.019 [-0.493,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.046071, mae: 1.865807, mean_q: 2.575773, mean_eps: 0.924989\n"," 83387/100000: episode: 2080, duration: 2.119s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.039831, mae: 1.901739, mean_q: 2.615625, mean_eps: 0.924965\n"," 83412/100000: episode: 2081, duration: 1.881s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.039082, mae: 1.846123, mean_q: 2.529588, mean_eps: 0.924941\n"," 83437/100000: episode: 2082, duration: 1.874s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.037508, mae: 1.885410, mean_q: 2.580406, mean_eps: 0.924918\n"," 83469/100000: episode: 2083, duration: 2.454s, episode steps:  32, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.156 [0.000, 3.000],  loss: 0.043547, mae: 1.844670, mean_q: 2.533658, mean_eps: 0.924893\n"," 83495/100000: episode: 2084, duration: 2.025s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.050479, mae: 1.905317, mean_q: 2.604156, mean_eps: 0.924867\n"," 83522/100000: episode: 2085, duration: 2.047s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.039747, mae: 1.842234, mean_q: 2.535457, mean_eps: 0.924843\n"," 83596/100000: episode: 2086, duration: 5.513s, episode steps:  74, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.419 [0.000, 3.000],  loss: 0.047977, mae: 1.865560, mean_q: 2.558263, mean_eps: 0.924797\n"," 83622/100000: episode: 2087, duration: 1.973s, episode steps:  26, steps per second:  13, episode reward: -0.485, mean reward: -0.019 [-0.485,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.038446, mae: 1.877428, mean_q: 2.590245, mean_eps: 0.924752\n"," 83654/100000: episode: 2088, duration: 2.491s, episode steps:  32, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.040715, mae: 1.818311, mean_q: 2.502937, mean_eps: 0.924726\n"," 83682/100000: episode: 2089, duration: 2.091s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.048691, mae: 1.875495, mean_q: 2.575793, mean_eps: 0.924699\n"," 83707/100000: episode: 2090, duration: 1.903s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.054747, mae: 1.831278, mean_q: 2.520890, mean_eps: 0.924675\n"," 83732/100000: episode: 2091, duration: 1.877s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.720 [0.000, 3.000],  loss: 0.046802, mae: 1.850294, mean_q: 2.534299, mean_eps: 0.924653\n"," 83760/100000: episode: 2092, duration: 2.076s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.714 [0.000, 3.000],  loss: 0.035726, mae: 1.865081, mean_q: 2.555534, mean_eps: 0.924629\n"," 83813/100000: episode: 2093, duration: 4.076s, episode steps:  53, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.321 [0.000, 3.000],  loss: 0.040440, mae: 1.825499, mean_q: 2.497518, mean_eps: 0.924593\n"," 83935/100000: episode: 2094, duration: 8.981s, episode steps: 122, steps per second:  14, episode reward:  1.911, mean reward:  0.016 [-0.489,  1.200], mean action: 1.361 [0.000, 3.000],  loss: 0.050874, mae: 1.873791, mean_q: 2.562511, mean_eps: 0.924514\n"," 83991/100000: episode: 2095, duration: 4.244s, episode steps:  56, steps per second:  13, episode reward:  0.724, mean reward:  0.013 [-0.476,  1.200], mean action: 1.607 [0.000, 3.000],  loss: 0.037192, mae: 1.816074, mean_q: 2.494185, mean_eps: 0.924434\n"," 84112/100000: episode: 2096, duration: 8.754s, episode steps: 121, steps per second:  14, episode reward:  1.911, mean reward:  0.016 [-0.489,  1.200], mean action: 1.364 [0.000, 3.000],  loss: 0.042462, mae: 1.866189, mean_q: 2.564759, mean_eps: 0.924354\n"," 84139/100000: episode: 2097, duration: 2.106s, episode steps:  27, steps per second:  13, episode reward: -0.476, mean reward: -0.018 [-0.476,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.051856, mae: 1.874862, mean_q: 2.561935, mean_eps: 0.924288\n"," 84248/100000: episode: 2098, duration: 8.058s, episode steps: 109, steps per second:  14, episode reward:  1.905, mean reward:  0.017 [-0.495,  1.200], mean action: 1.734 [0.000, 3.000],  loss: 0.048608, mae: 1.862413, mean_q: 2.548715, mean_eps: 0.924226\n"," 84277/100000: episode: 2099, duration: 2.252s, episode steps:  29, steps per second:  13, episode reward: -0.478, mean reward: -0.016 [-0.478,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.038225, mae: 1.835372, mean_q: 2.524194, mean_eps: 0.924164\n","\n","Target network updated after 30 episodes\n"," 84331/100000: episode: 2100, duration: 4.112s, episode steps:  54, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.611 [0.000, 3.000],  loss: 0.043836, mae: 1.836851, mean_q: 2.518247, mean_eps: 0.924127\n"," 84393/100000: episode: 2101, duration: 4.618s, episode steps:  62, steps per second:  13, episode reward:  0.711, mean reward:  0.011 [-0.489,  1.200], mean action: 1.774 [0.000, 3.000],  loss: 0.045307, mae: 1.872785, mean_q: 2.557391, mean_eps: 0.924075\n"," 84493/100000: episode: 2102, duration: 7.533s, episode steps: 100, steps per second:  13, episode reward:  1.912, mean reward:  0.019 [-0.488,  1.200], mean action: 1.630 [0.000, 3.000],  loss: 0.042475, mae: 1.872391, mean_q: 2.564183, mean_eps: 0.924002\n"," 84519/100000: episode: 2103, duration: 1.953s, episode steps:  26, steps per second:  13, episode reward: -0.480, mean reward: -0.018 [-0.480,  0.000], mean action: 1.154 [0.000, 3.000],  loss: 0.046079, mae: 1.828973, mean_q: 2.507542, mean_eps: 0.923945\n"," 84546/100000: episode: 2104, duration: 2.063s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.148 [0.000, 3.000],  loss: 0.052667, mae: 1.890623, mean_q: 2.583335, mean_eps: 0.923921\n"," 84571/100000: episode: 2105, duration: 1.857s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.040991, mae: 1.876074, mean_q: 2.578630, mean_eps: 0.923898\n"," 84626/100000: episode: 2106, duration: 4.142s, episode steps:  55, steps per second:  13, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.327 [0.000, 3.000],  loss: 0.044823, mae: 1.864299, mean_q: 2.555275, mean_eps: 0.923862\n"," 84666/100000: episode: 2107, duration: 2.991s, episode steps:  40, steps per second:  13, episode reward: -0.489, mean reward: -0.012 [-0.489,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 0.035905, mae: 1.819097, mean_q: 2.490452, mean_eps: 0.923819\n"," 84692/100000: episode: 2108, duration: 2.037s, episode steps:  26, steps per second:  13, episode reward: -0.492, mean reward: -0.019 [-0.492,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.045444, mae: 1.858550, mean_q: 2.534021, mean_eps: 0.923789\n"," 84718/100000: episode: 2109, duration: 1.931s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.047182, mae: 1.861482, mean_q: 2.544159, mean_eps: 0.923766\n"," 84750/100000: episode: 2110, duration: 2.452s, episode steps:  32, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.036505, mae: 1.832093, mean_q: 2.503036, mean_eps: 0.923740\n"," 84785/100000: episode: 2111, duration: 2.731s, episode steps:  35, steps per second:  13, episode reward: -0.494, mean reward: -0.014 [-0.494,  0.000], mean action: 1.971 [0.000, 3.000],  loss: 0.050556, mae: 1.873800, mean_q: 2.579334, mean_eps: 0.923710\n"," 84840/100000: episode: 2112, duration: 4.098s, episode steps:  55, steps per second:  13, episode reward:  0.707, mean reward:  0.013 [-0.493,  1.200], mean action: 1.655 [0.000, 3.000],  loss: 0.034329, mae: 1.817998, mean_q: 2.484071, mean_eps: 0.923669\n"," 84867/100000: episode: 2113, duration: 1.948s, episode steps:  27, steps per second:  14, episode reward: -0.489, mean reward: -0.018 [-0.489,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.033091, mae: 1.887013, mean_q: 2.570416, mean_eps: 0.923632\n"," 84892/100000: episode: 2114, duration: 1.885s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.042923, mae: 1.825544, mean_q: 2.494929, mean_eps: 0.923609\n"," 84931/100000: episode: 2115, duration: 3.017s, episode steps:  39, steps per second:  13, episode reward: -0.495, mean reward: -0.013 [-0.495,  0.000], mean action: 1.872 [0.000, 3.000],  loss: 0.034939, mae: 1.904987, mean_q: 2.606650, mean_eps: 0.923580\n"," 84959/100000: episode: 2116, duration: 2.158s, episode steps:  28, steps per second:  13, episode reward: -0.492, mean reward: -0.018 [-0.492,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.043485, mae: 1.853347, mean_q: 2.538289, mean_eps: 0.923550\n"," 85014/100000: episode: 2117, duration: 4.040s, episode steps:  55, steps per second:  14, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.600 [0.000, 3.000],  loss: 0.037272, mae: 1.859902, mean_q: 2.541136, mean_eps: 0.923513\n"," 85067/100000: episode: 2118, duration: 3.960s, episode steps:  53, steps per second:  13, episode reward:  0.711, mean reward:  0.013 [-0.489,  1.200], mean action: 1.547 [0.000, 3.000],  loss: 0.033924, mae: 1.912149, mean_q: 2.625237, mean_eps: 0.923464\n"," 85099/100000: episode: 2119, duration: 2.567s, episode steps:  32, steps per second:  12, episode reward: -0.489, mean reward: -0.015 [-0.489,  0.000], mean action: 1.156 [0.000, 3.000],  loss: 0.046916, mae: 1.871213, mean_q: 2.554195, mean_eps: 0.923426\n"," 85124/100000: episode: 2120, duration: 1.925s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.033014, mae: 1.905736, mean_q: 2.608978, mean_eps: 0.923400\n"," 85154/100000: episode: 2121, duration: 2.288s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.300 [0.000, 3.000],  loss: 0.037329, mae: 1.852661, mean_q: 2.532623, mean_eps: 0.923375\n"," 85183/100000: episode: 2122, duration: 2.228s, episode steps:  29, steps per second:  13, episode reward: -0.494, mean reward: -0.017 [-0.494,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.033029, mae: 1.824700, mean_q: 2.509872, mean_eps: 0.923349\n"," 85254/100000: episode: 2123, duration: 5.272s, episode steps:  71, steps per second:  13, episode reward:  0.706, mean reward:  0.010 [-0.494,  1.200], mean action: 1.197 [0.000, 3.000],  loss: 0.045420, mae: 1.831791, mean_q: 2.517833, mean_eps: 0.923304\n"," 85280/100000: episode: 2124, duration: 2.038s, episode steps:  26, steps per second:  13, episode reward: -0.486, mean reward: -0.019 [-0.486,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.042221, mae: 1.802196, mean_q: 2.477216, mean_eps: 0.923260\n"," 85306/100000: episode: 2125, duration: 1.982s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.041495, mae: 1.842820, mean_q: 2.532672, mean_eps: 0.923237\n"," 85335/100000: episode: 2126, duration: 2.214s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.724 [0.000, 3.000],  loss: 0.029923, mae: 1.839893, mean_q: 2.518906, mean_eps: 0.923212\n"," 85411/100000: episode: 2127, duration: 5.731s, episode steps:  76, steps per second:  13, episode reward:  0.706, mean reward:  0.009 [-0.494,  1.200], mean action: 1.671 [0.000, 3.000],  loss: 0.036873, mae: 1.833280, mean_q: 2.519306, mean_eps: 0.923165\n"," 85469/100000: episode: 2128, duration: 4.385s, episode steps:  58, steps per second:  13, episode reward:  0.715, mean reward:  0.012 [-0.485,  1.200], mean action: 1.534 [0.000, 3.000],  loss: 0.027764, mae: 1.844708, mean_q: 2.531390, mean_eps: 0.923104\n"," 85558/100000: episode: 2129, duration: 6.628s, episode steps:  89, steps per second:  13, episode reward:  0.712, mean reward:  0.008 [-0.488,  1.200], mean action: 1.551 [0.000, 3.000],  loss: 0.031081, mae: 1.819902, mean_q: 2.502681, mean_eps: 0.923038\n","\n","Target network updated after 30 episodes\n"," 85594/100000: episode: 2130, duration: 2.884s, episode steps:  36, steps per second:  12, episode reward: -0.482, mean reward: -0.013 [-0.482,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.033221, mae: 1.879830, mean_q: 2.580400, mean_eps: 0.922982\n"," 85620/100000: episode: 2131, duration: 1.919s, episode steps:  26, steps per second:  14, episode reward: -0.493, mean reward: -0.019 [-0.493,  0.000], mean action: 1.769 [0.000, 3.000],  loss: 0.084962, mae: 1.932869, mean_q: 2.630741, mean_eps: 0.922954\n"," 85787/100000: episode: 2132, duration: 12.124s, episode steps: 167, steps per second:  14, episode reward:  3.105, mean reward:  0.019 [-0.495,  1.200], mean action: 1.503 [0.000, 3.000],  loss: 0.038186, mae: 1.845865, mean_q: 2.525497, mean_eps: 0.922867\n"," 85815/100000: episode: 2133, duration: 2.087s, episode steps:  28, steps per second:  13, episode reward: -0.467, mean reward: -0.017 [-0.467,  0.000], mean action: 1.786 [0.000, 3.000],  loss: 0.036067, mae: 1.812834, mean_q: 2.479271, mean_eps: 0.922780\n"," 85840/100000: episode: 2134, duration: 1.851s, episode steps:  25, steps per second:  14, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.031045, mae: 1.798716, mean_q: 2.469156, mean_eps: 0.922756\n"," 85917/100000: episode: 2135, duration: 5.709s, episode steps:  77, steps per second:  13, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.351 [0.000, 3.000],  loss: 0.039321, mae: 1.857964, mean_q: 2.542426, mean_eps: 0.922710\n"," 85944/100000: episode: 2136, duration: 2.032s, episode steps:  27, steps per second:  13, episode reward: -0.485, mean reward: -0.018 [-0.485,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.036456, mae: 1.891683, mean_q: 2.600785, mean_eps: 0.922663\n"," 85971/100000: episode: 2137, duration: 1.975s, episode steps:  27, steps per second:  14, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.037459, mae: 1.843985, mean_q: 2.539017, mean_eps: 0.922639\n"," 85999/100000: episode: 2138, duration: 2.127s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.052827, mae: 1.798218, mean_q: 2.471174, mean_eps: 0.922614\n"," 86031/100000: episode: 2139, duration: 2.431s, episode steps:  32, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.029858, mae: 1.837222, mean_q: 2.530455, mean_eps: 0.922587\n"," 86056/100000: episode: 2140, duration: 1.871s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.880 [0.000, 3.000],  loss: 0.035190, mae: 1.852747, mean_q: 2.552620, mean_eps: 0.922561\n"," 86081/100000: episode: 2141, duration: 1.991s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.360 [0.000, 3.000],  loss: 0.033639, mae: 1.796575, mean_q: 2.465468, mean_eps: 0.922539\n"," 86113/100000: episode: 2142, duration: 2.359s, episode steps:  32, steps per second:  14, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.028478, mae: 1.863469, mean_q: 2.552106, mean_eps: 0.922513\n"," 86138/100000: episode: 2143, duration: 1.853s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.043119, mae: 1.818579, mean_q: 2.500461, mean_eps: 0.922488\n"," 86166/100000: episode: 2144, duration: 2.075s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.042020, mae: 1.819725, mean_q: 2.507636, mean_eps: 0.922464\n"," 86193/100000: episode: 2145, duration: 2.051s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.049100, mae: 1.829451, mean_q: 2.504520, mean_eps: 0.922439\n"," 86219/100000: episode: 2146, duration: 2.024s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.036860, mae: 1.826161, mean_q: 2.517774, mean_eps: 0.922415\n"," 86246/100000: episode: 2147, duration: 2.120s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.815 [0.000, 3.000],  loss: 0.041470, mae: 1.809119, mean_q: 2.492095, mean_eps: 0.922391\n"," 86278/100000: episode: 2148, duration: 2.387s, episode steps:  32, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.026528, mae: 1.803550, mean_q: 2.478546, mean_eps: 0.922365\n"," 86304/100000: episode: 2149, duration: 1.940s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.036158, mae: 1.771846, mean_q: 2.439387, mean_eps: 0.922339\n"," 86329/100000: episode: 2150, duration: 1.840s, episode steps:  25, steps per second:  14, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.680 [0.000, 3.000],  loss: 0.029494, mae: 1.871637, mean_q: 2.580111, mean_eps: 0.922316\n"," 86402/100000: episode: 2151, duration: 5.382s, episode steps:  73, steps per second:  14, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.438 [0.000, 3.000],  loss: 0.034793, mae: 1.838208, mean_q: 2.534232, mean_eps: 0.922272\n"," 86504/100000: episode: 2152, duration: 7.488s, episode steps: 102, steps per second:  14, episode reward:  1.915, mean reward:  0.019 [-0.485,  1.200], mean action: 1.725 [0.000, 3.000],  loss: 0.033471, mae: 1.832895, mean_q: 2.521476, mean_eps: 0.922193\n"," 86531/100000: episode: 2153, duration: 2.011s, episode steps:  27, steps per second:  13, episode reward: -0.480, mean reward: -0.018 [-0.480,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.046673, mae: 1.813377, mean_q: 2.492578, mean_eps: 0.922135\n"," 86559/100000: episode: 2154, duration: 2.261s, episode steps:  28, steps per second:  12, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.032100, mae: 1.842188, mean_q: 2.534440, mean_eps: 0.922110\n"," 86589/100000: episode: 2155, duration: 2.267s, episode steps:  30, steps per second:  13, episode reward: -0.494, mean reward: -0.016 [-0.494,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.046576, mae: 1.873863, mean_q: 2.582017, mean_eps: 0.922084\n"," 86697/100000: episode: 2156, duration: 7.828s, episode steps: 108, steps per second:  14, episode reward:  1.906, mean reward:  0.018 [-0.494,  1.200], mean action: 1.315 [0.000, 3.000],  loss: 0.040851, mae: 1.854318, mean_q: 2.562694, mean_eps: 0.922022\n"," 86723/100000: episode: 2157, duration: 1.979s, episode steps:  26, steps per second:  13, episode reward: -0.478, mean reward: -0.018 [-0.478,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.039635, mae: 1.800670, mean_q: 2.492434, mean_eps: 0.921961\n"," 86800/100000: episode: 2158, duration: 5.646s, episode steps:  77, steps per second:  14, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.442 [0.000, 3.000],  loss: 0.028235, mae: 1.816632, mean_q: 2.503527, mean_eps: 0.921915\n"," 86827/100000: episode: 2159, duration: 2.044s, episode steps:  27, steps per second:  13, episode reward: -0.485, mean reward: -0.018 [-0.485,  0.000], mean action: 1.704 [0.000, 3.000],  loss: 0.028124, mae: 1.782756, mean_q: 2.451514, mean_eps: 0.921868\n","\n","Target network updated after 30 episodes\n"," 86853/100000: episode: 2160, duration: 1.983s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.029703, mae: 1.837344, mean_q: 2.533421, mean_eps: 0.921844\n"," 86907/100000: episode: 2161, duration: 4.208s, episode steps:  54, steps per second:  13, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.519 [0.000, 3.000],  loss: 0.054759, mae: 1.878280, mean_q: 2.565756, mean_eps: 0.921808\n"," 86937/100000: episode: 2162, duration: 2.260s, episode steps:  30, steps per second:  13, episode reward: -0.489, mean reward: -0.016 [-0.489,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.038032, mae: 1.869652, mean_q: 2.549173, mean_eps: 0.921771\n"," 86963/100000: episode: 2163, duration: 1.938s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.043397, mae: 1.801109, mean_q: 2.481382, mean_eps: 0.921745\n"," 86989/100000: episode: 2164, duration: 2.010s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.028992, mae: 1.840377, mean_q: 2.538502, mean_eps: 0.921722\n"," 87015/100000: episode: 2165, duration: 1.950s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.038076, mae: 1.846651, mean_q: 2.532136, mean_eps: 0.921699\n"," 87093/100000: episode: 2166, duration: 5.774s, episode steps:  78, steps per second:  14, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.692 [0.000, 3.000],  loss: 0.032559, mae: 1.857465, mean_q: 2.561923, mean_eps: 0.921652\n"," 87182/100000: episode: 2167, duration: 6.436s, episode steps:  89, steps per second:  14, episode reward:  1.916, mean reward:  0.022 [-0.484,  1.200], mean action: 1.449 [0.000, 3.000],  loss: 0.037786, mae: 1.852076, mean_q: 2.551870, mean_eps: 0.921577\n"," 87209/100000: episode: 2168, duration: 2.117s, episode steps:  27, steps per second:  13, episode reward: -0.482, mean reward: -0.018 [-0.482,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.045236, mae: 1.772921, mean_q: 2.434719, mean_eps: 0.921524\n"," 87241/100000: episode: 2169, duration: 2.504s, episode steps:  32, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.034716, mae: 1.802372, mean_q: 2.476352, mean_eps: 0.921498\n"," 87349/100000: episode: 2170, duration: 7.844s, episode steps: 108, steps per second:  14, episode reward:  1.906, mean reward:  0.018 [-0.494,  1.200], mean action: 1.528 [0.000, 3.000],  loss: 0.035453, mae: 1.817619, mean_q: 2.505139, mean_eps: 0.921435\n"," 87385/100000: episode: 2171, duration: 2.879s, episode steps:  36, steps per second:  13, episode reward: -0.478, mean reward: -0.013 [-0.478,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.025555, mae: 1.871049, mean_q: 2.574019, mean_eps: 0.921370\n"," 87415/100000: episode: 2172, duration: 2.286s, episode steps:  30, steps per second:  13, episode reward: -0.493, mean reward: -0.016 [-0.493,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.041899, mae: 1.777433, mean_q: 2.452565, mean_eps: 0.921340\n"," 87443/100000: episode: 2173, duration: 2.207s, episode steps:  28, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.025707, mae: 1.856506, mean_q: 2.563266, mean_eps: 0.921314\n"," 87469/100000: episode: 2174, duration: 2.022s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.037142, mae: 1.807153, mean_q: 2.480419, mean_eps: 0.921290\n"," 87496/100000: episode: 2175, duration: 1.983s, episode steps:  27, steps per second:  14, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.074 [0.000, 3.000],  loss: 0.040856, mae: 1.793367, mean_q: 2.474170, mean_eps: 0.921266\n"," 87523/100000: episode: 2176, duration: 2.096s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.032415, mae: 1.824656, mean_q: 2.506437, mean_eps: 0.921242\n"," 87550/100000: episode: 2177, duration: 2.103s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.030335, mae: 1.827234, mean_q: 2.504507, mean_eps: 0.921218\n"," 87576/100000: episode: 2178, duration: 1.937s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.039556, mae: 1.839592, mean_q: 2.523476, mean_eps: 0.921194\n"," 87602/100000: episode: 2179, duration: 2.065s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.192 [0.000, 3.000],  loss: 0.033271, mae: 1.823204, mean_q: 2.518880, mean_eps: 0.921170\n"," 87631/100000: episode: 2180, duration: 2.216s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.029183, mae: 1.810171, mean_q: 2.485654, mean_eps: 0.921146\n"," 87658/100000: episode: 2181, duration: 2.057s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.021241, mae: 1.825441, mean_q: 2.508237, mean_eps: 0.921120\n"," 87684/100000: episode: 2182, duration: 2.006s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.023639, mae: 1.839572, mean_q: 2.530123, mean_eps: 0.921097\n"," 87713/100000: episode: 2183, duration: 2.283s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.034481, mae: 1.828505, mean_q: 2.513320, mean_eps: 0.921072\n"," 87753/100000: episode: 2184, duration: 2.971s, episode steps:  40, steps per second:  13, episode reward: -0.494, mean reward: -0.012 [-0.494,  0.000], mean action: 1.175 [0.000, 3.000],  loss: 0.029700, mae: 1.851526, mean_q: 2.542596, mean_eps: 0.921041\n"," 87786/100000: episode: 2185, duration: 2.426s, episode steps:  33, steps per second:  14, episode reward: -0.492, mean reward: -0.015 [-0.492,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 0.030301, mae: 1.803048, mean_q: 2.488740, mean_eps: 0.921008\n"," 87876/100000: episode: 2186, duration: 6.782s, episode steps:  90, steps per second:  13, episode reward:  1.907, mean reward:  0.021 [-0.493,  1.200], mean action: 1.589 [0.000, 3.000],  loss: 0.027824, mae: 1.815674, mean_q: 2.500625, mean_eps: 0.920953\n"," 87905/100000: episode: 2187, duration: 2.205s, episode steps:  29, steps per second:  13, episode reward: -0.482, mean reward: -0.017 [-0.482,  0.000], mean action: 1.241 [0.000, 3.000],  loss: 0.037237, mae: 1.873596, mean_q: 2.570512, mean_eps: 0.920899\n"," 87931/100000: episode: 2188, duration: 1.900s, episode steps:  26, steps per second:  14, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.026414, mae: 1.823191, mean_q: 2.511267, mean_eps: 0.920874\n"," 87959/100000: episode: 2189, duration: 2.088s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.042949, mae: 1.786663, mean_q: 2.459318, mean_eps: 0.920850\n","\n","Target network updated after 30 episodes\n"," 87995/100000: episode: 2190, duration: 2.776s, episode steps:  36, steps per second:  13, episode reward: -0.494, mean reward: -0.014 [-0.494,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.030021, mae: 1.807314, mean_q: 2.491661, mean_eps: 0.920821\n"," 88164/100000: episode: 2191, duration: 12.347s, episode steps: 169, steps per second:  14, episode reward:  4.307, mean reward:  0.025 [-0.493,  1.200], mean action: 1.462 [0.000, 3.000],  loss: 0.040825, mae: 1.812799, mean_q: 2.485653, mean_eps: 0.920729\n"," 88194/100000: episode: 2192, duration: 2.384s, episode steps:  30, steps per second:  13, episode reward: -0.466, mean reward: -0.016 [-0.466,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 0.020644, mae: 1.818715, mean_q: 2.491880, mean_eps: 0.920639\n"," 88220/100000: episode: 2193, duration: 1.989s, episode steps:  26, steps per second:  13, episode reward: -0.494, mean reward: -0.019 [-0.494,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.025478, mae: 1.811427, mean_q: 2.482040, mean_eps: 0.920614\n"," 88249/100000: episode: 2194, duration: 2.149s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.897 [0.000, 3.000],  loss: 0.032750, mae: 1.830924, mean_q: 2.505589, mean_eps: 0.920589\n"," 88276/100000: episode: 2195, duration: 2.040s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.025097, mae: 1.802582, mean_q: 2.467066, mean_eps: 0.920564\n"," 88302/100000: episode: 2196, duration: 1.915s, episode steps:  26, steps per second:  14, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.885 [0.000, 3.000],  loss: 0.029662, mae: 1.854244, mean_q: 2.543783, mean_eps: 0.920540\n"," 88331/100000: episode: 2197, duration: 2.230s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.690 [0.000, 3.000],  loss: 0.027384, mae: 1.846632, mean_q: 2.529935, mean_eps: 0.920516\n"," 88356/100000: episode: 2198, duration: 1.969s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.032905, mae: 1.845575, mean_q: 2.524760, mean_eps: 0.920491\n"," 88385/100000: episode: 2199, duration: 2.191s, episode steps:  29, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.030376, mae: 1.802298, mean_q: 2.471998, mean_eps: 0.920467\n"," 88410/100000: episode: 2200, duration: 1.878s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.032463, mae: 1.739502, mean_q: 2.385337, mean_eps: 0.920443\n"," 88441/100000: episode: 2201, duration: 2.367s, episode steps:  31, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.806 [0.000, 3.000],  loss: 0.032967, mae: 1.826482, mean_q: 2.499964, mean_eps: 0.920418\n"," 88466/100000: episode: 2202, duration: 1.930s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.026684, mae: 1.811597, mean_q: 2.482333, mean_eps: 0.920392\n"," 88491/100000: episode: 2203, duration: 1.896s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.025866, mae: 1.833052, mean_q: 2.504117, mean_eps: 0.920370\n"," 88517/100000: episode: 2204, duration: 2.130s, episode steps:  26, steps per second:  12, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.027850, mae: 1.880551, mean_q: 2.588268, mean_eps: 0.920347\n"," 88593/100000: episode: 2205, duration: 5.490s, episode steps:  76, steps per second:  14, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.592 [0.000, 3.000],  loss: 0.029536, mae: 1.782714, mean_q: 2.442882, mean_eps: 0.920301\n"," 88698/100000: episode: 2206, duration: 7.830s, episode steps: 105, steps per second:  13, episode reward:  1.915, mean reward:  0.018 [-0.485,  1.200], mean action: 1.381 [0.000, 3.000],  loss: 0.029877, mae: 1.832733, mean_q: 2.514559, mean_eps: 0.920219\n"," 88727/100000: episode: 2207, duration: 2.200s, episode steps:  29, steps per second:  13, episode reward: -0.479, mean reward: -0.017 [-0.479,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.034334, mae: 1.828700, mean_q: 2.497695, mean_eps: 0.920159\n"," 88760/100000: episode: 2208, duration: 2.495s, episode steps:  33, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.879 [0.000, 3.000],  loss: 0.025843, mae: 1.836663, mean_q: 2.511029, mean_eps: 0.920131\n"," 88825/100000: episode: 2209, duration: 4.824s, episode steps:  65, steps per second:  13, episode reward:  0.707, mean reward:  0.011 [-0.493,  1.200], mean action: 1.569 [0.000, 3.000],  loss: 0.023566, mae: 1.784100, mean_q: 2.440985, mean_eps: 0.920087\n"," 88880/100000: episode: 2210, duration: 4.142s, episode steps:  55, steps per second:  13, episode reward:  0.713, mean reward:  0.013 [-0.487,  1.200], mean action: 1.418 [0.000, 3.000],  loss: 0.029961, mae: 1.803871, mean_q: 2.473441, mean_eps: 0.920033\n"," 88906/100000: episode: 2211, duration: 1.936s, episode steps:  26, steps per second:  13, episode reward: -0.489, mean reward: -0.019 [-0.489,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.039407, mae: 1.786616, mean_q: 2.436030, mean_eps: 0.919997\n"," 88932/100000: episode: 2212, duration: 2.005s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.026920, mae: 1.806764, mean_q: 2.471251, mean_eps: 0.919973\n"," 89012/100000: episode: 2213, duration: 6.032s, episode steps:  80, steps per second:  13, episode reward:  0.705, mean reward:  0.009 [-0.495,  1.200], mean action: 1.550 [0.000, 3.000],  loss: 0.027647, mae: 1.833106, mean_q: 2.509252, mean_eps: 0.919926\n"," 89038/100000: episode: 2214, duration: 1.991s, episode steps:  26, steps per second:  13, episode reward: -0.484, mean reward: -0.019 [-0.484,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.026145, mae: 1.818501, mean_q: 2.492912, mean_eps: 0.919878\n"," 89070/100000: episode: 2215, duration: 2.401s, episode steps:  32, steps per second:  13, episode reward: -0.495, mean reward: -0.015 [-0.495,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.023337, mae: 1.821061, mean_q: 2.494982, mean_eps: 0.919852\n"," 89143/100000: episode: 2216, duration: 5.492s, episode steps:  73, steps per second:  13, episode reward:  0.706, mean reward:  0.010 [-0.494,  1.200], mean action: 1.699 [0.000, 3.000],  loss: 0.022543, mae: 1.836412, mean_q: 2.510868, mean_eps: 0.919805\n"," 89170/100000: episode: 2217, duration: 2.144s, episode steps:  27, steps per second:  13, episode reward: -0.485, mean reward: -0.018 [-0.485,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.019967, mae: 1.862022, mean_q: 2.559576, mean_eps: 0.919760\n"," 89196/100000: episode: 2218, duration: 2.008s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.022864, mae: 1.830734, mean_q: 2.510869, mean_eps: 0.919736\n"," 89355/100000: episode: 2219, duration: 11.664s, episode steps: 159, steps per second:  14, episode reward:  3.105, mean reward:  0.020 [-0.495,  1.200], mean action: 1.491 [0.000, 3.000],  loss: 0.026664, mae: 1.827101, mean_q: 2.507960, mean_eps: 0.919653\n","\n","Target network updated after 30 episodes\n"," 89382/100000: episode: 2220, duration: 2.010s, episode steps:  27, steps per second:  13, episode reward: -0.468, mean reward: -0.017 [-0.468,  0.000], mean action: 1.815 [0.000, 3.000],  loss: 0.022509, mae: 1.857237, mean_q: 2.548993, mean_eps: 0.919569\n"," 89454/100000: episode: 2221, duration: 5.177s, episode steps:  72, steps per second:  14, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.514 [0.000, 3.000],  loss: 0.051657, mae: 1.798566, mean_q: 2.461550, mean_eps: 0.919524\n"," 89482/100000: episode: 2222, duration: 2.148s, episode steps:  28, steps per second:  13, episode reward: -0.486, mean reward: -0.017 [-0.486,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.046076, mae: 1.853345, mean_q: 2.540167, mean_eps: 0.919479\n"," 89509/100000: episode: 2223, duration: 2.041s, episode steps:  27, steps per second:  13, episode reward: -0.494, mean reward: -0.018 [-0.494,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.050427, mae: 1.862164, mean_q: 2.556559, mean_eps: 0.919454\n"," 89565/100000: episode: 2224, duration: 4.069s, episode steps:  56, steps per second:  14, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.500 [0.000, 3.000],  loss: 0.030013, mae: 1.865998, mean_q: 2.558530, mean_eps: 0.919417\n"," 89647/100000: episode: 2225, duration: 6.209s, episode steps:  82, steps per second:  13, episode reward:  0.711, mean reward:  0.009 [-0.489,  1.200], mean action: 1.366 [0.000, 3.000],  loss: 0.045040, mae: 1.829903, mean_q: 2.511378, mean_eps: 0.919355\n"," 89682/100000: episode: 2226, duration: 2.671s, episode steps:  35, steps per second:  13, episode reward: -0.484, mean reward: -0.014 [-0.484,  0.000], mean action: 1.771 [0.000, 3.000],  loss: 0.046493, mae: 1.819049, mean_q: 2.504716, mean_eps: 0.919302\n"," 89709/100000: episode: 2227, duration: 1.980s, episode steps:  27, steps per second:  14, episode reward: -0.493, mean reward: -0.018 [-0.493,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.035991, mae: 1.842247, mean_q: 2.528844, mean_eps: 0.919274\n"," 89736/100000: episode: 2228, duration: 2.003s, episode steps:  27, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.296 [0.000, 3.000],  loss: 0.031999, mae: 1.810673, mean_q: 2.497404, mean_eps: 0.919250\n"," 89807/100000: episode: 2229, duration: 5.345s, episode steps:  71, steps per second:  13, episode reward:  0.705, mean reward:  0.010 [-0.495,  1.200], mean action: 1.493 [0.000, 3.000],  loss: 0.048698, mae: 1.848363, mean_q: 2.543570, mean_eps: 0.919206\n"," 89833/100000: episode: 2230, duration: 2.051s, episode steps:  26, steps per second:  13, episode reward: -0.486, mean reward: -0.019 [-0.486,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.052347, mae: 1.827248, mean_q: 2.513181, mean_eps: 0.919162\n"," 89858/100000: episode: 2231, duration: 1.907s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.037212, mae: 1.791600, mean_q: 2.461492, mean_eps: 0.919140\n"," 90016/100000: episode: 2232, duration: 11.631s, episode steps: 158, steps per second:  14, episode reward:  3.105, mean reward:  0.020 [-0.495,  1.200], mean action: 1.639 [0.000, 3.000],  loss: 0.036527, mae: 1.811156, mean_q: 2.490235, mean_eps: 0.919057\n"," 90041/100000: episode: 2233, duration: 1.916s, episode steps:  25, steps per second:  13, episode reward: -0.468, mean reward: -0.019 [-0.468,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.045980, mae: 1.784187, mean_q: 2.454200, mean_eps: 0.918975\n"," 90066/100000: episode: 2234, duration: 1.954s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.050886, mae: 1.787442, mean_q: 2.450312, mean_eps: 0.918952\n"," 90091/100000: episode: 2235, duration: 1.861s, episode steps:  25, steps per second:  13, episode reward: -0.495, mean reward: -0.020 [-0.495,  0.000], mean action: 1.280 [0.000, 3.000],  loss: 0.026230, mae: 1.893462, mean_q: 2.595165, mean_eps: 0.918930\n"," 90121/100000: episode: 2236, duration: 2.336s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.017 [-0.495,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.031325, mae: 1.808224, mean_q: 2.492801, mean_eps: 0.918905\n"," 90146/100000: episode: 2237, duration: 2.000s, episode steps:  25, steps per second:  13, episode reward: -0.494, mean reward: -0.020 [-0.494,  0.000], mean action: 1.880 [0.000, 3.000],  loss: 0.034006, mae: 1.788939, mean_q: 2.461456, mean_eps: 0.918880\n"," 90199/100000: episode: 2238, duration: 3.877s, episode steps:  53, steps per second:  14, episode reward:  0.705, mean reward:  0.013 [-0.495,  1.200], mean action: 1.377 [0.000, 3.000],  loss: 0.037995, mae: 1.809084, mean_q: 2.484149, mean_eps: 0.918845\n"," 90226/100000: episode: 2239, duration: 2.096s, episode steps:  27, steps per second:  13, episode reward: -0.489, mean reward: -0.018 [-0.489,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.045424, mae: 1.846701, mean_q: 2.519884, mean_eps: 0.918809\n"," 90252/100000: episode: 2240, duration: 1.963s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.654 [0.000, 3.000],  loss: 0.042927, mae: 1.883305, mean_q: 2.574948, mean_eps: 0.918785\n"," 90282/100000: episode: 2241, duration: 2.307s, episode steps:  30, steps per second:  13, episode reward: -0.495, mean reward: -0.016 [-0.495,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.031638, mae: 1.826971, mean_q: 2.516454, mean_eps: 0.918760\n"," 90314/100000: episode: 2242, duration: 2.413s, episode steps:  32, steps per second:  13, episode reward: -0.494, mean reward: -0.015 [-0.494,  0.000], mean action: 1.188 [0.000, 3.000],  loss: 0.036005, mae: 1.859445, mean_q: 2.538126, mean_eps: 0.918732\n"," 90354/100000: episode: 2243, duration: 3.024s, episode steps:  40, steps per second:  13, episode reward: -0.494, mean reward: -0.012 [-0.494,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.035979, mae: 1.808411, mean_q: 2.481080, mean_eps: 0.918700\n"," 90379/100000: episode: 2244, duration: 1.897s, episode steps:  25, steps per second:  13, episode reward: -0.492, mean reward: -0.020 [-0.492,  0.000], mean action: 1.440 [0.000, 3.000],  loss: 0.051819, mae: 1.822780, mean_q: 2.497360, mean_eps: 0.918671\n"," 90405/100000: episode: 2245, duration: 1.938s, episode steps:  26, steps per second:  13, episode reward: -0.495, mean reward: -0.019 [-0.495,  0.000], mean action: 1.423 [0.000, 3.000],  loss: 0.027477, mae: 1.818200, mean_q: 2.510349, mean_eps: 0.918648\n"," 90433/100000: episode: 2246, duration: 2.163s, episode steps:  28, steps per second:  13, episode reward: -0.495, mean reward: -0.018 [-0.495,  0.000], mean action: 1.821 [0.000, 3.000],  loss: 0.047713, mae: 1.790943, mean_q: 2.455454, mean_eps: 0.918623\n"," 90488/100000: episode: 2247, duration: 4.104s, episode steps:  55, steps per second:  13, episode reward:  0.706, mean reward:  0.013 [-0.494,  1.200], mean action: 1.327 [0.000, 3.000],  loss: 0.035703, mae: 1.816093, mean_q: 2.497178, mean_eps: 0.918586\n"," 90544/100000: episode: 2248, duration: 4.092s, episode steps:  56, steps per second:  14, episode reward:  0.711, mean reward:  0.013 [-0.489,  1.200], mean action: 1.625 [0.000, 3.000],  loss: 0.040319, mae: 1.819363, mean_q: 2.496351, mean_eps: 0.918536\n"," 90569/100000: episode: 2249, duration: 1.889s, episode steps:  25, steps per second:  13, episode reward: -0.489, mean reward: -0.020 [-0.489,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.028135, mae: 1.835044, mean_q: 2.523530, mean_eps: 0.918500\n"]}]},{"cell_type":"markdown","source":["### 11. Test a Trained Agent"],"metadata":{"id":"4JDO3kGqeJP5"}},{"cell_type":"code","source":["test_model('/content/drive/MyDrive/breakout_dqn/breakout_dqn_final.h5')"],"metadata":{"id":"nFTfY4P-eQpJ","executionInfo":{"status":"aborted","timestamp":1747568929104,"user_tz":300,"elapsed":80508,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analytics Usage"],"metadata":{"id":"nkkmO-ypj61f"}},{"cell_type":"code","source":["# Example usage after training completes\n","# Analyze a single training run\n","analyze_training_logs('/content/drive/MyDrive/breakout_dqn/logs/training_log.csv')\n","\n","# Example of comparing two different training strategies\n","# compare_training_strategies(\n","#     '/content/drive/MyDrive/breakout_dqn_reward_shaping/logs/training_log.csv',\n","#     '/content/drive/MyDrive/breakout_dqn_baseline/logs/training_log.csv',\n","#     labels=('With Reward Shaping', 'Baseline DQN'),\n","#     output_path='/content/drive/MyDrive/breakout_dqn/strategy_comparison.png'\n","# )"],"metadata":{"id":"63d-Eqa5kEYR","executionInfo":{"status":"aborted","timestamp":1747568929105,"user_tz":300,"elapsed":80507,"user":{"displayName":"Nathan Rhys","userId":"11346355926369003947"}}},"execution_count":null,"outputs":[]}]}